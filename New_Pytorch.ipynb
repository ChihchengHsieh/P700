{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence, pad_packed_sequence, pack_padded_sequence\n",
    "import math\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class args(object):\n",
    "    \n",
    "    #### DATA  ####\n",
    "    node_size = 6 # equals to the number of nodes + 1 (zero_padding) # number of colour + 1 for padding\n",
    "    seq_len = 10\n",
    "    \n",
    "    #### Training ####\n",
    "    batch_size = 32\n",
    "    lr = 0.02\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    LSTM_maxnorm = 0.25\n",
    "    acc_threshold = 0.5\n",
    "    try_valid = 20\n",
    "    running_loss = False\n",
    "    n_epoch = 50\n",
    "    weight_reg = 0\n",
    "    \n",
    "    \n",
    "    #### Model ####\n",
    "    RNN_model = 'lstm' # The hidden_size has to be a list if using nalu\n",
    "    hidden_size = [512]\n",
    "    num_layers = 5\n",
    "    embedding_dim = 256\n",
    "    embedding_maxnorm = None\n",
    "    bidirectional = False\n",
    "    model_name = 'SiameseColourWithoutRestriction' \n",
    "    model_path ='./'+ model_name +'/Model/'\n",
    "    \n",
    "if not os.path.exists(args.model_path):\n",
    "    os.makedirs(args.model_path)\n",
    "\n",
    "'''\n",
    "TODO:\n",
    "\n",
    "'''   \n",
    "# SimplePathWith10th5Colours\n",
    "with open(\"SimplePathWith10th5Colours.txt\", \"rb\") as fp:   # Unpickling\n",
    "    df = pickle.load(fp)\n",
    "X = df[['left','right']]     \n",
    "Y = df['target']    \n",
    "del df\n",
    "\n",
    "#Seperate to training, validation, and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 64)\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size= 0.05,random_state= 64)\n",
    "Y_test = Y_test.values\n",
    "Y_train = Y_train.values\n",
    "Y_validation = Y_validation.values\n",
    "\n",
    "#Check shape\n",
    "assert X_train['left'].shape == X_train['right'].shape\n",
    "assert len(X_train['left']) == len(Y_train)\n",
    "\n",
    "def padding(data):\n",
    "    left = [] \n",
    "    for i in range(data.shape[0]):\n",
    "        left.append((data.iloc[i]['left']))\n",
    "    right = [] \n",
    "    for i in range(data.shape[0]):\n",
    "        right.append((data.iloc[i]['right']))\n",
    "    return torch.tensor(np.array([right,left])).transpose(1,0)\n",
    "\n",
    "def plot_train_hist(train_hist, step = None, ):\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    for name in train_hist.keys():\n",
    "        if 'Loss' in name:\n",
    "            plt.subplot(211)\n",
    "            plt.plot(train_hist[name],marker='o',label= name)\n",
    "            plt.ylabel('Loss',fontsize=15)\n",
    "            plt.xlabel('Number of epochs',fontsize=15)\n",
    "            plt.title('Loss',fontsize=20,fontweight =\"bold\")\n",
    "            plt.legend(loc='upper left')\n",
    "        else:\n",
    "            plt.subplot(212)\n",
    "            plt.plot(train_hist[name],marker='o',label= name)\n",
    "            plt.ylabel('Accuracy',fontsize=15)\n",
    "            plt.xlabel('Number of epochs',fontsize=15)\n",
    "            plt.title('Accuracy',fontsize=20,fontweight =\"bold\")\n",
    "            plt.legend(loc='upper left')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    if step is not None:\n",
    "        fig.savefig(\"Train_Hist\"+str(step)+\".png\") \n",
    "\n",
    "def CreateDataset(X_left, X_right, Y):\n",
    "    \n",
    "    X_left_list = list(X_left)\n",
    "    X_right_list = list(X_right)\n",
    "    \n",
    "    X_left_seq_len = torch.tensor(list(map(len, X_left_list)))\n",
    "    X_right_seq_len = torch.tensor(list(map(len, X_right_list)))\n",
    "    X_left_seq_len, X_left_len_idx = X_left_seq_len.sort(0,descending = True)\n",
    "    X_right_seq_len, X_right_len_idx = X_right_seq_len.sort(0,descending = True)\n",
    "    print('X_len',  X_left_seq_len , X_right_seq_len)\n",
    "    X_left_ordered = [torch.LongTensor(X_left_list[i]) for i in X_left_len_idx]\n",
    "    X_right_ordered = [torch.LongTensor(X_right_list[i]) for i in X_right_len_idx]\n",
    "    X_left_p = pad_sequence(X_left_ordered, batch_first = True)\n",
    "    X_right_p = pad_sequence(X_right_ordered, batch_first = True)\n",
    "    Y = torch.FloatTensor(np.array(Y))\n",
    "    train_dataset  = Data.TensorDataset(X_left_p, X_left_seq_len, X_right_p, X_right_seq_len, Y)\n",
    "    \n",
    "    return train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If it's not work, we can pad the seq to certain len.\n",
    "# X_train_left_list = list(X_train['left'])\n",
    "# X_train_right_list = list(X_train['right'])\n",
    "# X_train_left_seq_len = torch.tensor(list(map(len, X_train_left_list)))\n",
    "# X_train_right_seq_len = torch.tensor(list(map(len, X_train_right_list)))\n",
    "# X_train_left_seq_len, X_train_left_len_idx = X_train_left_seq_len.sort(0,descending = True)\n",
    "# X_train_right_seq_len, X_train_right_len_idx = X_train_right_seq_len.sort(0,descending = True)\n",
    "# X_left_train_ordered = [torch.tensor(X_train_left_list[i]) for i in X_train_left_len_idx]\n",
    "# X_right_train_ordered = [torch.tensor(X_train_right_list[i]) for i in X_train_right_len_idx]\n",
    "# X_train_left = pad_sequence(X_left_train_ordered, batch_first = True)\n",
    "# X_train_right = pad_sequence(X_right_train_ordered, batch_first = True)\n",
    "# Y_train = torch.FloatTensor(np.array(Y_train))\n",
    "# train_dataset  = Data.TensorDataset(X_train_left, X_train_right, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_len = 36336\n",
    "# X_train_list = list(X_train['left']) + list(X_train['right'])\n",
    "# X_train_seq_len = torch.tensor(list(map(len, X_train_list)))\n",
    "# X_train_seq_len, X_train_idx = X_train_seq_len.sort(0, descending= True)\n",
    "# # the left and the right will be mixed after the sort operator \n",
    "# # Seperate the idx first and use the idx to choose the dordered list,\n",
    "# left_idx = [ i for i in X_train_idx if i < X_len]\n",
    "# right_idx = [ i for i in X_train_idx if i >= X_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_len tensor([20, 20, 20,  ...,  4,  2,  2]) tensor([20, 20, 20,  ...,  2,  2,  2])\n",
      "X_len tensor([20, 20, 20, 20, 20, 20, 20, 20, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19,\n",
      "        19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 13,\n",
      "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  8,  8,  8,  8,\n",
      "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  7,  7,  7,  7,\n",
      "         6,  6,  6,  6,  5,  5,  2,  2]) tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 19, 19, 19, 19, 19, 19, 19,\n",
      "        19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "        18, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "        17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 15, 15, 15, 15, 15, 15, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "        14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "        13, 13, 13, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,  9,  9,  9,  9,  9,  9,\n",
      "         9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
      "         9,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  7,  7,  7,  7,\n",
      "         6,  6,  6,  6,  5,  5,  4,  3])\n",
      "X_len tensor([20, 20, 20,  ...,  4,  2,  2]) tensor([20, 20, 20,  ...,  4,  3,  3])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CreateDataset(X_train['left'], X_train['right'], Y_train)\n",
    "val_dataset = CreateDataset(X_validation['left'], X_validation['right'], Y_validation)\n",
    "test_dataset = CreateDataset(X_test['left'], X_test['right'], Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traninig data size:  15200\n",
      "Validation data size:  800\n",
      "Test data size:  4000\n"
     ]
    }
   ],
   "source": [
    "print('Traninig data size: ', len(train_dataset))\n",
    "print('Validation data size: ', len(val_dataset))\n",
    "print('Test data size: ', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Padding and creat the loaders\n",
    "# X_train = padding(X_train)\n",
    "# Y_train = torch.FloatTensor(np.array(Y_train))\n",
    "# train_dataset  = Data.TensorDataset(X_train,Y_train)\n",
    "\n",
    "# X_validation = padding(X_validation)\n",
    "# Y_validation = torch.FloatTensor(np.array(Y_validation))\n",
    "# val_dataset  = Data.TensorDataset(X_validation,Y_validation)\n",
    "\n",
    "# X_test = padding(X_test)\n",
    "# Y_test = torch.FloatTensor(np.array(Y_test))\n",
    "# test_dataset  = Data.TensorDataset(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralAccumulatorCell(nn.Module):\n",
    "    \n",
    "    # Feed forward but Weight decomposition\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.W_hat = Parameter(torch.Tensor(out_dim, in_dim))\n",
    "        self.M_hat = Parameter(torch.Tensor(out_dim, in_dim))\n",
    "        self.W = Parameter(torch.tanh(self.W_hat) * torch.sigmoid(self.M_hat))\n",
    "        self.register_parameter('bias', None)\n",
    "\n",
    "        init.kaiming_uniform_(self.W_hat, a=math.sqrt(5))\n",
    "        init.kaiming_uniform_(self.M_hat, a=math.sqrt(5))\n",
    "        \n",
    "        #init.normal_(self.W_hat)\n",
    "        #init.normal_(self.M_hat)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.W, self.bias)\n",
    "\n",
    "\n",
    "class NAC(nn.Module):\n",
    "    \n",
    "    def __init__(self, dims):\n",
    "        '''\n",
    "        dims = [input_dim + hidden_dims + output_dims]\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.num_layers = len(dims) - 1\n",
    "        \n",
    "        layers = nn.ModuleList()\n",
    "        layers.extend([NeuralAccumulatorCell(dims[i],dims[i+1]) for i in range(self.num_layers)])\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "\n",
    "class NeuralArithmeticLogicUnitCell(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.eps = 1e-10\n",
    "\n",
    "        self.G = Parameter(torch.Tensor(out_dim, in_dim))\n",
    "        self.W = Parameter(torch.Tensor(out_dim, in_dim))\n",
    "        self.register_parameter('bias', None)\n",
    "        self.nac = NeuralAccumulatorCell(in_dim, out_dim)\n",
    "\n",
    "        init.kaiming_uniform_(self.G, a = math.sqrt(5))\n",
    "        init.kaiming_uniform_(self.W, a = math.sqrt(5))\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        a = self.nac(input)\n",
    "        g = torch.sigmoid(F.linear(input, self.G, self.bias))\n",
    "        add_sub = g * a\n",
    "        log_input = torch.log(torch.abs(input) + self.eps)\n",
    "        m = torch.exp(self.nac(log_input))\n",
    "        # m = torch.exp(F.linear(log_input, self.W, self.bias))\n",
    "        mul_div = (1 - g) * m\n",
    "        y = add_sub + mul_div\n",
    "        return y\n",
    "\n",
    "\n",
    "class NALU(nn.Module):\n",
    "    \n",
    "    def __init__(self, dims):\n",
    "        super().__init__()\n",
    "        self.num_layers = len(dims) - 1\n",
    "        layers = nn.ModuleList()\n",
    "        layers.extend([NeuralArithmeticLogicUnitCell(dims[i],dims[i+1]) for i in range(self.num_layers)])\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "\n",
    "class NALU_LSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        self.i2h = nn.Sequential(\n",
    "            nn.Linear(input_size, 4 * hidden_size, bias=bias),\n",
    "            nn.BatchNorm1d(4 * hidden_size),\n",
    "#             nn.LeakyReLU(0.2,inplace=True),\n",
    "#             nn.Linear(4 *input_size, 4 * hidden_size, bias=bias),\n",
    "        )\n",
    "        self.h2h = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 4 * hidden_size, bias=bias),\n",
    "           nn.BatchNorm1d(4 * hidden_size),\n",
    "#             nn.LeakyReLU(0.2,inplace=True),\n",
    "#             nn.Linear(4 * hidden_size, 4 * hidden_size, bias=bias)\n",
    "        )\n",
    "        self.nalu_h = NALU([hidden_size, hidden_size])\n",
    "        self.nalu_c = NALU([hidden_size, hidden_size])\n",
    "        self.out = nn.Linear(hidden_size, input_size, bias=bias)\n",
    "        self.apply(self.weight_init)\n",
    "\n",
    "    def weight_init(self,m):\n",
    "\n",
    "        std = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for name, w in m.named_parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, x, hidden = None):\n",
    "        \n",
    "        if hidden is None:\n",
    "            hidden = x.new_zeros(x.size(0), self.hidden_size, requires_grad=False)\n",
    "            hidden = (hidden, hidden)\n",
    "            \n",
    "        h, c = hidden\n",
    "        \n",
    "        preact = self.i2h(x) + self.h2h(h)\n",
    "        \n",
    "        # First: apply nalu to replace activation func\n",
    "        \n",
    "        # self.nalu(preact)\n",
    "        \n",
    "        gates = preact[:, :3 * self.hidden_size].sigmoid()\n",
    "        g_t = preact[:, 3 * self.hidden_size:].tanh()\n",
    "        i_t = gates[:, :self.hidden_size] \n",
    "        f_t = gates[:, self.hidden_size:2 * self.hidden_size]\n",
    "        o_t = gates[:, -self.hidden_size:]\n",
    "        \n",
    "        # Second: Apply it in the output and hidden layer\n",
    "\n",
    "        c_t = (c*f_t) + (i_t*g_t)\n",
    "\n",
    "        h_t = o_t * c_t.tanh()\n",
    "        \n",
    "        # return x, (h_t, c_t) # LSTM\n",
    "        \n",
    "        # return x + self.out(h_t), (h_t + h, c_t + c) # Residule LSTM\n",
    "\n",
    "        return  x + self.out(h_t) , (self.nalu_h(h_t + h), self.nalu_c(c_t + c)) # Residule NALU\n",
    "\n",
    "class NALU_LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size, hidden_sizes, bidirectional = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            self.num_dir = 2\n",
    "        else:\n",
    "            self.num_dir = 1\n",
    "            \n",
    "        self.input_size = input_size\n",
    "        self.L = len(hidden_sizes)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.extend([NALU_LSTMCell(input_size,i) for i in hidden_sizes])\n",
    "        self.c0 = nn.ParameterList([nn.Parameter(torch.randn(self.num_dir, 1,i)) for i in hidden_sizes])\n",
    "        self.h0 = nn.ParameterList([nn.Parameter(torch.randn(self.num_dir, 1,i)) for i in hidden_sizes])\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        '''\n",
    "        input_shape = B, S, input_size\n",
    "        output_shape = B, num_dir, L, S, input_size\n",
    "        hidden, cells = S * (B, hidden_dim)\n",
    "        '''\n",
    "        \n",
    "        B,S = input.shape[:-1]\n",
    "        \n",
    "        outputs = torch.zeros(B, self.num_dir, self.L+1, S, self.input_size)\n",
    "        outputs[:,:,0,:,:] = input.unsqueeze(1).expand_as(outputs[:,:,0,:,:])\n",
    "        hiddens = []\n",
    "        cells = []\n",
    "    \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            f_h, f_c = self.h0[i][0].repeat(B,1), self.c0[i][0].repeat(B,1)\n",
    "            if self.bidirectional:\n",
    "                i_h, i_c = self.h0[i][1].repeat(B,1), self.c0[i][1].repeat(B,1)\n",
    "            for j in range(S):\n",
    "                f_out, (f_h,f_c) = layer(outputs[:,0,i,j,:].clone(), (f_h,f_c))\n",
    "                outputs[:,0,i+1,j,:] = f_out\n",
    "\n",
    "            if self.bidirectional:\n",
    "                for j in reversed(range(S)):\n",
    "                    i_out, (i_h,i_c) = layer(outputs[:,1,i,j,:].clone(), (i_h,i_c))\n",
    "                    outputs[:,1,i+1,j,:] = i_out\n",
    "                hiddens.append((torch.stack([f_h,i_h])))\n",
    "                cells.append(torch.stack([f_c,i_c]))\n",
    "            else:\n",
    "                hiddens.append(f_h)\n",
    "                cells.append(f_c)\n",
    "                \n",
    "        #outputs = outputs[:,:,-1:,:,:]\n",
    "        return outputs[:,:,1:,:,:].contiguous(), (hiddens, cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Encoder(nn.Module):\n",
    "    def __init__(self, model = 'lstm'):\n",
    "        super(RNN_Encoder, self).__init__()\n",
    "        \n",
    "        # No need for padding, but if we need the padding, we have to set the idx as 0\n",
    "        # So that's why we need to make the graph start from 0\n",
    "        self.model = model\n",
    "        self.embedding = nn.Embedding(args.node_size, embedding_dim = args.embedding_dim, padding_idx=0,\n",
    "                                      max_norm = args.embedding_maxnorm)\n",
    "        \n",
    "\n",
    "        if args.bidirectional:\n",
    "            num_dir = 2\n",
    "            fc_size1= args.hidden_size[-1]*args.seq_len*2\n",
    "            fc_size2= 128\n",
    "        else:\n",
    "            num_dir = 1\n",
    "            fc_size1= args.hidden_size[-1]\n",
    "            fc_size2= 16\n",
    "            \n",
    "        self.fc= nn.Sequential(\n",
    "            nn.Linear(fc_size1,fc_size2),\n",
    "            nn.BatchNorm1d(fc_size2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(fc_size2,2),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.model == 'lstm':\n",
    "            self.rnn = nn.LSTM(input_size = args.embedding_dim, hidden_size = args.hidden_size[-1],\n",
    "                           num_layers = args.num_layers, bidirectional = args.bidirectional, batch_first = True)\n",
    "            self.h0 = nn.Parameter(torch.randn(args.num_layers*num_dir, 1, args.hidden_size[-1]))\n",
    "            self.c0 = nn.Parameter(torch.ones(args.num_layers*num_dir, 1, args.hidden_size[-1]))\n",
    "            print('Using LSTM')\n",
    "        elif self.model == 'gru':\n",
    "            self.h0 = nn.Parameter(torch.randn(args.num_layers*num_dir, 1, args.hidden_size[-1]))\n",
    "            self.rnn = nn.GRU(input_size= args.embedding_dim, hidden_size= args.hidden_size[-1],\n",
    "                              num_layers = args.num_layers, bidirectional = args.bidirectional, batch_first = True)\n",
    "            print('Using GRU')\n",
    "            \n",
    "        elif self.model == 'nalu':\n",
    "            self.rnn = NALU_LSTM(args.embedding_dim, args.hidden_size, bidirectional= args.bidirectional)\n",
    "            print('Using NALU')\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        \n",
    "    def forward(self, x, lengths = None):\n",
    "        \n",
    "        B = x.size(0)\n",
    "        \n",
    "        out = self.embedding(x)\n",
    "        \n",
    "        if self.model == 'lstm':\n",
    "            h_ = (self.h0.repeat(1,B,1), self.c0.repeat(1,B,1))\n",
    "        elif self.model == 'gru':\n",
    "            h_ = self.h0.repeat(1,B,1)\n",
    "        \n",
    "        if self.model =='nalu':\n",
    "            out, hidden = self.rnn(out)\n",
    "            out = out[:,:,-1,:,:].squeeze() \n",
    "        else:\n",
    "            out = pack_padded_sequence(out, lengths, batch_first = True)\n",
    "            out, hidden = self.rnn(out, h_)\n",
    "            out, unpacked_len = pad_packed_sequence(out, batch_first=True)\n",
    "        \n",
    "        if args.bidirectional:\n",
    "            out = self.fc(out.contiguous().view(B,-1))\n",
    "        else:\n",
    "            out = self.fc(out[:,-1,:])\n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(SiameseNet, self).__init__()\n",
    "        \n",
    "        self.encoder = RNN_Encoder(args.RNN_model)\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.train_hist = defaultdict(list)\n",
    "        self.acc_hist = []\n",
    "        self.apply(self.weight_init)\n",
    "        \n",
    "        self.optim = optim.Adam(self.encoder.parameters(), lr = args.lr, betas= (args.beta1, args.beta2),\n",
    "                                weight_decay = args.weight_reg)\n",
    "        \n",
    "    def forward(self, left, left_len, right, right_len, y):\n",
    "        \n",
    "        self.optim.zero_grad()\n",
    "#         left = x[:,0,:]\n",
    "#         right = x[:,1,:]\n",
    "        \n",
    "        left_out = self.encoder(left, left_len)\n",
    "        right_out = self.encoder(right, right_len)\n",
    "        self.prediction = torch.exp(-torch.norm((left_out - right_out),1,-1))\n",
    "        self.loss = self.mse(self.prediction, y)\n",
    "        self.take = left_out - right_out\n",
    "        \n",
    "        if args.LSTM_maxnorm is not None:\n",
    "            nn.utils.clip_grad_norm_(self.encoder.rnn.parameters(), args.LSTM_maxnorm)\n",
    "        self.acc = torch.mean(((self.prediction>args.acc_threshold) == (y.byte())).float())\n",
    "        self.train_hist['Loss'].append(self.loss.item())\n",
    "        self.train_hist['Accuracy'].append(self.acc.item())\n",
    "        self.loss.backward()\n",
    "        self.optim.step()\n",
    "        \n",
    "    def weight_init(self,m):\n",
    "        if type(m) in [nn.Conv2d, nn.ConvTranspose2d, nn.Linear]:\n",
    "            nn.init.kaiming_normal_(m.weight,0.2,nonlinearity='leaky_relu')\n",
    "        elif type(m) in [nn.LSTM]:\n",
    "            for name, value in m.named_parameters():\n",
    "                if 'weight' in name :\n",
    "                    nn.init.xavier_normal_(value.data)\n",
    "                if 'bias'in name:\n",
    "                    value.data.normal_()\n",
    "                    \n",
    "    def model_save(self,step):\n",
    "        \n",
    "        path = args.model_path + args.model_name+'_Step_' + str(step) + '.pth'\n",
    "        torch.save({args.model_name:self.state_dict()}, path)\n",
    "        print('Model Saved')\n",
    "        \n",
    "    def load_step_dict(self, step):\n",
    "        \n",
    "        path = args.model_path + args.model_name +'_Step_' + str(step) + '.pth'\n",
    "        self.load_state_dict(torch.load(path, map_location = lambda storage, loc: storage)[args.model_name])\n",
    "        print('Model Loaded')\n",
    "        \n",
    "           \n",
    "    def plot_all_loss(self, step):\n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        num_loss = 2\n",
    "        i = 0\n",
    "        for name in self.train_hist.keys():\n",
    "            if 'V' not in name:\n",
    "                i+= 1\n",
    "                fig.add_subplot(num_loss,1,i)\n",
    "                plt.plot(self.train_hist[name], label = name)\n",
    "                plt.xlabel('Number of Steps',fontsize=15)\n",
    "                plt.ylabel( name, fontsize=15)\n",
    "                plt.title(name, fontsize=30, fontweight =\"bold\")\n",
    "                plt.legend(loc = 'upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        fig.savefig(\"Train_Hist\"+str(step)+\".png\") \n",
    "        \n",
    "    def test_step(self, left, left_len, right, right_len, y):\n",
    "        \n",
    "        \n",
    "        left_out = self.encoder(left, left_len)\n",
    "        right_out = self.encoder(right, right_len)\n",
    "        \n",
    "        self.v_prediction = torch.exp(-torch.norm((left_out - right_out),1,-1)).detach()\n",
    "        self.v_loss = self.mse(self.v_prediction, y)\n",
    "        self.v_acc = torch.mean(((self.v_prediction>args.acc_threshold) == (y.byte())).float())\n",
    "        self.train_hist['V_Loss'].append(self.v_loss.item())\n",
    "        self.train_hist['V_Accuracy'].append(self.v_acc.item())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = args.batch_size, shuffle=False, drop_last = True)\n",
    "valid_loader = DataLoader(val_dataset, batch_size = args.batch_size, shuffle=False, drop_last= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LSTM\n"
     ]
    }
   ],
   "source": [
    "args.weight_reg = 0\n",
    "args.RNN_model = 'lstm'\n",
    "siamese = SiameseNet().to(device)\n",
    "siamese.train()\n",
    "scheduler = optim.lr_scheduler.StepLR(siamese.optim, 2000, gamma=0.7)\n",
    "siamese.optim.param_groups[0]['lr']= 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hist = defaultdict(list)\n",
    "args.running_loss = True\n",
    "args.n_epoch = 50\n",
    "all_step = 0\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch [0] | Step [1] | lr [0.020000] | Loss: [0.3605] | Acc: [0.6250] | Time: 1.5s\n",
      "| Epoch [0] | Step [2] | lr [0.020000] | Loss: [0.3563] | Acc: [0.5312] | Time: 1.5s\n",
      "| Epoch [0] | Step [3] | lr [0.020000] | Loss: [0.2949] | Acc: [0.6250] | Time: 1.6s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-498e4c506dc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0msiamese\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mend_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mall_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/Richard/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-511e8ced7f59>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, left, left_len, right, right_len, y)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;31m#         left = x[:,0,:]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m#         right = x[:,1,:]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/Richard/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while epoch < args.n_epoch:\n",
    "    siamese.train()\n",
    "    for i, (left, left_len, right, right_len, y) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        scheduler.step()\n",
    "        start_t = time.time()\n",
    "        \n",
    "        left = left.to(device)\n",
    "        left_len = left_len.to(device)\n",
    "        right = right.to(device)\n",
    "        right_len = right_len.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        siamese(left, left_len, right, right_len, y)\n",
    "        end_t = time.time()\n",
    "        all_step += 1\n",
    "        print('| Epoch [%d] | Step [%d] | lr [%.6f] | Loss: [%.4f] | Acc: [%.4f] | Time: %.1fs' %\\\n",
    "              ( epoch, all_step, siamese.optim.param_groups[0]['lr'], siamese.loss.item() , siamese.acc.item() ,\n",
    "                end_t - start_t))\n",
    "        \n",
    "#         if siamese.acc.item() > 0.95: # Stop early\n",
    "#             raise StopIteration\n",
    "    \n",
    "    if i % 1 == 0:\n",
    "        siamese.eval()\n",
    "        for j, (v_left, v_left_len, v_right, v_right_len, v_y) in enumerate(valid_loader):\n",
    "\n",
    "            v_left = v_left.to(device)\n",
    "            v_left_len = v_left_len.to(device)\n",
    "            v_right = v_right.to(device)\n",
    "            v_right_len = v_right_len.to(device)\n",
    "            v_y = v_y.to(device)\n",
    "            siamese.test_step(v_left, v_left_len, v_right, v_right_len, v_y)\n",
    "            end_t = time.time()\n",
    "            print('| Epoch [%d] | Validation | Step [%d] |  Loss: [%.4f] | Acc: [%.4f] | Time: %.1fs' %\\\n",
    "                  ( epoch, j, siamese.v_loss.item() , siamese.v_acc.item() ,end_t - start_t))\n",
    "                \n",
    "    siamese.plot_all_loss('S_LSTMCycle_Training_0reg')\n",
    "    \n",
    "    if epoch >= 1:\n",
    "        plot_train_hist(train_hist, 'S_LSTMCycle_Epoch_0reg')\n",
    "\n",
    "    for name in siamese.train_hist.keys():\n",
    "        train_hist[name].append(sum(siamese.train_hist[name])/len(siamese.train_hist[name]))\n",
    "\n",
    "    if not args.running_loss:\n",
    "        for name in siamese.train_hist.keys():\n",
    "            siamese.train_hist[name] = []\n",
    "            \n",
    "    epoch += 1\n",
    "    \n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        siamese.model_save(epoch)\n",
    "    \n",
    "    if epoch >= 20:\n",
    "        raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### This Algorithm can solve the problem and make the graph in a batch of training data #####\n",
    "\n",
    "class Graph_Alg(object):\n",
    "    \n",
    "    def __init__(self,):\n",
    "        self.G = nx.Graph()\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        paired_data = np.array(x[y.byte()])\n",
    "        for i in paired_data:\n",
    "            for j in i:\n",
    "                for k in range(len(j)):\n",
    "                    if not j[k] in self.G.nodes:\n",
    "                        self.G.add_node(j[k])\n",
    "                    if k >= 1:\n",
    "                        if not (j[k],j[k-1]) in self.G.edges:\n",
    "                            self.G.add_edge(j[k],j[k-1])\n",
    "                            \n",
    "    def draw(self,):\n",
    "        \n",
    "        nx.draw(self.G, with_labels=True, font_weight='bold')\n",
    "        plt.show()\n",
    "        \n",
    "    def test(self, x, y):\n",
    "        \n",
    "        out = []\n",
    "        paired_data = np.array(x)\n",
    "        \n",
    "        for i in paired_data:\n",
    "            out.append(self.check_step(i))\n",
    "        match = list(np.array(out) == np.array(y).astype(int))\n",
    "        self.acc = sum(match) / len(match)\n",
    "             \n",
    "    def check_step(self, i):\n",
    "        \n",
    "        # Check the path is in the graph\n",
    "        for j in i:\n",
    "            for k in range(len(j)):\n",
    "                if not j[k] in self.G.nodes:\n",
    "                    return 0\n",
    "                if k >= 1:\n",
    "                    if not (j[k],j[k-1]) in self.G.edges:\n",
    "                        return 0\n",
    "                        \n",
    "        # Check the the pair is possible to have the same st_node   \n",
    "        for node in list(self.G.adj[i[0][0].item()]):\n",
    "            if node in list(self.G.adj[i[1][0].item()]):\n",
    "                return 1\n",
    "\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_algo = Graph_Alg()\n",
    "algo_train_hist = defaultdict(list)\n",
    "num_steps = 20\n",
    "for i in range(num_steps):\n",
    "    data, label = iter(train_loader).next()\n",
    "\n",
    "    # Training\n",
    "    graph_algo.forward(data, label)\n",
    "    \n",
    "    # Testing\n",
    "    v_data, v_label = iter(valid_loader).next()\n",
    "    graph_algo.test(v_data, v_label)\n",
    "    algo_train_hist['Accuracy'].append(graph_algo.acc)\n",
    "    \n",
    "plot_train_hist(algo_train_hist, 'Graph_algo')\n",
    "graph_algo.draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
