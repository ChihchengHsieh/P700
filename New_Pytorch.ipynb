{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import networkx as nx\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import math\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class args(object):\n",
    "    \n",
    "    #### DATA  ####\n",
    "    node_size = 21 # equals to the number of nodes + 1 (zero_padding)\n",
    "    seq_len = 5\n",
    "    \n",
    "    #### Training ####\n",
    "    batch_size = 32\n",
    "    lr = 0.0002\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    LSTM_maxnorm = 0.25\n",
    "    acc_threshold = 0.5\n",
    "    try_valid = 20\n",
    "    running_loss = True\n",
    "    n_epoch = 50\n",
    "    weight_reg = 0\n",
    "    \n",
    "    \n",
    "    #### Model ####\n",
    "    RNN_model = 'nalu' # The hidden_size has to be a list if using nalu\n",
    "    hidden_size = [50]\n",
    "    num_layers = 5\n",
    "    embedding_dim = 50\n",
    "    embedding_maxnorm = None\n",
    "    bidirectional = True\n",
    "    model_name = 'Siamese' \n",
    "    model_path ='./'+ model_name +'/Model/'\n",
    "    \n",
    "if not os.path.exists(args.model_path):\n",
    "    os.makedirs(args.model_path)\n",
    "\n",
    "'''\n",
    "TODO:\n",
    "\n",
    "'''   \n",
    "\n",
    "with open(\"Planar10thData.txt\", \"rb\") as fp:   # Unpickling\n",
    "    df = pickle.load(fp)\n",
    "X = df[['left','right']]     \n",
    "Y = df['target']    \n",
    "del df\n",
    "\n",
    "#Seperate to training, validation, and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 64)\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size= 0.05,random_state= 64)\n",
    "Y_test = Y_test.values\n",
    "Y_train = Y_train.values\n",
    "Y_validation = Y_validation.values\n",
    "\n",
    "#Check shape\n",
    "assert X_train['left'].shape == X_train['right'].shape\n",
    "assert len(X_train['left']) == len(Y_train)\n",
    "\n",
    "def padding(data):\n",
    "    left = [] \n",
    "    for i in range(data.shape[0]):\n",
    "        left.append((data.iloc[i]['left']))\n",
    "    right = [] \n",
    "    for i in range(data.shape[0]):\n",
    "        right.append((data.iloc[i]['right']))\n",
    "    return torch.tensor(np.array([right,left])).transpose(1,0)\n",
    "\n",
    "\n",
    "def plot_train_hist(train_hist, step = None, ):\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    for name in train_hist.keys():\n",
    "        if 'Loss' in name:\n",
    "            plt.subplot(211)\n",
    "            plt.plot(train_hist[name],marker='o',label= name)\n",
    "            plt.ylabel('Loss',fontsize=15)\n",
    "            plt.xlabel('Number of epochs',fontsize=15)\n",
    "            plt.title('Loss',fontsize=20,fontweight =\"bold\")\n",
    "            plt.legend(loc='upper left')\n",
    "        else:\n",
    "            plt.subplot(212)\n",
    "            plt.plot(train_hist[name],marker='o',label= name)\n",
    "            plt.ylabel('Accuracy',fontsize=15)\n",
    "            plt.xlabel('Number of epochs',fontsize=15)\n",
    "            plt.title('Accuracy',fontsize=20,fontweight =\"bold\")\n",
    "            plt.legend(loc='upper left')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    if step is not None:\n",
    "        fig.savefig(\"Train_Hist\"+str(step)+\".png\") \n",
    "\n",
    "\n",
    "#Padding and creat the loaders\n",
    "X_train = padding(X_train)\n",
    "Y_train = torch.FloatTensor(np.array(Y_train))\n",
    "train_dataset  = Data.TensorDataset(X_train,Y_train)\n",
    "\n",
    "X_validation = padding(X_validation)\n",
    "Y_validation = torch.FloatTensor(np.array(Y_validation))\n",
    "val_dataset  = Data.TensorDataset(X_validation,Y_validation)\n",
    "\n",
    "X_test = padding(X_test)\n",
    "Y_test = torch.FloatTensor(np.array(Y_test))\n",
    "test_dataset  = Data.TensorDataset(X_test,Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralAccumulatorCell(nn.Module):\n",
    "    \n",
    "    # Feed forward but Weight decomposition\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.W_hat = Parameter(torch.Tensor(out_dim, in_dim))\n",
    "        self.M_hat = Parameter(torch.Tensor(out_dim, in_dim))\n",
    "        self.W = Parameter(torch.tanh(self.W_hat) * torch.sigmoid(self.M_hat))\n",
    "        self.register_parameter('bias', None)\n",
    "\n",
    "        init.kaiming_uniform_(self.W_hat, a=math.sqrt(5))\n",
    "        init.kaiming_uniform_(self.M_hat, a=math.sqrt(5))\n",
    "        \n",
    "        #init.normal_(self.W_hat)\n",
    "        #init.normal_(self.M_hat)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.W, self.bias)\n",
    "\n",
    "\n",
    "class NAC(nn.Module):\n",
    "    \n",
    "    def __init__(self, dims):\n",
    "        '''\n",
    "        dims = [input_dim + hidden_dims + output_dims]\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.num_layers = len(dims) - 1\n",
    "        \n",
    "        layers = nn.ModuleList()\n",
    "        layers.extend([NeuralAccumulatorCell(dims[i],dims[i+1]) for i in range(self.num_layers)])\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "\n",
    "class NeuralArithmeticLogicUnitCell(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.eps = 1e-10\n",
    "\n",
    "        self.G = Parameter(torch.Tensor(out_dim, in_dim))\n",
    "        self.W = Parameter(torch.Tensor(out_dim, in_dim))\n",
    "        self.register_parameter('bias', None)\n",
    "        self.nac = NeuralAccumulatorCell(in_dim, out_dim)\n",
    "\n",
    "        init.kaiming_uniform_(self.G, a=math.sqrt(5))\n",
    "        init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        a = self.nac(input)\n",
    "        g = torch.sigmoid(F.linear(input, self.G, self.bias))\n",
    "        add_sub = g * a\n",
    "        log_input = torch.log(torch.abs(input) + self.eps)\n",
    "        m = torch.exp(self.nac(log_input))\n",
    "        # m = torch.exp(F.linear(log_input, self.W, self.bias))\n",
    "        mul_div = (1 - g) * m\n",
    "        y = add_sub + mul_div\n",
    "        return y\n",
    "\n",
    "\n",
    "class NALU(nn.Module):\n",
    "    \n",
    "    def __init__(self, dims):\n",
    "        super().__init__()\n",
    "        self.num_layers = len(dims) - 1\n",
    "        layers = nn.ModuleList()\n",
    "        layers.extend([NeuralArithmeticLogicUnitCell(dims[i],dims[i+1]) for i in range(self.num_layers)])\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "\n",
    "class NALU_LSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        self.i2h = nn.Sequential(\n",
    "            nn.Linear(input_size, 4 * hidden_size, bias=bias),\n",
    "#             nn.BatchNorm1d(4 * hidden_size),\n",
    "#             nn.LeakyReLU(0.2,inplace=True),\n",
    "#             nn.Linear(4 *input_size, 4 * hidden_size, bias=bias),\n",
    "        )\n",
    "        self.h2h = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 4 * hidden_size, bias=bias),\n",
    "#            nn.BatchNorm1d(4 * hidden_size),\n",
    "#             nn.LeakyReLU(0.2,inplace=True),\n",
    "#             nn.Linear(4 * hidden_size, 4 * hidden_size, bias=bias)\n",
    "        )\n",
    "        self.nalu_h = NALU([hidden_size, hidden_size])\n",
    "        self.nalu_c = NALU([hidden_size, hidden_size])\n",
    "        self.out = nn.Linear(hidden_size, input_size, bias=bias)\n",
    "        self.apply(self.weight_init)\n",
    "\n",
    "    def weight_init(self,m):\n",
    "\n",
    "        std = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for name, w in m.named_parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "\n",
    "    def forward(self, x, hidden = None):\n",
    "        \n",
    "        if hidden is None:\n",
    "            hidden = x.new_zeros(x.size(0), self.hidden_size, requires_grad=False)\n",
    "            hidden = (hidden, hidden)\n",
    "            \n",
    "        h, c = hidden\n",
    "        \n",
    "        preact = self.i2h(x) + self.h2h(h)\n",
    "        \n",
    "        # First: apply nalu to replace activation func\n",
    "        \n",
    "        # self.nalu(preact)\n",
    "        \n",
    "        gates = preact[:, :3 * self.hidden_size].sigmoid()\n",
    "        g_t = preact[:, 3 * self.hidden_size:].tanh()\n",
    "        i_t = gates[:, :self.hidden_size] \n",
    "        f_t = gates[:, self.hidden_size:2 * self.hidden_size]\n",
    "        o_t = gates[:, -self.hidden_size:]\n",
    "        \n",
    "        # Second: Apply it in the output and hidden layer\n",
    "\n",
    "        c_t = (c*f_t) + (i_t*g_t)\n",
    "\n",
    "        h_t = o_t * c_t.tanh()\n",
    "        \n",
    "        # return x, (h_t, c_t) # LSTM\n",
    "        \n",
    "        # return x + self.out(h_t), (h_t + h, c_t + c) # Residule LSTM\n",
    "\n",
    "        return  x + self.out(h_t) , (self.nalu_h(h_t + h), self.nalu_c(c_t + c)) # Residule NALU\n",
    "\n",
    "class NALU_LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size, hidden_sizes, bidirectional = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            self.num_dir = 2\n",
    "        else:\n",
    "            self.num_dir = 1\n",
    "            \n",
    "        self.input_size = input_size\n",
    "        self.L = len(hidden_sizes)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.extend([NALU_LSTMCell(input_size,i) for i in hidden_sizes])\n",
    "        self.c0 = nn.ParameterList([nn.Parameter(torch.randn(self.num_dir, 1,i)) for i in hidden_sizes])\n",
    "        self.h0 = nn.ParameterList([nn.Parameter(torch.randn(self.num_dir, 1,i)) for i in hidden_sizes])\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        '''\n",
    "        input_shape = B, S, input_size\n",
    "        output_shape = B, num_dir, L, S, input_size\n",
    "        hidden, cells = S * (B, hidden_dim)\n",
    "        '''\n",
    "        \n",
    "        B,S = input.shape[:-1]\n",
    "        \n",
    "        outputs = torch.zeros(B, self.num_dir, self.L+1, S, self.input_size)\n",
    "        outputs[:,:,0,:,:] = input.unsqueeze(1).expand_as(outputs[:,:,0,:,:])\n",
    "        hiddens = []\n",
    "        cells = []\n",
    "    \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            f_h, f_c = self.h0[i][0].repeat(B,1), self.c0[i][0].repeat(B,1)\n",
    "            if self.bidirectional:\n",
    "                i_h, i_c = self.h0[i][1].repeat(B,1), self.c0[i][1].repeat(B,1)\n",
    "            for j in range(S):\n",
    "                f_out, (f_h,f_c) = layer(outputs[:,0,i,j,:].clone(), (f_h,f_c))\n",
    "                outputs[:,0,i+1,j,:] = f_out\n",
    "\n",
    "            if self.bidirectional:\n",
    "                for j in reversed(range(S)):\n",
    "                    i_out, (i_h,i_c) = layer(outputs[:,1,i,j,:].clone(), (i_h,i_c))\n",
    "                    outputs[:,1,i+1,j,:] = i_out\n",
    "                hiddens.append((torch.stack([f_h,i_h])))\n",
    "                cells.append(torch.stack([f_c,i_c]))\n",
    "            else:\n",
    "                hiddens.append(f_h)\n",
    "                cells.append(f_c)\n",
    "                \n",
    "        #outputs = outputs[:,:,-1:,:,:]\n",
    "        return outputs[:,:,1:,:,:].contiguous(), (hiddens, cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Encoder(nn.Module):\n",
    "    def __init__(self, model = 'lstm'):\n",
    "        super(RNN_Encoder, self).__init__()\n",
    "        \n",
    "        # No need for padding, but if we need the padding, we have to set the idx as 0\n",
    "        # So that's why we need to make the graph start from 0\n",
    "        self.model = model\n",
    "        self.embedding = nn.Embedding(args.node_size, embedding_dim = args.embedding_dim, padding_idx=0,\n",
    "                                      max_norm = args.embedding_maxnorm)\n",
    "        \n",
    "\n",
    "        if args.bidirectional:\n",
    "            num_dir = 2\n",
    "            fc_size1= args.hidden_size[-1]*args.seq_len*2\n",
    "            fc_size2= 128\n",
    "        else:\n",
    "            num_dir = 1\n",
    "            fc_size1= args.hidden_size[-1]\n",
    "            fc_size2= 16\n",
    "            \n",
    "        self.fc= nn.Sequential(\n",
    "            nn.Linear(fc_size1,fc_size2),\n",
    "            nn.BatchNorm1d(fc_size2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(fc_size2,2),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.model == 'lstm':\n",
    "            self.rnn = nn.LSTM(input_size = args.embedding_dim, hidden_size = args.hidden_size[-1],\n",
    "                           num_layers = args.num_layers, bidirectional = args.bidirectional, batch_first = True)\n",
    "            self.h0 = nn.Parameter(torch.randn(args.num_layers*num_dir, 1, args.hidden_size[-1]))\n",
    "            self.c0 = nn.Parameter(torch.ones(args.num_layers*num_dir, 1, args.hidden_size[-1]))\n",
    "            print('Using LSTM')\n",
    "        elif self.model == 'gru':\n",
    "            self.h0 = nn.Parameter(torch.randn(args.num_layers*num_dir, 1, args.hidden_size[-1]))\n",
    "            self.rnn = nn.GRU(input_size= args.embedding_dim, hidden_size= args.hidden_size[-1],\n",
    "                              num_layers = args.num_layers, bidirectional = args.bidirectional, batch_first = True)\n",
    "            print('Using GRU')\n",
    "            \n",
    "        elif self.model == 'nalu':\n",
    "            self.rnn = NALU_LSTM(args.embedding_dim, args.hidden_size, bidirectional= args.bidirectional)\n",
    "            print('Using NALU')\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        B = x.size(0)\n",
    "        \n",
    "        out = self.embedding(x)\n",
    "        \n",
    "        if self.model == 'lstm':\n",
    "            h_ = (self.h0.repeat(1,B,1), self.c0.repeat(1,B,1))\n",
    "        elif self.model == 'gru':\n",
    "            h_ = self.h0.repeat(1,B,1)\n",
    "        \n",
    "        if self.model =='nalu':\n",
    "            out, hidden = self.rnn(out)\n",
    "            out = out[:,:,-1,:,:].squeeze() \n",
    "        else:\n",
    "            out, hidden = self.rnn(out, h_)\n",
    "        \n",
    "        if args.bidirectional:\n",
    "            out = self.fc(out.contiguous().view(B,-1))\n",
    "        else:\n",
    "            out = self.fc(out[:,-1,:])\n",
    "        return out\n",
    "    \n",
    "    \n",
    "\n",
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(SiameseNet, self).__init__()\n",
    "        \n",
    "        self.encoder = RNN_Encoder(args.RNN_model)\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.train_hist = defaultdict(list)\n",
    "        self.acc_hist = []\n",
    "        self.apply(self.weight_init)\n",
    "        \n",
    "        self.optim = optim.Adam(self.encoder.parameters(), lr = args.lr, betas= (args.beta1, args.beta2),\n",
    "                                weight_decay = args.weight_reg)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        self.optim.zero_grad()\n",
    "        left = x[:,0,:]\n",
    "        right = x[:,1,:]\n",
    "        \n",
    "        left_out = self.encoder(left)\n",
    "        right_out = self.encoder(right)\n",
    "        self.prediction = torch.exp(-torch.norm((left_out - right_out),1,-1))\n",
    "        self.loss = self.mse(self.prediction, y)\n",
    "        self.take = left_out - right_out\n",
    "        \n",
    "        if args.LSTM_maxnorm is not None:\n",
    "            nn.utils.clip_grad_norm_(self.encoder.rnn.parameters(), args.LSTM_maxnorm)\n",
    "        self.acc = torch.mean(((self.prediction>args.acc_threshold) == (y.byte())).float())\n",
    "        self.train_hist['Loss'].append(self.loss.item())\n",
    "        self.train_hist['Accuracy'].append(self.acc.item())\n",
    "        self.loss.backward()\n",
    "        self.optim.step()\n",
    "        \n",
    "    def weight_init(self,m):\n",
    "        if type(m) in [nn.Conv2d, nn.ConvTranspose2d, nn.Linear]:\n",
    "            nn.init.kaiming_normal_(m.weight,0.2,nonlinearity='leaky_relu')\n",
    "        elif type(m) in [nn.LSTM]:\n",
    "            for name, value in m.named_parameters():\n",
    "                if 'weight' in name :\n",
    "                    nn.init.xavier_normal_(value.data)\n",
    "                if 'bias'in name:\n",
    "                    value.data.normal_()\n",
    "                    \n",
    "    def model_save(self,step):\n",
    "        \n",
    "        path = args.model_path + args.model_name+'_Step_' + str(step) + '.pth'\n",
    "        torch.save({args.model_name:self.state_dict()}, path)\n",
    "        print('Model Saved')\n",
    "        \n",
    "    def load_step_dict(self, step):\n",
    "        \n",
    "        path = args.model_path + args.model_name +'_Step_' + str(step) + '.pth'\n",
    "        self.load_state_dict(torch.load(path, map_location = lambda storage, loc: storage)[args.model_name])\n",
    "        print('Model Loaded')\n",
    "        \n",
    "           \n",
    "    def plot_all_loss(self, step):\n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        num_loss = 2\n",
    "        i = 0\n",
    "        for name in self.train_hist.keys():\n",
    "            if 'V' not in name:\n",
    "                i+= 1\n",
    "                fig.add_subplot(num_loss,1,i)\n",
    "                plt.plot(self.train_hist[name], label = name)\n",
    "                plt.xlabel('Number of Steps',fontsize=15)\n",
    "                plt.ylabel( name, fontsize=15)\n",
    "                plt.title(name, fontsize=30, fontweight =\"bold\")\n",
    "                plt.legend(loc = 'upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        fig.savefig(\"Train_Hist\"+str(step)+\".png\") \n",
    "        \n",
    "    def test_step(self, x, y):\n",
    "        \n",
    "        left = x[:,0,:]\n",
    "        right = x[:,0,:]\n",
    "        \n",
    "        left_out = self.encoder(left)\n",
    "        right_out = self.encoder(right)\n",
    "        \n",
    "        self.v_prediction = torch.exp(-torch.norm((left_out - right_out),1,-1)).detach()\n",
    "        self.v_loss = self.mse(self.v_prediction, y)\n",
    "        self.v_acc = torch.mean(((self.v_prediction>args.acc_threshold) == (y.byte())).float())\n",
    "        self.train_hist['V_Loss'].append(self.v_loss.item())\n",
    "        self.train_hist['V_Accuracy'].append(self.v_acc.item())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = args.batch_size, shuffle=True, drop_last = True)\n",
    "valid_loader = DataLoader(val_dataset, batch_size = args.batch_size, shuffle=True, drop_last= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using NALU\n"
     ]
    }
   ],
   "source": [
    "args.weight_reg = 0\n",
    "args.RNN_model = 'nalu'\n",
    "siamese = SiameseNet().to(device)\n",
    "siamese.train()\n",
    "scheduler = optim.lr_scheduler.StepLR(siamese.optim, 5000, gamma=0.5)\n",
    "siamese.optim.param_groups[0]['lr']= 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hist = defaultdict(list)\n",
    "args.running_loss = True\n",
    "args.n_epoch = 50\n",
    "all_step = 0\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch [0] | Step [1] | lr [0.000200] | Loss: [0.3691] | Acc: [0.4062] | Time: 0.0s\n",
      "| Epoch [0] | Step [2] | lr [0.000200] | Loss: [0.1748] | Acc: [0.7188] | Time: 0.0s\n",
      "| Epoch [0] | Step [3] | lr [0.000200] | Loss: [0.3135] | Acc: [0.5312] | Time: 0.1s\n",
      "| Epoch [0] | Step [4] | lr [0.000200] | Loss: [0.2903] | Acc: [0.5000] | Time: 0.1s\n",
      "| Epoch [0] | Step [5] | lr [0.000200] | Loss: [0.2873] | Acc: [0.5312] | Time: 0.0s\n",
      "| Epoch [0] | Step [6] | lr [0.000200] | Loss: [0.3330] | Acc: [0.4375] | Time: 0.0s\n",
      "| Epoch [0] | Step [7] | lr [0.000200] | Loss: [0.3150] | Acc: [0.5000] | Time: 0.0s\n",
      "| Epoch [0] | Step [8] | lr [0.000200] | Loss: [0.3361] | Acc: [0.4688] | Time: 0.0s\n",
      "| Epoch [0] | Step [9] | lr [0.000200] | Loss: [0.2780] | Acc: [0.5312] | Time: 0.0s\n",
      "| Epoch [0] | Step [10] | lr [0.000200] | Loss: [0.2897] | Acc: [0.5312] | Time: 0.0s\n",
      "| Epoch [0] | Step [11] | lr [0.000200] | Loss: [0.2300] | Acc: [0.6562] | Time: 0.0s\n",
      "| Epoch [0] | Step [12] | lr [0.000200] | Loss: [0.2737] | Acc: [0.4688] | Time: 0.0s\n",
      "| Epoch [0] | Step [13] | lr [0.000200] | Loss: [0.2720] | Acc: [0.5625] | Time: 0.0s\n",
      "| Epoch [0] | Step [14] | lr [0.000200] | Loss: [0.2826] | Acc: [0.4688] | Time: 0.0s\n",
      "| Epoch [0] | Step [15] | lr [0.000200] | Loss: [0.2295] | Acc: [0.5938] | Time: 0.0s\n",
      "| Epoch [0] | Step [16] | lr [0.000200] | Loss: [0.3517] | Acc: [0.4375] | Time: 0.0s\n",
      "| Epoch [0] | Step [17] | lr [0.000200] | Loss: [0.2330] | Acc: [0.6250] | Time: 0.0s\n",
      "| Epoch [0] | Step [18] | lr [0.000200] | Loss: [0.2672] | Acc: [0.4688] | Time: 0.0s\n",
      "| Epoch [0] | Step [19] | lr [0.000200] | Loss: [0.2913] | Acc: [0.6250] | Time: 0.0s\n",
      "| Epoch [0] | Step [20] | lr [0.000200] | Loss: [0.2928] | Acc: [0.5625] | Time: 0.0s\n",
      "| Epoch [0] | Step [21] | lr [0.000200] | Loss: [0.2561] | Acc: [0.5625] | Time: 0.0s\n",
      "| Epoch [0] | Step [22] | lr [0.000200] | Loss: [0.3187] | Acc: [0.3750] | Time: 0.0s\n",
      "| Epoch [0] | Step [23] | lr [0.000200] | Loss: [0.3259] | Acc: [0.5312] | Time: 0.0s\n",
      "| Epoch [0] | Step [24] | lr [0.000200] | Loss: [0.2787] | Acc: [0.4688] | Time: 0.0s\n",
      "| Epoch [0] | Step [25] | lr [0.000200] | Loss: [0.3412] | Acc: [0.5312] | Time: 0.0s\n",
      "| Epoch [0] | Step [26] | lr [0.000200] | Loss: [0.3357] | Acc: [0.4062] | Time: 0.0s\n",
      "| Epoch [0] | Step [27] | lr [0.000200] | Loss: [0.2843] | Acc: [0.5938] | Time: 0.0s\n",
      "| Epoch [0] | Step [28] | lr [0.000200] | Loss: [0.3140] | Acc: [0.4688] | Time: 0.0s\n",
      "| Epoch [0] | Step [29] | lr [0.000200] | Loss: [0.2250] | Acc: [0.6250] | Time: 0.0s\n",
      "| Epoch [0] | Step [30] | lr [0.000200] | Loss: [0.2691] | Acc: [0.5000] | Time: 0.0s\n",
      "| Epoch [0] | Step [31] | lr [0.000200] | Loss: [0.2693] | Acc: [0.6250] | Time: 0.0s\n",
      "| Epoch [0] | Step [32] | lr [0.000200] | Loss: [0.2603] | Acc: [0.5938] | Time: 0.0s\n",
      "| Epoch [0] | Step [33] | lr [0.000200] | Loss: [0.2516] | Acc: [0.6250] | Time: 0.0s\n",
      "| Epoch [0] | Step [34] | lr [0.000200] | Loss: [0.2949] | Acc: [0.4375] | Time: 0.0s\n",
      "| Epoch [0] | Step [35] | lr [0.000200] | Loss: [0.2418] | Acc: [0.5625] | Time: 0.0s\n",
      "| Epoch [0] | Step [36] | lr [0.000200] | Loss: [0.2346] | Acc: [0.6250] | Time: 0.0s\n",
      "| Epoch [0] | Step [37] | lr [0.000200] | Loss: [0.2887] | Acc: [0.4688] | Time: 0.0s\n",
      "| Epoch [0] | Step [38] | lr [0.000200] | Loss: [0.2423] | Acc: [0.5938] | Time: 0.0s\n",
      "| Epoch [0] | Step [39] | lr [0.000200] | Loss: [0.2999] | Acc: [0.4688] | Time: 0.0s\n",
      "| Epoch [0] | Step [40] | lr [0.000200] | Loss: [0.2160] | Acc: [0.6562] | Time: 0.0s\n",
      "| Epoch [0] | Step [41] | lr [0.000200] | Loss: [0.2505] | Acc: [0.5625] | Time: 0.0s\n",
      "| Epoch [0] | Step [42] | lr [0.000200] | Loss: [0.2405] | Acc: [0.6562] | Time: 0.0s\n",
      "| Epoch [0] | Step [43] | lr [0.000200] | Loss: [0.2344] | Acc: [0.5625] | Time: 0.0s\n",
      "| Epoch [0] | Step [44] | lr [0.000200] | Loss: [0.2644] | Acc: [0.6250] | Time: 0.0s\n",
      "| Epoch [0] | Step [45] | lr [0.000200] | Loss: [0.2115] | Acc: [0.7188] | Time: 0.0s\n",
      "| Epoch [0] | Step [46] | lr [0.000200] | Loss: [0.2572] | Acc: [0.5312] | Time: 0.0s\n",
      "| Epoch [0] | Step [47] | lr [0.000200] | Loss: [0.2571] | Acc: [0.5625] | Time: 0.0s\n",
      "| Epoch [0] | Step [48] | lr [0.000200] | Loss: [0.2647] | Acc: [0.5625] | Time: 0.0s\n",
      "| Epoch [0] | Step [49] | lr [0.000200] | Loss: [0.1881] | Acc: [0.7188] | Time: 0.0s\n",
      "| Epoch [0] | Step [50] | lr [0.000200] | Loss: [0.2398] | Acc: [0.6250] | Time: 0.0s\n",
      "| Epoch [0] | Step [51] | lr [0.000200] | Loss: [0.2863] | Acc: [0.4375] | Time: 0.0s\n",
      "| Epoch [0] | Step [52] | lr [0.000200] | Loss: [0.2598] | Acc: [0.5312] | Time: 0.0s\n",
      "| Epoch [0] | Step [53] | lr [0.000200] | Loss: [0.2711] | Acc: [0.5625] | Time: 0.0s\n",
      "| Epoch [0] | Step [54] | lr [0.000200] | Loss: [0.2903] | Acc: [0.4375] | Time: 0.0s\n",
      "| Epoch [0] | Step [55] | lr [0.000200] | Loss: [0.2148] | Acc: [0.6875] | Time: 0.0s\n",
      "| Epoch [0] | Step [56] | lr [0.000200] | Loss: [0.2732] | Acc: [0.5625] | Time: 0.0s\n",
      "| Epoch [0] | Step [57] | lr [0.000200] | Loss: [0.2277] | Acc: [0.6562] | Time: 0.0s\n",
      "| Epoch [0] | Step [58] | lr [0.000200] | Loss: [0.3082] | Acc: [0.4688] | Time: 0.0s\n",
      "| Epoch [0] | Step [59] | lr [0.000200] | Loss: [0.2164] | Acc: [0.6250] | Time: 0.0s\n",
      "| Epoch [0] | Step [60] | lr [0.000200] | Loss: [0.2601] | Acc: [0.5312] | Time: 0.0s\n",
      "| Epoch [0] | Step [61] | lr [0.000200] | Loss: [0.2169] | Acc: [0.6875] | Time: 0.0s\n",
      "| Epoch [0] | Step [62] | lr [0.000200] | Loss: [0.2653] | Acc: [0.4688] | Time: 0.0s\n",
      "| Epoch [0] | Step [63] | lr [0.000200] | Loss: [0.2822] | Acc: [0.5625] | Time: 0.1s\n",
      "| Epoch [0] | Step [64] | lr [0.000200] | Loss: [0.2727] | Acc: [0.5625] | Time: 0.0s\n",
      "| Epoch [0] | Step [65] | lr [0.000200] | Loss: [0.1941] | Acc: [0.7188] | Time: 0.0s\n",
      "| Epoch [0] | Step [66] | lr [0.000200] | Loss: [0.2624] | Acc: [0.5312] | Time: 0.0s\n",
      "| Epoch [0] | Step [67] | lr [0.000200] | Loss: [0.2160] | Acc: [0.5625] | Time: 0.0s\n",
      "| Epoch [0] | Step [68] | lr [0.000200] | Loss: [0.2333] | Acc: [0.6250] | Time: 0.0s\n",
      "| Epoch [0] | Step [69] | lr [0.000200] | Loss: [0.2292] | Acc: [0.5938] | Time: 0.0s\n",
      "| Epoch [0] | Step [70] | lr [0.000200] | Loss: [0.2046] | Acc: [0.5938] | Time: 0.0s\n",
      "| Epoch [0] | Step [71] | lr [0.000200] | Loss: [0.2352] | Acc: [0.5938] | Time: 0.0s\n",
      "| Epoch [0] | Step [72] | lr [0.000200] | Loss: [0.2186] | Acc: [0.5938] | Time: 0.0s\n",
      "| Epoch [0] | Step [73] | lr [0.000200] | Loss: [0.2735] | Acc: [0.5000] | Time: 0.0s\n",
      "| Epoch [0] | Step [74] | lr [0.000200] | Loss: [0.2255] | Acc: [0.6250] | Time: 0.0s\n",
      "| Epoch [0] | Step [75] | lr [0.000200] | Loss: [0.2434] | Acc: [0.5625] | Time: 0.0s\n",
      "| Epoch [0] | Step [76] | lr [0.000200] | Loss: [0.2531] | Acc: [0.5000] | Time: 0.0s\n",
      "| Epoch [0] | Step [77] | lr [0.000200] | Loss: [0.2278] | Acc: [0.6562] | Time: 0.0s\n",
      "| Epoch [0] | Step [78] | lr [0.000200] | Loss: [0.2783] | Acc: [0.5625] | Time: 0.0s\n",
      "| Epoch [0] | Step [79] | lr [0.000200] | Loss: [0.2509] | Acc: [0.5938] | Time: 0.0s\n",
      "| Epoch [0] | Step [80] | lr [0.000200] | Loss: [0.2608] | Acc: [0.5000] | Time: 0.0s\n",
      "| Epoch [0] | Step [81] | lr [0.000200] | Loss: [0.2401] | Acc: [0.5938] | Time: 0.0s\n",
      "| Epoch [0] | Step [82] | lr [0.000200] | Loss: [0.2121] | Acc: [0.6875] | Time: 0.0s\n",
      "| Epoch [0] | Step [83] | lr [0.000200] | Loss: [0.2487] | Acc: [0.6250] | Time: 0.0s\n",
      "| Epoch [0] | Step [84] | lr [0.000200] | Loss: [0.2553] | Acc: [0.5312] | Time: 0.0s\n",
      "| Epoch [0] | Step [85] | lr [0.000200] | Loss: [0.2210] | Acc: [0.6562] | Time: 0.0s\n",
      "| Epoch [0] | Step [86] | lr [0.000200] | Loss: [0.2766] | Acc: [0.5625] | Time: 0.0s\n",
      "| Epoch [0] | Step [87] | lr [0.000200] | Loss: [0.2439] | Acc: [0.5938] | Time: 0.0s\n",
      "| Epoch [0] | Step [88] | lr [0.000200] | Loss: [0.2535] | Acc: [0.5938] | Time: 0.0s\n",
      "| Epoch [0] | Step [89] | lr [0.000200] | Loss: [0.2263] | Acc: [0.5312] | Time: 0.0s\n",
      "| Epoch [0] | Step [90] | lr [0.000200] | Loss: [0.1920] | Acc: [0.7500] | Time: 0.0s\n",
      "| Epoch [0] | Step [91] | lr [0.000200] | Loss: [0.1953] | Acc: [0.7500] | Time: 0.0s\n",
      "| Epoch [0] | Step [92] | lr [0.000200] | Loss: [0.2731] | Acc: [0.5625] | Time: 0.0s\n",
      "| Epoch [0] | Step [93] | lr [0.000200] | Loss: [0.2471] | Acc: [0.5312] | Time: 0.0s\n",
      "| Epoch [0] | Step [94] | lr [0.000200] | Loss: [0.2403] | Acc: [0.5938] | Time: 0.0s\n",
      "| Epoch [0] | Step [95] | lr [0.000200] | Loss: [0.2376] | Acc: [0.5625] | Time: 0.0s\n",
      "| Epoch [0] | Step [96] | lr [0.000200] | Loss: [0.2389] | Acc: [0.5938] | Time: 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch [0] | Step [97] | lr [0.000200] | Loss: [0.2425] | Acc: [0.6875] | Time: 0.0s\n",
      "| Epoch [0] | Step [98] | lr [0.000200] | Loss: [0.2505] | Acc: [0.6250] | Time: 0.0s\n",
      "| Epoch [0] | Step [99] | lr [0.000200] | Loss: [0.1938] | Acc: [0.7188] | Time: 0.0s\n",
      "| Epoch [0] | Step [100] | lr [0.000200] | Loss: [0.2163] | Acc: [0.6562] | Time: 0.0s\n",
      "| Epoch [0] | Step [101] | lr [0.000200] | Loss: [0.1984] | Acc: [0.6562] | Time: 0.0s\n",
      "| Epoch [0] | Step [102] | lr [0.000200] | Loss: [0.2842] | Acc: [0.4688] | Time: 0.0s\n",
      "| Epoch [0] | Step [103] | lr [0.000200] | Loss: [0.1595] | Acc: [0.7188] | Time: 0.0s\n",
      "| Epoch [0] | Step [104] | lr [0.000200] | Loss: [0.2047] | Acc: [0.6562] | Time: 0.0s\n",
      "| Epoch [0] | Step [105] | lr [0.000200] | Loss: [0.2407] | Acc: [0.5625] | Time: 0.0s\n",
      "| Epoch [0] | Step [106] | lr [0.000200] | Loss: [0.2176] | Acc: [0.6562] | Time: 0.0s\n",
      "| Epoch [0] | Step [107] | lr [0.000200] | Loss: [0.2223] | Acc: [0.6250] | Time: 0.0s\n",
      "| Epoch [0] | Step [108] | lr [0.000200] | Loss: [0.2460] | Acc: [0.6250] | Time: 0.0s\n",
      "| Epoch [0] | Step [109] | lr [0.000200] | Loss: [0.2258] | Acc: [0.6875] | Time: 0.0s\n",
      "| Epoch [0] | Step [110] | lr [0.000200] | Loss: [0.1736] | Acc: [0.7500] | Time: 0.0s\n",
      "| Epoch [0] | Step [111] | lr [0.000200] | Loss: [0.2182] | Acc: [0.5938] | Time: 0.0s\n",
      "| Epoch [0] | Step [112] | lr [0.000200] | Loss: [0.1855] | Acc: [0.7500] | Time: 0.0s\n",
      "| Epoch [0] | Step [113] | lr [0.000200] | Loss: [0.1895] | Acc: [0.6875] | Time: 0.0s\n",
      "| Epoch [0] | Step [114] | lr [0.000200] | Loss: [0.2593] | Acc: [0.5000] | Time: 0.0s\n",
      "| Epoch [0] | Step [115] | lr [0.000200] | Loss: [0.2223] | Acc: [0.5938] | Time: 0.0s\n",
      "| Epoch [0] | Step [116] | lr [0.000200] | Loss: [0.1667] | Acc: [0.7500] | Time: 0.0s\n",
      "| Epoch [0] | Step [117] | lr [0.000200] | Loss: [0.2267] | Acc: [0.6875] | Time: 0.0s\n",
      "| Epoch [0] | Step [118] | lr [0.000200] | Loss: [0.1719] | Acc: [0.8125] | Time: 0.0s\n",
      "| Epoch [0] | Step [119] | lr [0.000200] | Loss: [0.2161] | Acc: [0.6250] | Time: 0.1s\n",
      "| Epoch [0] | Step [120] | lr [0.000200] | Loss: [0.1914] | Acc: [0.7500] | Time: 0.0s\n",
      "| Epoch [0] | Step [121] | lr [0.000200] | Loss: [0.2457] | Acc: [0.5938] | Time: 0.0s\n",
      "| Epoch [0] | Step [122] | lr [0.000200] | Loss: [0.1853] | Acc: [0.6875] | Time: 0.0s\n",
      "| Epoch [0] | Step [123] | lr [0.000200] | Loss: [0.1739] | Acc: [0.7500] | Time: 0.0s\n",
      "| Epoch [0] | Step [124] | lr [0.000200] | Loss: [0.2362] | Acc: [0.5938] | Time: 0.0s\n",
      "| Epoch [0] | Step [125] | lr [0.000200] | Loss: [0.1760] | Acc: [0.6875] | Time: 0.0s\n",
      "| Epoch [0] | Step [126] | lr [0.000200] | Loss: [0.1734] | Acc: [0.7500] | Time: 0.0s\n",
      "| Epoch [0] | Step [127] | lr [0.000200] | Loss: [0.1520] | Acc: [0.8125] | Time: 0.0s\n",
      "| Epoch [0] | Step [128] | lr [0.000200] | Loss: [0.1793] | Acc: [0.7500] | Time: 0.0s\n",
      "| Epoch [0] | Step [129] | lr [0.000200] | Loss: [0.2159] | Acc: [0.6562] | Time: 0.0s\n",
      "| Epoch [0] | Step [130] | lr [0.000200] | Loss: [0.1759] | Acc: [0.7188] | Time: 0.0s\n",
      "| Epoch [0] | Step [131] | lr [0.000200] | Loss: [0.2524] | Acc: [0.5312] | Time: 0.0s\n",
      "| Epoch [0] | Step [132] | lr [0.000200] | Loss: [0.2041] | Acc: [0.6875] | Time: 0.0s\n",
      "| Epoch [0] | Step [133] | lr [0.000200] | Loss: [0.1395] | Acc: [0.8125] | Time: 0.0s\n",
      "| Epoch [0] | Step [134] | lr [0.000200] | Loss: [0.2548] | Acc: [0.5625] | Time: 0.0s\n",
      "| Epoch [0] | Step [135] | lr [0.000200] | Loss: [0.2315] | Acc: [0.5938] | Time: 0.0s\n",
      "| Epoch [0] | Step [136] | lr [0.000200] | Loss: [0.2314] | Acc: [0.5938] | Time: 0.0s\n",
      "| Epoch [0] | Step [137] | lr [0.000200] | Loss: [0.1499] | Acc: [0.7812] | Time: 0.0s\n",
      "| Epoch [0] | Step [138] | lr [0.000200] | Loss: [0.2031] | Acc: [0.5625] | Time: 0.0s\n",
      "| Epoch [0] | Step [139] | lr [0.000200] | Loss: [0.2276] | Acc: [0.7188] | Time: 0.0s\n",
      "| Epoch [0] | Step [140] | lr [0.000200] | Loss: [0.2322] | Acc: [0.6250] | Time: 0.0s\n",
      "| Epoch [0] | Step [141] | lr [0.000200] | Loss: [0.1959] | Acc: [0.6562] | Time: 0.0s\n",
      "| Epoch [0] | Step [142] | lr [0.000200] | Loss: [0.2204] | Acc: [0.6562] | Time: 0.0s\n",
      "| Epoch [0] | Step [143] | lr [0.000200] | Loss: [0.2365] | Acc: [0.6562] | Time: 0.0s\n",
      "| Epoch [0] | Step [144] | lr [0.000200] | Loss: [0.2050] | Acc: [0.6875] | Time: 0.1s\n",
      "| Epoch [0] | Step [145] | lr [0.000200] | Loss: [0.1971] | Acc: [0.7500] | Time: 0.0s\n",
      "| Epoch [0] | Step [146] | lr [0.000200] | Loss: [0.2382] | Acc: [0.5938] | Time: 0.0s\n",
      "| Epoch [0] | Step [147] | lr [0.000200] | Loss: [0.1820] | Acc: [0.7188] | Time: 0.0s\n",
      "| Epoch [0] | Step [148] | lr [0.000200] | Loss: [0.2063] | Acc: [0.6875] | Time: 0.0s\n",
      "| Epoch [0] | Step [149] | lr [0.000200] | Loss: [0.1797] | Acc: [0.7188] | Time: 0.0s\n",
      "| Epoch [0] | Step [150] | lr [0.000200] | Loss: [0.1642] | Acc: [0.6875] | Time: 0.0s\n",
      "| Epoch [0] | Step [151] | lr [0.000200] | Loss: [0.1920] | Acc: [0.7188] | Time: 0.0s\n",
      "| Epoch [0] | Step [152] | lr [0.000200] | Loss: [0.1612] | Acc: [0.7188] | Time: 0.0s\n",
      "| Epoch [0] | Step [153] | lr [0.000200] | Loss: [0.1934] | Acc: [0.7500] | Time: 0.0s\n",
      "| Epoch [0] | Step [154] | lr [0.000200] | Loss: [0.1629] | Acc: [0.7500] | Time: 0.0s\n",
      "| Epoch [0] | Step [155] | lr [0.000200] | Loss: [0.1784] | Acc: [0.7500] | Time: 0.0s\n",
      "| Epoch [0] | Step [156] | lr [0.000200] | Loss: [0.2101] | Acc: [0.6875] | Time: 0.0s\n",
      "| Epoch [0] | Step [157] | lr [0.000200] | Loss: [0.1790] | Acc: [0.7500] | Time: 0.0s\n",
      "| Epoch [0] | Step [158] | lr [0.000200] | Loss: [0.1521] | Acc: [0.7812] | Time: 0.0s\n",
      "| Epoch [0] | Step [159] | lr [0.000200] | Loss: [0.2256] | Acc: [0.5312] | Time: 0.0s\n",
      "| Epoch [0] | Step [160] | lr [0.000200] | Loss: [0.1758] | Acc: [0.6875] | Time: 0.0s\n",
      "| Epoch [0] | Step [161] | lr [0.000200] | Loss: [0.2506] | Acc: [0.6875] | Time: 0.0s\n",
      "| Epoch [0] | Step [162] | lr [0.000200] | Loss: [0.1714] | Acc: [0.6875] | Time: 0.0s\n",
      "| Epoch [0] | Step [163] | lr [0.000200] | Loss: [0.1371] | Acc: [0.8125] | Time: 0.0s\n",
      "| Epoch [0] | Step [164] | lr [0.000200] | Loss: [0.1329] | Acc: [0.8438] | Time: 0.1s\n",
      "| Epoch [0] | Step [165] | lr [0.000200] | Loss: [0.2017] | Acc: [0.6562] | Time: 0.0s\n",
      "| Epoch [0] | Step [166] | lr [0.000200] | Loss: [0.1706] | Acc: [0.8125] | Time: 0.0s\n",
      "| Epoch [0] | Step [167] | lr [0.000200] | Loss: [0.1960] | Acc: [0.7188] | Time: 0.0s\n",
      "| Epoch [0] | Step [168] | lr [0.000200] | Loss: [0.1347] | Acc: [0.8125] | Time: 0.0s\n",
      "| Epoch [0] | Step [169] | lr [0.000200] | Loss: [0.1905] | Acc: [0.6875] | Time: 0.0s\n",
      "| Epoch [0] | Step [170] | lr [0.000200] | Loss: [0.1715] | Acc: [0.6875] | Time: 0.0s\n",
      "| Epoch [0] | Step [171] | lr [0.000200] | Loss: [0.1655] | Acc: [0.7188] | Time: 0.0s\n",
      "| Epoch [0] | Step [172] | lr [0.000200] | Loss: [0.1680] | Acc: [0.7812] | Time: 0.0s\n",
      "| Epoch [0] | Step [173] | lr [0.000200] | Loss: [0.1721] | Acc: [0.7500] | Time: 0.0s\n",
      "| Epoch [0] | Step [174] | lr [0.000200] | Loss: [0.2143] | Acc: [0.5938] | Time: 0.0s\n",
      "| Epoch [0] | Step [175] | lr [0.000200] | Loss: [0.1719] | Acc: [0.7812] | Time: 0.1s\n",
      "| Epoch [0] | Step [176] | lr [0.000200] | Loss: [0.2143] | Acc: [0.7500] | Time: 0.0s\n",
      "| Epoch [0] | Step [177] | lr [0.000200] | Loss: [0.1539] | Acc: [0.8438] | Time: 0.0s\n",
      "| Epoch [0] | Step [178] | lr [0.000200] | Loss: [0.2410] | Acc: [0.5938] | Time: 0.0s\n",
      "| Epoch [0] | Step [179] | lr [0.000200] | Loss: [0.1939] | Acc: [0.6875] | Time: 0.0s\n",
      "| Epoch [0] | Step [180] | lr [0.000200] | Loss: [0.1615] | Acc: [0.8750] | Time: 0.0s\n",
      "| Epoch [0] | Step [181] | lr [0.000200] | Loss: [0.1528] | Acc: [0.7812] | Time: 0.0s\n",
      "| Epoch [0] | Step [182] | lr [0.000200] | Loss: [0.1461] | Acc: [0.8438] | Time: 0.0s\n",
      "| Epoch [0] | Step [183] | lr [0.000200] | Loss: [0.1955] | Acc: [0.5938] | Time: 0.0s\n",
      "| Epoch [0] | Step [184] | lr [0.000200] | Loss: [0.1345] | Acc: [0.8125] | Time: 0.0s\n",
      "| Epoch [0] | Step [185] | lr [0.000200] | Loss: [0.2023] | Acc: [0.7188] | Time: 0.0s\n",
      "| Epoch [0] | Step [186] | lr [0.000200] | Loss: [0.1555] | Acc: [0.7500] | Time: 0.0s\n",
      "| Epoch [0] | Step [187] | lr [0.000200] | Loss: [0.1719] | Acc: [0.7500] | Time: 0.0s\n",
      "| Epoch [0] | Step [188] | lr [0.000200] | Loss: [0.1890] | Acc: [0.6875] | Time: 0.0s\n",
      "| Epoch [0] | Step [189] | lr [0.000200] | Loss: [0.2050] | Acc: [0.6250] | Time: 0.0s\n",
      "| Epoch [0] | Step [190] | lr [0.000200] | Loss: [0.1086] | Acc: [0.8750] | Time: 0.1s\n",
      "| Epoch [0] | Step [191] | lr [0.000200] | Loss: [0.2047] | Acc: [0.6250] | Time: 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch [0] | Step [192] | lr [0.000200] | Loss: [0.1415] | Acc: [0.7812] | Time: 0.0s\n",
      "| Epoch [0] | Step [193] | lr [0.000200] | Loss: [0.1374] | Acc: [0.9375] | Time: 0.1s\n",
      "| Epoch [0] | Step [194] | lr [0.000200] | Loss: [0.1812] | Acc: [0.7188] | Time: 0.0s\n",
      "| Epoch [0] | Step [195] | lr [0.000200] | Loss: [0.1405] | Acc: [0.7500] | Time: 0.0s\n",
      "| Epoch [0] | Step [196] | lr [0.000200] | Loss: [0.1060] | Acc: [0.9062] | Time: 0.0s\n",
      "| Epoch [0] | Step [197] | lr [0.000200] | Loss: [0.1786] | Acc: [0.6875] | Time: 0.1s\n",
      "| Epoch [0] | Step [198] | lr [0.000200] | Loss: [0.1444] | Acc: [0.8750] | Time: 0.0s\n",
      "| Epoch [0] | Step [199] | lr [0.000200] | Loss: [0.1369] | Acc: [0.8438] | Time: 0.0s\n",
      "| Epoch [0] | Step [200] | lr [0.000200] | Loss: [0.1559] | Acc: [0.6875] | Time: 0.0s\n",
      "| Epoch [0] | Step [201] | lr [0.000200] | Loss: [0.1721] | Acc: [0.6562] | Time: 0.0s\n",
      "| Epoch [0] | Step [202] | lr [0.000200] | Loss: [0.1488] | Acc: [0.9062] | Time: 0.0s\n",
      "| Epoch [0] | Step [203] | lr [0.000200] | Loss: [0.1515] | Acc: [0.7500] | Time: 0.0s\n",
      "| Epoch [0] | Step [204] | lr [0.000200] | Loss: [0.1498] | Acc: [0.7812] | Time: 0.0s\n",
      "| Epoch [0] | Step [205] | lr [0.000200] | Loss: [0.1312] | Acc: [0.7812] | Time: 0.0s\n",
      "| Epoch [0] | Step [206] | lr [0.000200] | Loss: [0.1722] | Acc: [0.7188] | Time: 0.0s\n",
      "| Epoch [0] | Step [207] | lr [0.000200] | Loss: [0.1675] | Acc: [0.7188] | Time: 0.0s\n",
      "| Epoch [0] | Step [208] | lr [0.000200] | Loss: [0.1410] | Acc: [0.7812] | Time: 0.0s\n",
      "| Epoch [0] | Step [209] | lr [0.000200] | Loss: [0.1611] | Acc: [0.7812] | Time: 0.0s\n",
      "| Epoch [0] | Step [210] | lr [0.000200] | Loss: [0.1632] | Acc: [0.7188] | Time: 0.0s\n",
      "| Epoch [0] | Step [211] | lr [0.000200] | Loss: [0.1724] | Acc: [0.7812] | Time: 0.0s\n",
      "| Epoch [0] | Step [212] | lr [0.000200] | Loss: [0.1663] | Acc: [0.6250] | Time: 0.0s\n",
      "| Epoch [0] | Step [213] | lr [0.000200] | Loss: [0.1822] | Acc: [0.7812] | Time: 0.1s\n",
      "| Epoch [0] | Step [214] | lr [0.000200] | Loss: [0.1823] | Acc: [0.7500] | Time: 0.0s\n",
      "| Epoch [0] | Step [215] | lr [0.000200] | Loss: [0.2057] | Acc: [0.7812] | Time: 0.0s\n",
      "| Epoch [0] | Step [216] | lr [0.000200] | Loss: [0.1409] | Acc: [0.7500] | Time: 0.0s\n",
      "| Epoch [0] | Step [217] | lr [0.000200] | Loss: [0.1264] | Acc: [0.8438] | Time: 0.0s\n",
      "| Epoch [0] | Step [218] | lr [0.000200] | Loss: [0.1592] | Acc: [0.7812] | Time: 0.0s\n",
      "| Epoch [0] | Step [219] | lr [0.000200] | Loss: [0.1552] | Acc: [0.7812] | Time: 0.0s\n",
      "| Epoch [0] | Step [220] | lr [0.000200] | Loss: [0.1755] | Acc: [0.7500] | Time: 0.0s\n",
      "| Epoch [0] | Step [221] | lr [0.000200] | Loss: [0.1733] | Acc: [0.7188] | Time: 0.0s\n",
      "| Epoch [0] | Step [222] | lr [0.000200] | Loss: [0.1685] | Acc: [0.7500] | Time: 0.0s\n",
      "| Epoch [0] | Step [223] | lr [0.000200] | Loss: [0.1444] | Acc: [0.7500] | Time: 0.0s\n",
      "| Epoch [0] | Step [224] | lr [0.000200] | Loss: [0.1530] | Acc: [0.8438] | Time: 0.0s\n",
      "| Epoch [0] | Step [225] | lr [0.000200] | Loss: [0.1835] | Acc: [0.6875] | Time: 0.0s\n",
      "| Epoch [0] | Step [226] | lr [0.000200] | Loss: [0.1200] | Acc: [0.8125] | Time: 0.0s\n",
      "| Epoch [0] | Step [227] | lr [0.000200] | Loss: [0.1134] | Acc: [0.8750] | Time: 0.0s\n",
      "| Epoch [0] | Step [228] | lr [0.000200] | Loss: [0.2020] | Acc: [0.6562] | Time: 0.0s\n",
      "| Epoch [0] | Step [229] | lr [0.000200] | Loss: [0.1501] | Acc: [0.8125] | Time: 0.0s\n",
      "| Epoch [0] | Step [230] | lr [0.000200] | Loss: [0.1686] | Acc: [0.7812] | Time: 0.0s\n",
      "| Epoch [0] | Step [231] | lr [0.000200] | Loss: [0.1983] | Acc: [0.6562] | Time: 0.0s\n",
      "| Epoch [0] | Step [232] | lr [0.000200] | Loss: [0.1414] | Acc: [0.9375] | Time: 0.0s\n",
      "| Epoch [0] | Step [233] | lr [0.000200] | Loss: [0.1600] | Acc: [0.7500] | Time: 0.0s\n",
      "| Epoch [0] | Step [234] | lr [0.000200] | Loss: [0.1232] | Acc: [0.9062] | Time: 0.0s\n",
      "| Epoch [0] | Step [235] | lr [0.000200] | Loss: [0.1216] | Acc: [0.8750] | Time: 0.0s\n",
      "| Epoch [0] | Step [236] | lr [0.000200] | Loss: [0.1969] | Acc: [0.6875] | Time: 0.0s\n",
      "| Epoch [0] | Step [237] | lr [0.000200] | Loss: [0.1653] | Acc: [0.7500] | Time: 0.0s\n",
      "| Epoch [0] | Step [238] | lr [0.000200] | Loss: [0.0707] | Acc: [1.0000] | Time: 0.0s\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-309-820dc50d492a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msiamese\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.95\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while epoch < args.n_epoch:\n",
    "    siamese.train()\n",
    "    for i,(data, label) in enumerate(train_loader):\n",
    "\n",
    "        #scheduler.step()\n",
    "        start_t = time.time()\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        siamese(data, label)\n",
    "        end_t = time.time()\n",
    "        all_step += 1\n",
    "        print('| Epoch [%d] | Step [%d] | lr [%.6f] | Loss: [%.4f] | Acc: [%.4f] | Time: %.1fs' %\\\n",
    "              ( epoch, all_step, siamese.optim.param_groups[0]['lr'], siamese.loss.item() , siamese.acc.item() ,\n",
    "                end_t - start_t))\n",
    "        \n",
    "#         if siamese.acc.item() > 0.95: # Stop early\n",
    "#             raise StopIteration\n",
    "    \n",
    "    if i % 1 == 0:\n",
    "        siamese.eval()\n",
    "        for j, (v_data, v_label) in enumerate(valid_loader):\n",
    "\n",
    "            start_t = time.time()\n",
    "            v_data = v_data.to(device)\n",
    "            v_label = v_label.to(device)\n",
    "            siamese.test_step(v_data, v_label)\n",
    "            end_t = time.time()\n",
    "            print('| Epoch [%d] | Validation | Step [%d] |  Loss: [%.4f] | Acc: [%.4f] | Time: %.1fs' %\\\n",
    "                  ( epoch, j, siamese.v_loss.item() , siamese.v_acc.item() ,end_t - start_t))\n",
    "                \n",
    "    siamese.plot_all_loss('Training_0reg')\n",
    "    \n",
    "    if epoch >= 1:\n",
    "        plot_train_hist(train_hist, 'Epoch_0reg')\n",
    "\n",
    "    for name in siamese.train_hist.keys():\n",
    "        train_hist[name].append(sum(siamese.train_hist[name])/len(siamese.train_hist[name]))\n",
    "\n",
    "    if not args.running_loss:\n",
    "        for name in siamese.train_hist.keys():\n",
    "            siamese.train_hist[name] = []\n",
    "            \n",
    "    epoch += 1\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        siamese.model_save(epoch)\n",
    "    \n",
    "    if epoch >= 5:\n",
    "        raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### This Algorithm can solve the problem and make the graph in a batch of training data #####\n",
    "\n",
    "class Graph_Alg(object):\n",
    "    \n",
    "    def __init__(self,):\n",
    "        self.G = nx.Graph()\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        paired_data = np.array(x[y.byte()])\n",
    "        for i in paired_data:\n",
    "            for j in i:\n",
    "                for k in range(len(j)):\n",
    "                    if not j[k] in self.G.nodes:\n",
    "                        self.G.add_node(j[k])\n",
    "                    if k >= 1:\n",
    "                        if not (j[k],j[k-1]) in self.G.edges:\n",
    "                            self.G.add_edge(j[k],j[k-1])\n",
    "                            \n",
    "    def draw(self,):\n",
    "        \n",
    "        nx.draw(self.G, with_labels=True, font_weight='bold')\n",
    "        plt.show()\n",
    "        \n",
    "    def test(self, x, y):\n",
    "        \n",
    "        out = []\n",
    "        paired_data = np.array(x)\n",
    "        \n",
    "        for i in paired_data:\n",
    "            out.append(self.check_step(i))\n",
    "        match = list(np.array(out) == np.array(y).astype(int))\n",
    "        self.acc = sum(match) / len(match)\n",
    "             \n",
    "    def check_step(self, i):\n",
    "        \n",
    "        # Check the path is in the graph\n",
    "        for j in i:\n",
    "            for k in range(len(j)):\n",
    "                if not j[k] in self.G.nodes:\n",
    "                    return 0\n",
    "                if k >= 1:\n",
    "                    if not (j[k],j[k-1]) in self.G.edges:\n",
    "                        return 0\n",
    "                        \n",
    "        # Check the the pair is possible to have the same st_node   \n",
    "        for node in list(self.G.adj[i[0][0].item()]):\n",
    "            if node in list(self.G.adj[i[1][0].item()]):\n",
    "                return 1\n",
    "\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_algo = Graph_Alg()\n",
    "algo_train_hist = defaultdict(list)\n",
    "num_steps = 20\n",
    "for i in range(num_steps):\n",
    "    data, label = iter(train_loader).next()\n",
    "\n",
    "    # Training\n",
    "    graph_algo.forward(data, label)\n",
    "    \n",
    "    # Testing\n",
    "    v_data, v_label = iter(valid_loader).next()\n",
    "    graph_algo.test(v_data, v_label)\n",
    "    algo_train_hist['Accuracy'].append(graph_algo.acc)\n",
    "    \n",
    "plot_train_hist(algo_train_hist, 'Graph_algo')\n",
    "graph_algo.draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
