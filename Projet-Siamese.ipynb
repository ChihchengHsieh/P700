{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# Load data\n",
    "with open(\"data.txt\", \"rb\") as fp:   # Unpickling\n",
    "    df = pickle.load(fp)\n",
    "X = df[['left','right']]     \n",
    "Y = df['target']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperate to training, validation, and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 101)\n",
    "validation_size = int(len(X_train) * 0.1)\n",
    "training_size = len(X_train) - validation_size\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=validation_size,random_state= 101)\n",
    "Y_test = Y_test.values\n",
    "Y_train = Y_train.values\n",
    "Y_validation = Y_validation.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 1440\n",
      "Validation size: 160\n",
      "test size: 400\n"
     ]
    }
   ],
   "source": [
    "print('Training size:',X_train.shape[0])\n",
    "print('Validation size:',X_validation.shape[0])\n",
    "print('test size:',X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check shape\n",
    "assert X_train['left'].shape == X_train['right'].shape\n",
    "assert len(X_train['left']) == len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two help function\n",
    "def one_hot(s):\n",
    "    nb_digits=201\n",
    "    batch_size = s.shape[0]\n",
    "    seqlen = s.shape[1]\n",
    "    s_onehot = torch.FloatTensor(batch_size,seqlen,nb_digits)\n",
    "    s_onehot.zero_()\n",
    "    s_onehot.scatter_(2, s.unsqueeze(2), 1)\n",
    "    return s_onehot\n",
    "def padding(data):\n",
    "    left = [] \n",
    "    maxlen= 50\n",
    "    for i in range(data.shape[0]):\n",
    "        diff = maxlen - len((data.iloc[i]['left']))\n",
    "        if diff>=1:\n",
    "            data.iloc[i]['left']+= [0]*diff\n",
    "        left.append((data.iloc[i]['left']))\n",
    "    right = [] \n",
    "    maxlen= 50\n",
    "    for i in range(data.shape[0]):\n",
    "        diff = maxlen - len((data.iloc[i]['right']))\n",
    "        if diff>=1:\n",
    "            data.iloc[i]['right']+= [0]*diff\n",
    "        right.append((data.iloc[i]['right']))\n",
    "    return torch.tensor(np.array([right,left])).transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding and creat the loaders\n",
    "X_train = padding(X_train)\n",
    "Y_train = torch.FloatTensor(np.array(Y_train))\n",
    "train_dataset  = Data.TensorDataset(X_train,Y_train)\n",
    "train_loader = DataLoader(dataset, batch_size=32,shuffle=True)\n",
    "\n",
    "X_validation = padding(X_validation)\n",
    "Y_validation = torch.FloatTensor(np.array(Y_validation))\n",
    "val_dataset  = Data.TensorDataset(X_validation,Y_validation)\n",
    "\n",
    "X_test = padding(X_test)\n",
    "Y_test = torch.FloatTensor(np.array(Y_test))\n",
    "test_dataset  = Data.TensorDataset(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    \"\"\" Implements the network type integrated within the Siamese RNN architecture. \"\"\"\n",
    "    def __init__(self, opt, is_train=False):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.node_size = opt['node_size']\n",
    "        self.name = 'sim_encoder'\n",
    "        self.hidden_size= opt['hidden_size']\n",
    "        self.num_layers= opt['num_layers']\n",
    "        self.embedding_dim = opt['embedding_dim']\n",
    "        self.embedding_table = nn.Embedding(num_embeddings=self.node_size, embedding_dim=self.embedding_dim,\n",
    "                                          padding_idx=0, max_norm=None, scale_grad_by_freq=False, sparse=False)\n",
    "        self.lstm_rnn = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.hidden_size, num_layers=self.num_layers)\n",
    "\n",
    "    def initialize_hidden_plus_cell(self, batch_size):\n",
    "        \"\"\" Re-initializes the hidden state, cell state, and the forget gate bias of the network. \"\"\"\n",
    "        zero_hidden = torch.randn(self.num_layers, batch_size, self.hidden_size)\n",
    "        zero_cell = torch.randn(self.num_layers, batch_size,self.hidden_size)\n",
    "        return zero_hidden, zero_cell\n",
    "\n",
    "    def forward(self, input_data, hidden, cell):\n",
    "        \"\"\" Performs a forward pass through the network. \"\"\"\n",
    "        output = self.embedding_table(input_data)\n",
    "        output, (hidden, cell) = self.lstm_rnn(output, (hidden, cell))\n",
    "        return output[-1], hidden[-1], cell[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseClassifier(nn.Module):\n",
    "    \"\"\" Sentence similarity estimator implementing a siamese arcitecture. Uses pretrained word2vec embeddings. \n",
    "    Different to the paper, the weights are untied, to avoid exploding/ vanishing gradients. \"\"\"\n",
    "    def __init__(self, opt, is_train=False):\n",
    "        super(SiameseClassifier, self).__init__()\n",
    "        self.learning_rate= opt['learning_rate']\n",
    "        # Initialize network\n",
    "        self.encoder_a =  LSTMEncoder(opt, is_train)\n",
    "        # Initialize network parameters\n",
    "        self.initialize_parameters()\n",
    "        # Declare loss function\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        # Initialize network optimizers\n",
    "        self.optimizer_a = optim.Adam(self.encoder_a.parameters(), lr=self.learning_rate,\n",
    "                                      betas=(0.9, 0.999),weight_decay=0.5)\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\" Performs a single forward pass through the siamese architecture. \"\"\"\n",
    "        \n",
    "        # Obtain the input length (each batch consists of padded sentences)\n",
    "        input_length = self.batch_a.size(0)\n",
    "        \n",
    "        # Obtain sentence encodings from each encoder\n",
    "        hidden_a, cell_a = self.encoder_a.initialize_hidden_plus_cell(self.batch_size)\n",
    "        output_a, hidden_a, cell_a = self.encoder_a(self.batch_a, hidden_a, cell_a)\n",
    "\n",
    "        hidden_b, cell_b = self.encoder_a.initialize_hidden_plus_cell(self.batch_size)\n",
    "        output_b, hidden_b, cell_b = self.encoder_a(self.batch_b, hidden_b, cell_b)\n",
    "\n",
    "        # Format sentence encodings as 2D tensors\n",
    "        self.encoding_a = output_a.squeeze()\n",
    "        self.encoding_b = output_b.squeeze()\n",
    "\n",
    "        # Obtain similarity score predictions by calculating the Manhattan distance between sentence encodings\n",
    "        if self.batch_size == 1:\n",
    "            self.prediction = torch.exp(-torch.norm((self.encoding_a - self.encoding_b), 1))\n",
    "        else:\n",
    "            self.prediction = torch.exp(-torch.norm((self.encoding_a - self.encoding_b), 1, 1))\n",
    "            \n",
    "\n",
    "    def get_loss(self):\n",
    "        \"\"\" Calculates the MSE loss between the network predictions and the ground truth. \"\"\"\n",
    "        # Loss is the L1 norm of the difference between the obtained sentence encodings\n",
    "        self.loss = self.loss_function(self.prediction, self.labels)\n",
    "\n",
    "    def load_pretrained_parameters(self,pretrained_state_dict_path):\n",
    "        \"\"\" Loads the parameters learned during the pre-training on the SemEval data. \"\"\"\n",
    "        self.encoder_a.load_state_dict(torch.load(pretrained_state_dict_path))\n",
    "        print('Pretrained parameters have been successfully loaded into the encoder networks.')\n",
    "    \n",
    "    def save_lstm(self,path):\n",
    "        torch.save(self.encoder_a.state_dict(), path)\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\" Initializes network parameters. \"\"\"\n",
    "        state_dict = self.encoder_a.state_dict()\n",
    "        for key in state_dict.keys():\n",
    "            if '.weight' in key:\n",
    "                state_dict[key] = torch.nn.init.xavier_uniform_((state_dict[key]),gain=1)\n",
    "            if '.bias' in key:\n",
    "                bias_length = state_dict[key].size()[0]\n",
    "                start, end = bias_length // 4, bias_length // 2\n",
    "                state_dict[key][start:end].fill_(2.5)\n",
    "        self.encoder_a.load_state_dict(state_dict)\n",
    "\n",
    "    def train_step(self, train_batch_a, train_batch_b, train_labels):\n",
    "        \"\"\" Optimizes the parameters of the active networks, i.e. performs a single training step. \"\"\"\n",
    "        # Get batches\n",
    "        self.batch_a = train_batch_a.transpose(0,1)\n",
    "        self.batch_b = train_batch_b.transpose(0,1)\n",
    "        self.labels = train_labels\n",
    "        self.batch_size = self.batch_a.size(1)\n",
    "        self.encoder_a.zero_grad() \n",
    "        self.forward()\n",
    "        self.get_loss()\n",
    "        self.loss.backward()\n",
    "        clip_grad_norm(self.encoder_a.parameters(), 0.25)\n",
    "        self.optimizer_a.step()\n",
    "\n",
    "    def test_step(self, test_batch_a, test_batch_b, test_labels):\n",
    "        \"\"\" Performs a single test step. \"\"\"\n",
    "        self.batch_a = test_batch_a.transpose(0,1)\n",
    "        self.batch_b = test_batch_b.transpose(0,1)\n",
    "        self.labels = test_labels\n",
    "        self.batch_size = self.batch_a.size(1)\n",
    "        self.forward()\n",
    "        self.get_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richard/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:77: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/Users/richard/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:41: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training batch loss at epoch 0: 0.5090\n",
      "Average validation fold accuracy at epoch 0: 0.5312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richard/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:71: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training batch loss at epoch 1: 0.5090\n",
      "Average validation fold accuracy at epoch 1: 0.5312\n",
      "Average training batch loss at epoch 2: 0.5090\n",
      "Average validation fold accuracy at epoch 2: 0.5312\n",
      "Average training batch loss at epoch 3: 0.5090\n",
      "Average validation fold accuracy at epoch 3: 0.5312\n",
      "Average training batch loss at epoch 4: 0.5090\n",
      "Average validation fold accuracy at epoch 4: 0.5312\n",
      "Average training batch loss at epoch 5: 0.5090\n",
      "Average validation fold accuracy at epoch 5: 0.5312\n",
      "Average training batch loss at epoch 6: 0.5090\n",
      "Average validation fold accuracy at epoch 6: 0.5312\n",
      "Average training batch loss at epoch 7: 0.5090\n",
      "Average validation fold accuracy at epoch 7: 0.5312\n",
      "Average training batch loss at epoch 8: 0.5090\n",
      "Average validation fold accuracy at epoch 8: 0.5312\n",
      "Average training batch loss at epoch 9: 0.5090\n",
      "Average validation fold accuracy at epoch 9: 0.5312\n",
      "Average training batch loss at epoch 10: 0.5090\n",
      "Average validation fold accuracy at epoch 10: 0.5312\n",
      "Average training batch loss at epoch 11: 0.5090\n",
      "Average validation fold accuracy at epoch 11: 0.5312\n",
      "Average training batch loss at epoch 12: 0.5090\n",
      "Average validation fold accuracy at epoch 12: 0.5312\n",
      "Average training batch loss at epoch 13: 0.5090\n",
      "Average validation fold accuracy at epoch 13: 0.5312\n",
      "Average training batch loss at epoch 14: 0.5090\n",
      "Average validation fold accuracy at epoch 14: 0.5312\n",
      "Average training batch loss at epoch 15: 0.5089\n",
      "Average validation fold accuracy at epoch 15: 0.5311\n",
      "Average training batch loss at epoch 16: 0.5089\n",
      "Average validation fold accuracy at epoch 16: 0.5310\n",
      "Average training batch loss at epoch 17: 0.5087\n",
      "Average validation fold accuracy at epoch 17: 0.5308\n",
      "Average training batch loss at epoch 18: 0.5084\n",
      "Average validation fold accuracy at epoch 18: 0.5303\n",
      "Average training batch loss at epoch 19: 0.5076\n",
      "Average validation fold accuracy at epoch 19: 0.5290\n",
      "Average training batch loss at epoch 20: 0.5062\n",
      "Average validation fold accuracy at epoch 20: 0.5272\n",
      "Average training batch loss at epoch 21: 0.5042\n",
      "Average validation fold accuracy at epoch 21: 0.5246\n",
      "Average training batch loss at epoch 22: 0.5014\n",
      "Average validation fold accuracy at epoch 22: 0.5211\n",
      "Average training batch loss at epoch 23: 0.4977\n",
      "Average validation fold accuracy at epoch 23: 0.5169\n",
      "Average training batch loss at epoch 24: 0.4932\n",
      "Average validation fold accuracy at epoch 24: 0.5115\n",
      "Average training batch loss at epoch 25: 0.4879\n",
      "Average validation fold accuracy at epoch 25: 0.5052\n",
      "Average training batch loss at epoch 26: 0.4819\n",
      "Average validation fold accuracy at epoch 26: 0.4983\n",
      "Average training batch loss at epoch 27: 0.4753\n",
      "Average validation fold accuracy at epoch 27: 0.4911\n",
      "Average training batch loss at epoch 28: 0.4685\n",
      "Average validation fold accuracy at epoch 28: 0.4836\n",
      "Average training batch loss at epoch 29: 0.4616\n",
      "Average validation fold accuracy at epoch 29: 0.4762\n",
      "Average training batch loss at epoch 30: 0.4550\n",
      "Average validation fold accuracy at epoch 30: 0.4690\n",
      "Average training batch loss at epoch 31: 0.4486\n",
      "Average validation fold accuracy at epoch 31: 0.4623\n",
      "Average training batch loss at epoch 32: 0.4427\n",
      "Average validation fold accuracy at epoch 32: 0.4560\n",
      "Average training batch loss at epoch 33: 0.4375\n",
      "Average validation fold accuracy at epoch 33: 0.4505\n",
      "Average training batch loss at epoch 34: 0.4332\n",
      "Average validation fold accuracy at epoch 34: 0.4456\n",
      "Average training batch loss at epoch 35: 0.4296\n",
      "Average validation fold accuracy at epoch 35: 0.4418\n",
      "Average training batch loss at epoch 36: 0.4264\n",
      "Average validation fold accuracy at epoch 36: 0.4379\n",
      "Average training batch loss at epoch 37: 0.4234\n",
      "Average validation fold accuracy at epoch 37: 0.4351\n",
      "Average training batch loss at epoch 38: 0.4209\n",
      "Average validation fold accuracy at epoch 38: 0.4330\n",
      "Average training batch loss at epoch 39: 0.4188\n",
      "Average validation fold accuracy at epoch 39: 0.4302\n",
      "Average training batch loss at epoch 40: 0.4169\n",
      "Average validation fold accuracy at epoch 40: 0.4280\n",
      "Average training batch loss at epoch 41: 0.4157\n",
      "Average validation fold accuracy at epoch 41: 0.4266\n",
      "Average training batch loss at epoch 42: 0.4144\n",
      "Average validation fold accuracy at epoch 42: 0.4248\n",
      "Average training batch loss at epoch 43: 0.4133\n",
      "Average validation fold accuracy at epoch 43: 0.4233\n",
      "Average training batch loss at epoch 44: 0.4127\n",
      "Average validation fold accuracy at epoch 44: 0.4220\n",
      "Average training batch loss at epoch 45: 0.4118\n",
      "Average validation fold accuracy at epoch 45: 0.4209\n",
      "Average training batch loss at epoch 46: 0.4116\n",
      "Average validation fold accuracy at epoch 46: 0.4201\n",
      "Average training batch loss at epoch 47: 0.4117\n",
      "Average validation fold accuracy at epoch 47: 0.4194\n",
      "Average training batch loss at epoch 48: 0.4118\n",
      "Average validation fold accuracy at epoch 48: 0.4191\n",
      "Average training batch loss at epoch 49: 0.4122\n",
      "Average validation fold accuracy at epoch 49: 0.4191\n",
      "Average training batch loss at epoch 50: 0.4128\n",
      "Average validation fold accuracy at epoch 50: 0.4193\n",
      "Average training batch loss at epoch 51: 0.4137\n",
      "Average validation fold accuracy at epoch 51: 0.4199\n",
      "Average training batch loss at epoch 52: 0.4147\n",
      "Average validation fold accuracy at epoch 52: 0.4206\n",
      "Average training batch loss at epoch 53: 0.4161\n",
      "Average validation fold accuracy at epoch 53: 0.4215\n",
      "Average training batch loss at epoch 54: 0.4175\n",
      "Average validation fold accuracy at epoch 54: 0.4224\n",
      "Average training batch loss at epoch 55: 0.4188\n",
      "Average validation fold accuracy at epoch 55: 0.4232\n",
      "Average training batch loss at epoch 56: 0.4201\n",
      "Average validation fold accuracy at epoch 56: 0.4240\n",
      "Average training batch loss at epoch 57: 0.4213\n",
      "Average validation fold accuracy at epoch 57: 0.4248\n",
      "Average training batch loss at epoch 58: 0.4225\n",
      "Average validation fold accuracy at epoch 58: 0.4255\n",
      "Average training batch loss at epoch 59: 0.4236\n",
      "Average validation fold accuracy at epoch 59: 0.4262\n",
      "Average training batch loss at epoch 60: 0.4247\n",
      "Average validation fold accuracy at epoch 60: 0.4269\n",
      "Average training batch loss at epoch 61: 0.4258\n",
      "Average validation fold accuracy at epoch 61: 0.4276\n",
      "Average training batch loss at epoch 62: 0.4268\n",
      "Average validation fold accuracy at epoch 62: 0.4282\n",
      "Average training batch loss at epoch 63: 0.4278\n",
      "Average validation fold accuracy at epoch 63: 0.4289\n",
      "Average training batch loss at epoch 64: 0.4288\n",
      "Average validation fold accuracy at epoch 64: 0.4295\n",
      "Average training batch loss at epoch 65: 0.4297\n",
      "Average validation fold accuracy at epoch 65: 0.4301\n",
      "Average training batch loss at epoch 66: 0.4306\n",
      "Average validation fold accuracy at epoch 66: 0.4307\n",
      "Average training batch loss at epoch 67: 0.4315\n",
      "Average validation fold accuracy at epoch 67: 0.4312\n",
      "Average training batch loss at epoch 68: 0.4324\n",
      "Average validation fold accuracy at epoch 68: 0.4318\n",
      "Average training batch loss at epoch 69: 0.4332\n",
      "Average validation fold accuracy at epoch 69: 0.4323\n",
      "Average training batch loss at epoch 70: 0.4340\n",
      "Average validation fold accuracy at epoch 70: 0.4328\n",
      "Average training batch loss at epoch 71: 0.4348\n",
      "Average validation fold accuracy at epoch 71: 0.4333\n",
      "Average training batch loss at epoch 72: 0.4356\n",
      "Average validation fold accuracy at epoch 72: 0.4338\n",
      "Average training batch loss at epoch 73: 0.4363\n",
      "Average validation fold accuracy at epoch 73: 0.4343\n",
      "Average training batch loss at epoch 74: 0.4371\n",
      "Average validation fold accuracy at epoch 74: 0.4347\n",
      "Average training batch loss at epoch 75: 0.4378\n",
      "Average validation fold accuracy at epoch 75: 0.4352\n",
      "Average training batch loss at epoch 76: 0.4385\n",
      "Average validation fold accuracy at epoch 76: 0.4356\n",
      "Average training batch loss at epoch 77: 0.4391\n",
      "Average validation fold accuracy at epoch 77: 0.4360\n",
      "Average training batch loss at epoch 78: 0.4398\n",
      "Average validation fold accuracy at epoch 78: 0.4364\n",
      "Average training batch loss at epoch 79: 0.4404\n",
      "Average validation fold accuracy at epoch 79: 0.4369\n",
      "Average training batch loss at epoch 80: 0.4411\n",
      "Average validation fold accuracy at epoch 80: 0.4372\n",
      "Average training batch loss at epoch 81: 0.4417\n",
      "Average validation fold accuracy at epoch 81: 0.4376\n",
      "Average training batch loss at epoch 82: 0.4423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation fold accuracy at epoch 82: 0.4380\n",
      "Average training batch loss at epoch 83: 0.4429\n",
      "Average validation fold accuracy at epoch 83: 0.4384\n",
      "Average training batch loss at epoch 84: 0.4434\n",
      "Average validation fold accuracy at epoch 84: 0.4387\n",
      "Average training batch loss at epoch 85: 0.4440\n",
      "Average validation fold accuracy at epoch 85: 0.4391\n",
      "Average training batch loss at epoch 86: 0.4445\n",
      "Average validation fold accuracy at epoch 86: 0.4394\n",
      "Average training batch loss at epoch 87: 0.4450\n",
      "Average validation fold accuracy at epoch 87: 0.4398\n",
      "Average training batch loss at epoch 88: 0.4456\n",
      "Average validation fold accuracy at epoch 88: 0.4401\n",
      "Average training batch loss at epoch 89: 0.4461\n",
      "Average validation fold accuracy at epoch 89: 0.4404\n",
      "Average training batch loss at epoch 90: 0.4466\n",
      "Average validation fold accuracy at epoch 90: 0.4407\n",
      "Average training batch loss at epoch 91: 0.4470\n",
      "Average validation fold accuracy at epoch 91: 0.4410\n",
      "Average training batch loss at epoch 92: 0.4475\n",
      "Average validation fold accuracy at epoch 92: 0.4413\n",
      "Average training batch loss at epoch 93: 0.4480\n",
      "Average validation fold accuracy at epoch 93: 0.4416\n",
      "Average training batch loss at epoch 94: 0.4484\n",
      "Average validation fold accuracy at epoch 94: 0.4419\n",
      "Average training batch loss at epoch 95: 0.4489\n",
      "Average validation fold accuracy at epoch 95: 0.4422\n",
      "Average training batch loss at epoch 96: 0.4493\n",
      "Average validation fold accuracy at epoch 96: 0.4424\n",
      "Average training batch loss at epoch 97: 0.4497\n",
      "Average validation fold accuracy at epoch 97: 0.4427\n",
      "Average training batch loss at epoch 98: 0.4501\n",
      "Average validation fold accuracy at epoch 98: 0.4430\n",
      "Average training batch loss at epoch 99: 0.4506\n",
      "Average validation fold accuracy at epoch 99: 0.4432\n",
      "Average training batch loss at epoch 100: 0.4510\n",
      "Average validation fold accuracy at epoch 100: 0.4435\n",
      "Average training batch loss at epoch 101: 0.4513\n",
      "Average validation fold accuracy at epoch 101: 0.4437\n",
      "Average training batch loss at epoch 102: 0.4517\n",
      "Average validation fold accuracy at epoch 102: 0.4440\n",
      "Average training batch loss at epoch 103: 0.4521\n",
      "Average validation fold accuracy at epoch 103: 0.4442\n",
      "Average training batch loss at epoch 104: 0.4525\n",
      "Average validation fold accuracy at epoch 104: 0.4444\n",
      "Average training batch loss at epoch 105: 0.4528\n",
      "Average validation fold accuracy at epoch 105: 0.4447\n",
      "Average training batch loss at epoch 106: 0.4532\n",
      "Average validation fold accuracy at epoch 106: 0.4449\n",
      "Average training batch loss at epoch 107: 0.4535\n",
      "Average validation fold accuracy at epoch 107: 0.4451\n",
      "Average training batch loss at epoch 108: 0.4539\n",
      "Average validation fold accuracy at epoch 108: 0.4453\n",
      "Average training batch loss at epoch 109: 0.4542\n",
      "Average validation fold accuracy at epoch 109: 0.4456\n",
      "Average training batch loss at epoch 110: 0.4546\n",
      "Average validation fold accuracy at epoch 110: 0.4458\n",
      "Average training batch loss at epoch 111: 0.4549\n",
      "Average validation fold accuracy at epoch 111: 0.4460\n",
      "Average training batch loss at epoch 112: 0.4552\n",
      "Average validation fold accuracy at epoch 112: 0.4462\n",
      "Average training batch loss at epoch 113: 0.4555\n",
      "Average validation fold accuracy at epoch 113: 0.4464\n",
      "Average training batch loss at epoch 114: 0.4558\n",
      "Average validation fold accuracy at epoch 114: 0.4466\n",
      "Average training batch loss at epoch 115: 0.4561\n",
      "Average validation fold accuracy at epoch 115: 0.4468\n",
      "Average training batch loss at epoch 116: 0.4564\n",
      "Average validation fold accuracy at epoch 116: 0.4469\n",
      "Average training batch loss at epoch 117: 0.4567\n",
      "Average validation fold accuracy at epoch 117: 0.4471\n",
      "Average training batch loss at epoch 118: 0.4570\n",
      "Average validation fold accuracy at epoch 118: 0.4473\n",
      "Average training batch loss at epoch 119: 0.4573\n",
      "Average validation fold accuracy at epoch 119: 0.4475\n",
      "Average training batch loss at epoch 120: 0.4576\n",
      "Average validation fold accuracy at epoch 120: 0.4477\n",
      "Average training batch loss at epoch 121: 0.4578\n",
      "Average validation fold accuracy at epoch 121: 0.4478\n",
      "Average training batch loss at epoch 122: 0.4581\n",
      "Average validation fold accuracy at epoch 122: 0.4480\n",
      "Average training batch loss at epoch 123: 0.4584\n",
      "Average validation fold accuracy at epoch 123: 0.4482\n",
      "Average training batch loss at epoch 124: 0.4586\n",
      "Average validation fold accuracy at epoch 124: 0.4483\n",
      "Average training batch loss at epoch 125: 0.4589\n",
      "Average validation fold accuracy at epoch 125: 0.4485\n",
      "Average training batch loss at epoch 126: 0.4591\n",
      "Average validation fold accuracy at epoch 126: 0.4487\n",
      "Average training batch loss at epoch 127: 0.4594\n",
      "Average validation fold accuracy at epoch 127: 0.4488\n",
      "Average training batch loss at epoch 128: 0.4596\n",
      "Average validation fold accuracy at epoch 128: 0.4490\n",
      "Average training batch loss at epoch 129: 0.4599\n",
      "Average validation fold accuracy at epoch 129: 0.4491\n",
      "Average training batch loss at epoch 130: 0.4601\n",
      "Average validation fold accuracy at epoch 130: 0.4493\n",
      "Average training batch loss at epoch 131: 0.4603\n",
      "Average validation fold accuracy at epoch 131: 0.4494\n",
      "Average training batch loss at epoch 132: 0.4606\n",
      "Average validation fold accuracy at epoch 132: 0.4496\n",
      "Average training batch loss at epoch 133: 0.4608\n",
      "Average validation fold accuracy at epoch 133: 0.4497\n",
      "Average training batch loss at epoch 134: 0.4610\n",
      "Average validation fold accuracy at epoch 134: 0.4498\n",
      "Average training batch loss at epoch 135: 0.4613\n",
      "Average validation fold accuracy at epoch 135: 0.4500\n",
      "Average training batch loss at epoch 136: 0.4615\n",
      "Average validation fold accuracy at epoch 136: 0.4501\n",
      "Average training batch loss at epoch 137: 0.4617\n",
      "Average validation fold accuracy at epoch 137: 0.4503\n",
      "Average training batch loss at epoch 138: 0.4619\n",
      "Average validation fold accuracy at epoch 138: 0.4504\n",
      "Average training batch loss at epoch 139: 0.4621\n",
      "Average validation fold accuracy at epoch 139: 0.4505\n",
      "Average training batch loss at epoch 140: 0.4623\n",
      "Average validation fold accuracy at epoch 140: 0.4507\n",
      "Average training batch loss at epoch 141: 0.4625\n",
      "Average validation fold accuracy at epoch 141: 0.4508\n",
      "Average training batch loss at epoch 142: 0.4627\n",
      "Average validation fold accuracy at epoch 142: 0.4509\n",
      "Average training batch loss at epoch 143: 0.4629\n",
      "Average validation fold accuracy at epoch 143: 0.4510\n",
      "Average training batch loss at epoch 144: 0.4631\n",
      "Average validation fold accuracy at epoch 144: 0.4512\n",
      "Average training batch loss at epoch 145: 0.4633\n",
      "Average validation fold accuracy at epoch 145: 0.4513\n",
      "Average training batch loss at epoch 146: 0.4635\n",
      "Average validation fold accuracy at epoch 146: 0.4514\n",
      "Average training batch loss at epoch 147: 0.4637\n",
      "Average validation fold accuracy at epoch 147: 0.4515\n",
      "Average training batch loss at epoch 148: 0.4638\n",
      "Average validation fold accuracy at epoch 148: 0.4516\n",
      "Average training batch loss at epoch 149: 0.4640\n",
      "Average validation fold accuracy at epoch 149: 0.4517\n",
      "Average training batch loss at epoch 150: 0.4642\n",
      "Average validation fold accuracy at epoch 150: 0.4519\n",
      "Average training batch loss at epoch 151: 0.4644\n",
      "Average validation fold accuracy at epoch 151: 0.4520\n",
      "Average training batch loss at epoch 152: 0.4646\n",
      "Average validation fold accuracy at epoch 152: 0.4521\n",
      "Average training batch loss at epoch 153: 0.4647\n",
      "Average validation fold accuracy at epoch 153: 0.4522\n",
      "Average training batch loss at epoch 154: 0.4649\n",
      "Average validation fold accuracy at epoch 154: 0.4523\n",
      "Average training batch loss at epoch 155: 0.4651\n",
      "Average validation fold accuracy at epoch 155: 0.4524\n",
      "Average training batch loss at epoch 156: 0.4652\n",
      "Average validation fold accuracy at epoch 156: 0.4525\n",
      "Average training batch loss at epoch 157: 0.4654\n",
      "Average validation fold accuracy at epoch 157: 0.4526\n",
      "Average training batch loss at epoch 158: 0.4655\n",
      "Average validation fold accuracy at epoch 158: 0.4527\n",
      "Average training batch loss at epoch 159: 0.4657\n",
      "Average validation fold accuracy at epoch 159: 0.4528\n",
      "Average training batch loss at epoch 160: 0.4659\n",
      "Average validation fold accuracy at epoch 160: 0.4529\n",
      "Average training batch loss at epoch 161: 0.4660\n",
      "Average validation fold accuracy at epoch 161: 0.4530\n",
      "Average training batch loss at epoch 162: 0.4662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation fold accuracy at epoch 162: 0.4531\n",
      "Average training batch loss at epoch 163: 0.4663\n",
      "Average validation fold accuracy at epoch 163: 0.4532\n",
      "Average training batch loss at epoch 164: 0.4665\n",
      "Average validation fold accuracy at epoch 164: 0.4533\n",
      "Average training batch loss at epoch 165: 0.4666\n",
      "Average validation fold accuracy at epoch 165: 0.4534\n",
      "Average training batch loss at epoch 166: 0.4668\n",
      "Average validation fold accuracy at epoch 166: 0.4535\n",
      "Average training batch loss at epoch 167: 0.4669\n",
      "Average validation fold accuracy at epoch 167: 0.4536\n",
      "Average training batch loss at epoch 168: 0.4671\n",
      "Average validation fold accuracy at epoch 168: 0.4537\n",
      "Average training batch loss at epoch 169: 0.4672\n",
      "Average validation fold accuracy at epoch 169: 0.4537\n",
      "Average training batch loss at epoch 170: 0.4673\n",
      "Average validation fold accuracy at epoch 170: 0.4538\n",
      "Average training batch loss at epoch 171: 0.4675\n",
      "Average validation fold accuracy at epoch 171: 0.4539\n",
      "Average training batch loss at epoch 172: 0.4676\n",
      "Average validation fold accuracy at epoch 172: 0.4540\n",
      "Average training batch loss at epoch 173: 0.4677\n",
      "Average validation fold accuracy at epoch 173: 0.4541\n",
      "Average training batch loss at epoch 174: 0.4679\n",
      "Average validation fold accuracy at epoch 174: 0.4542\n",
      "Average training batch loss at epoch 175: 0.4680\n",
      "Average validation fold accuracy at epoch 175: 0.4543\n",
      "Average training batch loss at epoch 176: 0.4681\n",
      "Average validation fold accuracy at epoch 176: 0.4543\n",
      "Average training batch loss at epoch 177: 0.4683\n",
      "Average validation fold accuracy at epoch 177: 0.4544\n",
      "Average training batch loss at epoch 178: 0.4684\n",
      "Average validation fold accuracy at epoch 178: 0.4545\n",
      "Average training batch loss at epoch 179: 0.4685\n",
      "Average validation fold accuracy at epoch 179: 0.4546\n",
      "Average training batch loss at epoch 180: 0.4686\n",
      "Average validation fold accuracy at epoch 180: 0.4547\n",
      "Average training batch loss at epoch 181: 0.4688\n",
      "Average validation fold accuracy at epoch 181: 0.4547\n",
      "Average training batch loss at epoch 182: 0.4689\n",
      "Average validation fold accuracy at epoch 182: 0.4548\n",
      "Average training batch loss at epoch 183: 0.4690\n",
      "Average validation fold accuracy at epoch 183: 0.4549\n",
      "Average training batch loss at epoch 184: 0.4691\n",
      "Average validation fold accuracy at epoch 184: 0.4550\n",
      "Average training batch loss at epoch 185: 0.4692\n",
      "Average validation fold accuracy at epoch 185: 0.4550\n",
      "Average training batch loss at epoch 186: 0.4694\n",
      "Average validation fold accuracy at epoch 186: 0.4551\n",
      "Average training batch loss at epoch 187: 0.4695\n",
      "Average validation fold accuracy at epoch 187: 0.4552\n",
      "Average training batch loss at epoch 188: 0.4696\n",
      "Average validation fold accuracy at epoch 188: 0.4552\n",
      "Average training batch loss at epoch 189: 0.4697\n",
      "Average validation fold accuracy at epoch 189: 0.4553\n",
      "Average training batch loss at epoch 190: 0.4698\n",
      "Average validation fold accuracy at epoch 190: 0.4554\n",
      "Average training batch loss at epoch 191: 0.4699\n",
      "Average validation fold accuracy at epoch 191: 0.4555\n",
      "Average training batch loss at epoch 192: 0.4700\n",
      "Average validation fold accuracy at epoch 192: 0.4555\n",
      "Average training batch loss at epoch 193: 0.4701\n",
      "Average validation fold accuracy at epoch 193: 0.4556\n",
      "Average training batch loss at epoch 194: 0.4702\n",
      "Average validation fold accuracy at epoch 194: 0.4557\n",
      "Average training batch loss at epoch 195: 0.4703\n",
      "Average validation fold accuracy at epoch 195: 0.4557\n",
      "Average training batch loss at epoch 196: 0.4705\n",
      "Average validation fold accuracy at epoch 196: 0.4558\n",
      "Average training batch loss at epoch 197: 0.4706\n",
      "Average validation fold accuracy at epoch 197: 0.4559\n",
      "Average training batch loss at epoch 198: 0.4707\n",
      "Average validation fold accuracy at epoch 198: 0.4559\n",
      "Average training batch loss at epoch 199: 0.4708\n",
      "Average validation fold accuracy at epoch 199: 0.4560\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "pretrain=False\n",
    "opt = {\n",
    "    'node_size':201,\n",
    "    'hidden_size':50,\n",
    "    'num_layers':1,\n",
    "    'embedding_dim':100,\n",
    "    'learning_rate':0.001\n",
    "}\n",
    "\n",
    "# Initialize global tracking variables\n",
    "best_validation_accuracy = 0\n",
    "epochs_without_improvement = 0\n",
    "total_train_loss = list()\n",
    "total_valid_loss = []\n",
    "avg_trainings = []\n",
    "avg_valids = []\n",
    "\n",
    "\n",
    "# Loading model\n",
    "if pretrain:\n",
    "    classifier = torch.load('SiameseNN1.pt')\n",
    "else:\n",
    "    classifier = SiameseClassifier(opt, is_train=True)\n",
    "    # Initialize parameters\n",
    "    classifier.initialize_parameters()\n",
    "\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Initiate the training data loader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32,shuffle=True)\n",
    "    running_loss = list()\n",
    "    # Training loop\n",
    "    for i, (batch_x,label_var) in enumerate(train_loader):\n",
    "        s1_var = batch_x[:,0,:]\n",
    "        s2_var  = batch_x[:,1,:]\n",
    "        #s1_var = one_hot(s1_var)\n",
    "        #s2_var = one_hot(s2_var)\n",
    "        classifier.train_step(s1_var, s2_var, label_var)\n",
    "        train_batch_loss = classifier.loss.data[0]\n",
    "        running_loss.append(train_batch_loss)\n",
    "        total_train_loss.append(train_batch_loss)\n",
    "\n",
    "        if i % 50 == 0 and i != 0:\n",
    "            running_avg_loss = sum(running_loss) / len(running_loss)\n",
    "            print('Epoch: %d | Training Batch: %d | Average loss: %.4f' %\n",
    "                  (epoch, i , running_avg_loss))\n",
    "            running_loss = []\n",
    "            \n",
    "\n",
    "    # Report epoch statistics\n",
    "    avg_training_accuracy = sum(total_train_loss) / len(total_train_loss)\n",
    "    print('Average training batch loss at epoch %d: %.4f' % (epoch, avg_training_accuracy))\n",
    "    avg_trainings.append(avg_training_accuracy) \n",
    "    \n",
    "\n",
    "    # Validate after each epoch; set tracking variables\n",
    "    if epoch >= 0:\n",
    "        # Initiate the training data loader\n",
    "        valid_loader = DataLoader(val_dataset, batch_size=32,shuffle=True)\n",
    "        \n",
    "        # Validation loop (i.e. perform inference on the validation set)\n",
    "        for i, (batch_x,label_var) in enumerate(valid_loader):\n",
    "            s1_var = batch_x[:,0,:]\n",
    "            s2_var  = batch_x[:,1,:]\n",
    "            #s1_var = one_hot(s1_var)\n",
    "            #s2_var = one_hot(s2_var)\n",
    "            # Get predictions and update tracking values\n",
    "            classifier.test_step(s1_var, s2_var, label_var)\n",
    "            valid_batch_loss = classifier.loss.data[0]\n",
    "            total_valid_loss.append(valid_batch_loss)\n",
    "\n",
    "        # Report fold statistics\n",
    "        avg_valid_accuracy = sum(total_valid_loss) / len(total_valid_loss)\n",
    "        print('Average validation fold accuracy at epoch %d: %.4f' % (epoch, avg_valid_accuracy))\n",
    "        avg_valids.append(avg_valid_accuracy)\n",
    "        # Save network parameters if performance has improved\n",
    "        if avg_valid_accuracy <= best_validation_accuracy:\n",
    "            epochs_without_improvement += 1\n",
    "        else:\n",
    "            best_validation_accuracy = avg_valid_accuracy\n",
    "            epochs_without_improvement = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a36bb2898>"
      ]
     },
     "execution_count": 653,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIsAAAEyCAYAAAB6clB0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3Xt81NWd//H3yWRCRi4ZQRSTgFBL4wUigXhbXKtQjdgFsuhSdXvRbuu2XcrWrmlhL5Sl2x9paUul24u21Xa3F0qVohRbasHWFnUhCAYUUbySgIJgguIkmSTn98ckITP5TvKd+yTzej4ePMicnO93ThAh3zef8znGWisAAAAAAABAkvIyvQAAAAAAAABkD8IiAAAAAAAA9CAsAgAAAAAAQA/CIgAAAAAAAPQgLAIAAAAAAEAPwiIAAAAAAAD0ICwCAAAAAABAD8IiAAAAAAAA9CAsAgAAAAAAQI/8TC8g0hlnnGEnTpyY6WUAAAAAAAAMGTt37nzTWjvWzdysC4smTpyourq6TC8DAAAAAABgyDDGvOp2LtvQAAAAAAAA0IOwCAAAAAAAAD0IiwAAAAAAANAj63oWAQAAAAAAJEswGFRDQ4NaWloyvZS0KCwsVGlpqbxeb9z3ICwCAAAAAABDVkNDg0aOHKmJEyfKGJPp5aSUtVbHjh1TQ0ODJk2aFPd92IYGAAAAAACGrJaWFo0ZM2bIB0WSZIzRmDFjEq6iIiwCAAAAAABDWi4ERd2S8bWyDS0FNuxq1KrN+3WoKaBiv081VWWSFPdYdUVJ0u+ZqbHqipLU/uIDAAAAAICEGGttptcQprKy0tbV1WV6GXHbsKtRS9fvUSDY0TPmzTOSkYIdNuYxn9ejG2aU6IGdjUm7Z6bGfF6PVi6YKokACgAAAACQHvv27dP555+fsfe/6qqrtHTpUlVVVfWMfetb39Lzzz+v7373u47XjBgxQu+8807c7+n0NRtjdlprK91cT1iUZDNrt6qxKZDUexojZdl/priNGOZRW4dVW3tnz5jHhMrk2jtPfZH5eaExt2Hao88dJUACAAAAAPQRa1jktLMnkWfMu+++W08++aTuu+++nrHLLrtMq1at0l//9V87XpPpsIieRUl2KMlBkTR0giJJeqe1IywokqQOq7CgSJLaO8NDIUkKdto+Y4Fgh3725GtqbArISmpsCmjp+j3asKsxJesHAAAAAAxd3buFkvmMeeONN+o3v/mNWltbJUmvvPKKDh06pGnTpmn27NmaPn26pk6dqgcffDBJX0Xi6FmUZMV+X9Iri/KM1DmEAqNki/ylCQQ7tGrzfqqLAAAAAABh/nPjM3r20Imon9/1WpPaOsILHALBDn3h/nr9YvtrjtdcUDxKX5p7YdR7jhkzRpdccol+97vfaf78+Vq7dq0+9KEPyefz6de//rVGjRqlN998U5dddpnmzZuXFc24qSxKspqqMvm8nrAxb56R12PiGvN5Pbrl0glJvWemxnxej04/zat0aGwKaGbtVk1askkza7dSaQQAAAAAGFBkUDTQuFs333yz1q5dK0lau3atbr75Zllr9a//+q8qLy/XBz7wATU2NuqNN95I6H2ShcqiJOuuZkl2A+fKc0ZnzYlmiY4lswG4Ud/Kom7dFV7dZYO9//sAAAAAAHJPfxVAUvQ+xCV+n375j5fH/b7V1dX6/Oc/r6eeekqBQEDTp0/Xj3/8Yx09elQ7d+6U1+vVxIkT1dLSEvd7JBMNrpF2Ts3CpPgCqKvPG9vnpLhoSvw+bVsyK3VfGAAAAAAg68TS4NrphPPuk70TLT5YuHChnn/+eVVXV2v58uW66667dODAAX3729/Wo48+qlmzZunll1/WxIkTM97gmsoipF11RYnj/2TxjkVWXUXrGZWK5uMAAAAAgKEj2m6hZOxSufnmm7VgwYKe7Wh///d/r7lz56qyslLTpk3Teeedl/B7JAuVRRhyopUNnubN0+nDhyX9f3gAAAAAQPaKpbJoqEi0sogG1xhynJqMS9K7wc6kHn8IAAAAAMBQRFiEIae6okQrF0xVid8no1CvIqdT2ALBDq3avD/9CwQAAAAAIIvRswhDUmRfpElLNjnOo48RAAAAAADhqCxCTij2+xzHxxUVpnklAAAAAABkN8Ii5IRofYystfqrlVs0ackmzazdSg8jAAAAAEDOYxsacoLT8YcTRvv0xEvHe+Z0N73uPR8AAAAAgFxDWIScEdnHaGbt1j5zupteExYBAAAAAJLh2LFjmj17tiTp9ddfl8fj0dixYyVJ27dvV0FBwYD3uO2227RkyRKVlZWldK3dCIuQs6I1t6bpNQAAAADksPp10pYVUnODVFQqzV4mlS+M+3ZjxozR7t27JUnLly/XiBEjdOedd4bNsdbKWqu8POduQffdd1/c7x8PehYhZ0Vreh1tHAAAAAAwxNWvkzYulpoPSrKhnzcuDo0n2YEDBzRlyhR96lOf0vTp03X48GHdfvvtqqys1IUXXqgVK1b0zL3iiiu0e/dutbe3y+/3a8mSJbrooot0+eWX68iRI0lfG5VFyFk1VWVaun6PAsGOnjEj6XMfmJy5RQEAAAAAUue3S6TX90T/fMMOqaM1fCwYkB5cJO38ifM146ZKc2rjWs6zzz6r++67T9///vclSbW1tRo9erTa29t19dVX68Ybb9QFF1wQdk1zc7Pe//73q7a2Vp///Od17733asmSJXG9fzRUFiFnVVeUaOWCqSrx+2QkjRleICupvqE500sDAAAAAGRCZFA00HiCzj33XF188cU9r3/xi19o+vTpmj59uvbt26dnn322zzU+n09z5syRJM2YMUOvvPJK0tdFZRFyWmTT669selY/+PPL2rTnsN462aZiv081VWU0vAYAAACAoWCgCqDVU7q2oEUoGi/dtinpyxk+fHjPxy+88ILuuusubd++XX6/Xx/+8IfV0tLS55reDbE9Ho/a29uTvi4qi4Beys4aKWOk4yfbZCU1NgW0dP0ebdjVmOmlAQAAAABSbfYyyRvRx9brC42n2IkTJzRy5EiNGjVKhw8f1ubNm1P+ntG4CouMMdcZY/YbYw4YY/pshDPG3GqMOWqM2d314xNd49OMMU8YY54xxtQbYz6U7C8ASKbVf3hB1oaPBYIdWrV5f2YWBAAAAABIn/KF0tw1oUoimdDPc9ckdBqaW9OnT9cFF1ygKVOm6JOf/KRmzpyZ8veMxtjIJ+PICcZ4JD0v6RpJDZJ2SLrZWvtsrzm3Sqq01i6KuPZ9kqy19gVjTLGknZLOt9Y2RXu/yspKW1dXF+eXAyRm0pJNcvo/wkh6ufaD6V4OAAAAACBB+/bt0/nnn5/pZaSV09dsjNlpra10c72byqJLJB2w1r5krW2TtFbSfDc3t9Y+b619oevjQ5KOSBrr5logE4r9vpjGAQAAAAAYatyERSWSend3augai3RD11az+40x4yM/aYy5RFKBpBfjWimQBjVVZfJ5PWFj+XlGNVVlGVoRAAAAAADp5SYsMg5jkTt1NkqaaK0tl/QHST8Ju4ExZ0v6X0m3WWs7+7yBMbcbY+qMMXVHjx51t3IgBaorSrRywVSV+H0yknxej6y1mjben+mlAQAAAADiNFALnqEkGV+rm7CoQVLvSqFSSYciFnLMWtva9fIHkmZ0f84YM0rSJkn/bq190ukNrLX3WGsrrbWVY8eySw2ZVV1Rom1LZunl2g/qTzVXqdDr0X9t2pfpZQEAAAAA4lBYWKhjx47lRGBkrdWxY8dUWFiY0H3yXczZIWmyMWaSpEZJN0m6pfcEY8zZ1trDXS/nSdrXNV4g6deS/sda+6uEVgpkwJmjCrVo1mR99XfP6bHnj+rK9xFmAgAAAMBgUlpaqoaGBuXKTqbCwkKVlpYmdI8BwyJrbbsxZpGkzZI8ku611j5jjFkhqc5a+5CkxcaYeZLaJR2XdGvX5QslXSlpTNeJaZJ0q7V2d0KrBtLo41dM1A///KI+/uMd6ui0Kvb7VFNVpuoKp9ZdAAAAAIBs4vV6NWnSpEwvY1Ax2VaGVVlZaevq6jK9DKDHhl2N+sL99WrrONVuy+f1aOWCqQRGAAAAAIBBwRiz01pb6Waum55FQE5btXl/WFAkSYFgh1Zt3p+hFQEAAAAAkDqERcAADjUFYhoHAAAAAGAwIywCBlDs98U0DgAAAADAYEZYBAygpqpMPq8nbMzrMaqpKsvQigAAAAAASJ0BT0MDcl13E+tVm/frUFNA+R6jUYX5mntRcYZXBgAAAABA8hEWAS5UV5T0hEa/23tYn/rpU9r8zOu6furZGV4ZAAAAAADJxTY0IEbXXDBOE8ecprv/9KKstZleDgAAAAAASUVYBMTIk2f0ySvfo6cbmvXkS8czvRwAAAAAAJKKsAiIww3TS3XGiALd/diLmV4KAAAAAABJRc8iIA6FXo8+dvlEfeOR53XJV/6go2+3qtjvU01VWU9vIwAAAAAABiPCIiBOY0YUSJKOvN0qSWpsCmjp+j2SRGAEAAAAABi02IYGxOk7j/bdghYIdmjV5v0ZWA0AAAAAAMlBWATE6VBTIKZxAAAAAAAGA8IiIE7Ffl9M4wAAAAAADAb0LEqF+nXSlhVSc4NUVCrNXhYaj3esfGHy75mpsVi+lvKFyf3vkmQ1VWVaun6PAsGOnjGf16OaqrIMrgoAAAAAgMQYa22m1xCmsrLS1tXVZXoZ8atfJ21cLAV7bUXK80rGSB1tsY95fdJFt0hP/zx598zUWCxfS/fcF36f1aHShl2N+trvntOh5hYVevNUu6Cc5tYAAAAAgKxjjNlpra10NZewKMlWT5GaDyb5pkZSdv13ipvJk2xnfNfGEiqlOUD6r988q5888YqeWDpbZ4wYltb3BgAAAABgILGERfQsSrbmhhTcdIgERVL8QZEkdQbDgyIpVKFU96OugM6Gft64WPrN50PB3XJ/6Of6dQkteyAfuni8gh1Wv36qMaXvAwAAAABAqhEWJVtRafLvaTzJv2emmDT8losWIKUwMJp81khNn+DX2h2vKduq9QAAAAAAiAVhUbLNXhbaGtVbnlfyFMQ35vVJM25N7j0zNeb1STNuc/e1yCipgoFQr6MUuuniCXrx6Ek99dpbKX0fAAAAAABSibAo2coXSnPXSEXjJZnQz9XfleZ/J76xuWukv/lmcu+ZqbFYvpbKjyc/VGo+mNKtaR8sP1vDCzxauz3ZPasAAAAAAEgfGlwje9Wv63vymRQ+NvnavqeruW0I7vWFgqskNsNe8kC9Htx9SNv/bbZGFnqTdl8AAAAAABLBaWjILZGhkmOAFEXReOmOvUlbyq7X3tLffvdx+X1eNQeCKvb7VFNVpuqKkqS9BwAAAAAAsYolLMpP9WKAlCtf2Lc6aMJl4QFSc5StYUk+ve6VN0/KSGoKBCVJjU0BLV2/R5IIjAAAAAAAgwKVRcgNq6c4B0bDRkmFReFb3RLYljazdqsam/pWNJX4fdq2ZFbc9wUAAAAAIBGxVBbR4Bq5wemUOklqPdEVItnQzxsXJ9T4+pBDUNTfOAAAAAAA2YawCLnB6ZS6Qn/fecFAaPtanIr9DoFUP+MAAAAAAGQbwiLkjvKFoWbWy5tCP7c0O89LoI9RTVWZfF5P2JjP61FNVVnc9wQAAAAAIJ0Ii5C7ikpjG3ehuqJEKxdMVXFRoSTptAKPVi6YSnNrAAAAAMCgQViE3BWtj9G0Dyd02+qKEj2+dLZumF4qjzG6bsq4hO4HAAAAAEA6ERYhd0X2MRpVLJ12hvT4Gukb50nL/aFT1OJseP23FSV6u7VdW587ktx1AwAAAACQQvmZXgCQUeULQz+6bfu29Mi/S8GTodfdJ6R1z43B5eeO0Zkjh2nDrkZdP/XsJC0YAAAAAIDUorII6G373X3H4jwhzZNnNPeiYv1x/1E1vxtMwuIAAAAAAEg9V2GRMeY6Y8x+Y8wBY8wSh8/faow5aozZ3fXjE70+9zFjzAtdPz6WzMUDSRftJLQ4T0irnlaito5OPbz3cAKLAgAAAAAgfQYMi4wxHknfkTRH0gWSbjbGXOAw9ZfW2mldP37Yde1oSV+SdKmkSyR9yRhzetJWDyRbkk9Im1IySu8ZO1y/3tWYwKIAAAAAAEgfN5VFl0g6YK19yVrbJmmtpPku718l6RFr7XFr7VuSHpF0XXxLBdLA8YQ0I13Vp6DOFWOM/nZaiba/fFyNTYHE1wcAAAAAQIq5CYtKJB3s9bqhayzSDcaYemPM/caY8bFca4y53RhTZ4ypO3r0qMulAykQeULa8LGSrPTGM3Hfcv600G/5Od96TJOWbNLM2q3aQKURAAAAACBLuQmLjMOYjXi9UdJEa225pD9I+kkM18pae4+1ttJaWzl27FgXSwJSqHyhdMdeaXmTVHNAuviT0pPflV7+c1y3e+q1t2SMdKKlXVZSY1NAS9fvITACAAAAAGQlN2FRg6TxvV6XSjrUe4K19pi1trXr5Q8kzXB7LZD1rvnPUIXR/8yXlvul1VOk+nWuL1+1eb9sREQaCHZo1eb9SV4oAAAAAACJcxMW7ZA02RgzyRhTIOkmSQ/1nmCMObvXy3mS9nV9vFnStcaY07saW1/bNQYMHs9tklpOSLZDkpWaD0obF7sOjA5F6VUUbRwAAAAAgEwaMCyy1rZLWqRQyLNP0jpr7TPGmBXGmHld0xYbY54xxjwtabGkW7uuPS7pywoFTjskregaAwaPLSukjtbwsWAgNO5CsT+yYXb/4wAAAAAAZFK+m0nW2oclPRwxtqzXx0slLY1y7b2S7k1gjUBmNTfENh6hpqpMS9fvUSDY0TPm83pUU1WWjNUBAAAAAJBUbrahAbmtqDS28QjVFSVauWCqzi4qlCQNH+bRygVTVV3hdKggAAAAAACZRVgEDGT2MskbuWXMSFf/q+tbVFeU6Imls3XdheN0WkG+5l5UnNw1AgAAAACQJIRFwEDKF0pz10hF4yWZ0MloslLgrZhvNfeiYh19u1X/9/KxpC8TAAAAAIBkICwC3ChfKN2xV1reJNUckM6dJf3pazEHRrPOO1OnFXi08enDKVooAAAAAACJISwC4nHNl6WWZunP34jpMl+BR9dccJZ+u/ew2to7U7Q4AAAAAADiR1gExGPcFGnaLdIT35W+cZ603C+tniLVrxvw0nkXFavp3aC2HXgzDQsFAAAAACA2hEVAvMZdJNkO6e3DkqzUfFDauHjAwOivJ4/VqMJ8bXz6UHrWCQAAAABADAiLgHg98e2+Y8GAtGVFv5cV5OdpzpSztfmZ19US7EjR4gAAAAAAiA9hERCv5obYxnuZN61YJ9s69OhzR5K8KAAAAAAAEkNYBMSrqDS28V4ue88YjRjm0R3rdmvSkk2aWbtVG3Y1JnmBAAAAAADEjrAIiNfsZZLXFz7m9YXGB7Dx6UMKBDvVEuyUldTYFNDS9XsIjAAAAAAAGUdYBMSrfKE0d41UNL5rwEjXfiU0PoBVm/ero9OGjQWCHVq1eX8KFgoAAAAAgHuERUAiyhdKd+yV/mmHJCu984aryw41BWIaBwAAAAAgXQiLgGQY+z6p7IPS9nuktpMDTi/2+2IaBwAAAAAgXQiLgGS54nNS4C3pqf8dcGpNVZl8Xk/YmM/rUU1VWapWBwAAAACAK4RFQLKMv0SacLn0xH9LHcF+p1ZXlGjlgqkq9hdKkgrz87RywVRVV5SkY6UAAAAAAERFWAQk08zPSc0Hpa+/T1rul1ZPkerXOU6trijR40tm67aZE9Upafb5Z6Z3rQAAAAAAOCAsApKppVmSkQLHJdlQcLRxcdTASJKun3q22to7tfW5I2lbJgAAAAAA0RAWAcm09cuSbPhYMCBtWRH1khkTTtdZo4bp4T2HU7s2AAAAAABcICwCkqm5IbZxSXl5RnOmnK0/7j+qk63tKVoYAAAAAADuEBYByVRUGtt4lzlTxqmVrWgAAAAAgCxAWAQk0+xlktcXPub1hcb7UTlxtMaOZCsaAAAAACDzCIuAZCpfKM1dIxWND702edIHV4fG++HJM7ruwnF6dP8RvdvGVjQAAAAAQOYQFgHJVr5QumOvdPMvJdspFZzm6rLrp56tlmCnHn3uaIoXCAAAAABAdIRFQKpMviZUYbTjR66mXzJptM4YUaCH97IVDQAAAACQOYRFQKrkeaQZH5Ne/pP05oEBp3vyjN531gg9XH9Yk5Zs0szardqwqzENCwUAAAAA4BTCIiCVKj4q5eVLO+8bcOqGXY2qe6VJVpKV1NgU0NL1ewiMAAAAAABpRVgEpNLIs6Tz/kba/TMpGOh36qrN+9XW0Rk2Fgh2aNXm/alcIQAAAAAAYQiLgFSr/LgUeEv65gXScr+0eopUv67PtENNzmFStHEAAAAAAFKBsAhItXfekGSkwHFJVmo+KG1c3CcwKvb7HC+PNg4AAAAAQCoQFgGptmWFQl2IegkGusZPqakqk8/rCRvzeT2qqSpL8QIBAAAAADglP9MLAIa85gZX49UVJZJCvYsamwIyklbMv7BnHAAAAACAdHBVWWSMuc4Ys98Yc8AYs6SfeTcaY6wxprLrtdcY8xNjzB5jzD5jzNJkLRwYNIpKXY9XV5Ro25JZ+snHL5GVdPppBaldGwAAAAAAEQYMi4wxHknfkTRH0gWSbjbGXOAwb6SkxZL+r9fw30kaZq2dKmmGpH80xkxMfNnAIDJ7meSN6Dvk9YXGo/irc8eoyOfVw3sOp3hxAAAAAACEc1NZdImkA9bal6y1bZLWSprvMO/Lkr4mqaXXmJU03BiTL8knqU3SicSWDAwy5QuluWtOVRLlF4Zely+MeonXk6drLzhLj+x7Q63tHWlaKAAAAAAA7sKiEkkHe71u6BrrYYypkDTeWvubiGvvl3RS0mFJr0n6urX2eOQbGGNuN8bUGWPqjh49Gsv6gcGhfKF0xzPSFZ+XOtqkSVcOeMn15Wfr7ZZ2bTvwZhoWCAAAAABAiJuwyDiM9RztZIzJk7Ra0r84zLtEUoekYkmTJP2LMeY9fW5m7T3W2kprbeXYsWNdLRwYlKb9vWQ7pfpfDjh15rlnaFRhvjbVv56GhQEAAAAAEOImLGqQNL7X61JJh3q9HilpiqQ/GmNekXSZpIe6mlzfIul31tqgtfaIpG2SKpOxcGBQOuO90vhLpd0/l6ztd2pBfp6uuWCcHnn2dbW1d6ZpgQAAAACAXOcmLNohabIxZpIxpkDSTZIe6v6ktbbZWnuGtXaitXaipCclzbPW1im09WyWCRmuUJD0XNK/CmAwmXaLdPQ5qfGpAadeP3WcTrS0a9uLbEUDAAAAAKTHgGGRtbZd0iJJmyXtk7TOWvuMMWaFMWbeAJd/R9IISXsVCp3us9bWJ7hmYHC78G+lfJ+0+2cDTr1i8hkaOSxfv+VUNAAAAABAmhg7wFaYdKusrLR1dXWZXgaQWg98Unphs/Qvz0vewn6n3vi9bdr5WpNkpWK/TzVVZaquKOn3GgAAAAAAejPG7LTWumoN5GYbGoBkm3aL1NIs7d/U77QNuxpV33BC1oa6yjc2BbR0/R5t2NWYnnUCAAAAAHIOYRGQCZPeLxWeLm34jLTcL62eItWv6zNt1eb9ausIb24dCHZo1eb96VopAAAAACDH5Gd6AUBO2nu/1Pa21Nkeet18UNq4OPRx+cKeaYeaAo6XRxsHAAAAACBRVBYBmbBlxamgqFswEBrvpdjvc7w82jgAAAAAAIkiLAIyobnB1XhNVZl8Xk/YmM/rUU1VWapWBgAAAADIcYRFQCYUlboar64o0coFU1XSVUlkJP3nvAs5DQ0AAAAAkDKERUAmzF4meSO2knl9ofEI1RUl2rZkln7+iUtlJQ0fRqsxAAAAAEDqEBYBmVC+UJq7RioaH3pt8qQPfiusuXWkS98zRmNHDtPGpw+laZEAAAAAgFxEWARkSvlC6Y690k0/l2yndNrofqd78ow+OPVsbd1/RCdagmlaJAAAAAAg1xAWAZn23msk32jp6V8MOHXetGK1tXfq98+8kYaFAQAAAAByEWERkGn5BdKUG6T9D0stzf1OrRjvV+npPj3EVjQAAAAASK36ddLqKdJyf+jn+nWZXlHa0CkXyAYX3STt+IH07EPS9I9EnWaM0dyLinXPYy/p2DutGjNiWBoXCQAAAABZoH6dtGWF1NwQOlG6+6AgN2PlC91dP/la6emfS8FA6HPNB6WNi0Mf99Nrdqgw1tpMryFMZWWlraury/QygPSyVvr2DGlUsXTrb/qduu/wCc2568/6cvUUfeSyc9K0QAAAAABwIZEgx81YZIgjSXleyRipo63/Ma9PuuiWvtebfMlI6mwf+OsrGh/qPTsIGWN2WmsrXc0lLAKyxJ++Jj36FelzeyT/hKjTrLW6dvVjOv20Aq371OVpXCAAAACAIcVtsBNvNY7kPsiJOiapI5sO+DHS8qZMLyIuhEXAYPTWK9JdF0mz/kO68s5+p376pzv1272vy0gq9vtUU1Wm6oqStCwTAAAAQIYks2rHbbATrRrHaW4uoLIoMwiLkNPWVEpNL0udHeEpfi8bdjXqiw/Uq7W9s2fM5/Vo5YKpBEYAAABANslEuON2LFcZj2Q73E6W1Csz8fqkuWsGbc8iwiJgMKpfJz34mfASS4c/jGbWblVjU6DP5SV+n7YtmZWOlQIAAAC5IdGwZ+Niwp2UiAhxEu1Z1N/cF37fd0veIBVLWMRpaEC22LKi717cYCA03usPpEMOQVF/4wAAAEDOSGUlT/NBacNnwkOFaGMPfVbKHxYeSEhSp0PvHbdjmZRINU7CPYtchjiS+/5LEy5zPzdHUVkEZIvlfoX9odojvIEalUUAAAAY0uINfBLZpmW6GilnW0iTci6CnWRU40jJPSEth0OcRLANDRiMVk8J/StEpIgGaht2NWrp+j0KBE8l+z5vnlYuKKdnEQAAANIrFcekx7t1KycksWonlmDH7WloBDlZjbAIGIzq1/X9izFKA7UNuxq1avP+ngqjz1x1rr5w3XnpXC0AAACGikxU8jgdie4pkDxeqe1kyr7UtPKNltoDyetZlIqqHYKdnEJYBAxWPX9Rd1VU2BuPAAAgAElEQVQYzb1LmnFr1OktwQ5d/JU/6JoLztI3F05LzxoBAACQfTIR+OSEBBopz10T+pjtV8gShEXAYNewU/rhLGnet6XpH+136tL19Xpw9yHt+LcPaPgwetYDAAAMSpk6dWvIydA2Lacxgh1kGcIiYLCzVvrvSmnEOOm2Tf1O3fHKcf3d95/QNxdepAXTS9O0QAAAAPSRieqe/MLQ9q3WE6n/+tIhka1bbNMC+kVYBAwFf1olPfpf0uf2SP4JUadZa3Xlqkd1zujh+uknLk3jAgEAAHKEmxAoZ7dzJbmSJ9GtW4Q7QFSERcBQ8Nar0l3l0qz/kK68s9+pqx95Xmu2vqDHl8zS2UW+NC0QAABgEEl11U9kaDKoJNCXh0oeYNAgLAKGinvnSCePSot2hP5yjuLVYyf1/lV/1BevO0+fvurcNC4QAAAgg2IJgFz39FH4CV2DQSa3bhHuAIMGYREwVOz8Segbm09ulUpm9Dv1xu89ruZAUL+/40qZfoIlAACArJLqip9B09Mng6duEfgAOYGwCBgqAk3S198nzbhVuv5r/U6tuf9p/aquQUZSsd+nmqoyVVeUpGWZAAAAfSS9z48GQcVPhrZzEfYAcIGwCBhK7pklHd4VOiEtyjcEG3Y1asn6erUEO3vGfF6PVi6YSmAEAADil/I+P4OFixCIwAdAliMsAoaK+nXSg//kXGrc6xuKmbVb1dgU6HN5id+nbUtmpWOlAABgsEhJn58sD4HS1dOHwAdAFoslLMpP9WIAJGDLir7feAUDofFe34wccgiK+hsHAABDUDzbvpoPShs+Ex6QNB8MhUT5heHhiiR1OmwDcxpLG5cVP3O+Gvo42T19CIcADFGuwiJjzHWS7pLkkfRDa21tlHk3SvqVpIuttXVdY+WS7pY0SlJn1+dakrB2YOhrbnA1Xuz3OVYWFft9qVgVAABIp2SGQHX3qs/x7k5hTzDQNyhKmTT0+ekOdZzCHbdjAJBDBgyLjDEeSd+RdI2kBkk7jDEPWWufjZg3UtJiSf/Xayxf0k8lfcRa+7QxZoykbO9KB2SPotLQN3ZO473UVJVp6fo9CgQ7esaG5eeppqos1SsEAADxSmoI9KO+93es+ElnC4oM9vkh7AGAhLipLLpE0gFr7UuSZIxZK2m+pGcj5n1Z0tck3dlr7FpJ9dbapyXJWnss4RUDuWT2sr69Ary+U98wdeluYr1q834dagrISrrqfWNpbg0AQCZkJARKgUz3+aHiBwAyxk1YVCKpd2lDg6RLe08wxlRIGm+t/Y0xpndY9D5J1hizWdJYSWuttf2f/w3glO5viLasOFVhNMv5m6rqipKecOij927XnsZmdXRaefJMulYLAMDQFW9T6GwMgejzAwAYgJuwyOlJs+dvF2NMnqTVkm6Ncv8rJF0s6V1JW7q6b28JewNjbpd0uyRNmDDB1cKBnFG+MPTjrVelu8ql4MkBL7n54vH69M+e0mMvHNXVZWemYZEAAAxSyawCeuizkqfAXVPolEjyti/6/ABAzjLW9r9v2RhzuaTl1tqqrtdLJclau7LrdZGkFyW903XJOEnHJc2T9F5J11lrb+2a+x+SWqy1q6K9X2Vlpa2rq0vgSwKGsPuul945Ii3aEfrmL4q29k5dvnKLKieerrs/4upkRAAAhr7IYCgyBJKy9Bj4FIVAAICc0lW84+oB0U1l0Q5Jk40xkyQ1SrpJ0i3dn7TWNks6o9eb/1HSndbaOmPMi5K+YIw5TVKbpPcrVIUEIB4X3RT6V8vGp6TSGVGnFeTn6cYZpfrRX17WkbdbdObIwjQuEgCANIu3OsjtyWApk6YQiHAIABCjAcMia227MWaRpM2SPJLutdY+Y4xZIanOWvtQP9e+ZYz5pkKBk5X0sLV2U5LWDuSeC+ZLD9dI9Wv7DYskaeHF43X3Yy/pgZ2N+vRV56ZpgQAAJFFCW8QkdQRPjTn1CErFyWBum0ITAgEAstiA29DSjW1owAB+dav08mPSv+yXPN5+py78/hM68naLHr3zKpl+tq0BAJBxWb9FzGUV0Nw1oY8TaQANAEAKJHsbGoBsUn6T9MyvpQN/kMrm9Dt18lnDtf2V43rP0odV7Peppqqs58Q0AABSLpETxNJ2WliGm0IDAGK2YVejVm3er0NNgZ7nHEmuxqorShK+PhdQWQQMNh1B6RvnSROvkBb+JOq0DbsatWR9vVqCnT1jPq9HKxdMzZk/4AAAKRLP9jDJOYjJ80omT+poTcPCI4IhmkIDgGuJBCzJHLv6vLF6YGejAsGOnrV584xkpGCH7XfM5/XohhklCV0/mJ+nYqksIiwCBqP/+Vvppa2STNRvYmfWblVjU6DPpSV+n7YtmZWmhQIABr2s3x4mJVQdRAgEYBDJZGCzdP2euAKWRMby8yRjTNhYoiL+xojZYH6eIiwChrL6ddJDi6T2Xv8C290jodc3vJOWbHL8Q9BIern2gylfJgAgy8VbHZTwt9mx4Mh4ANnHbWCT6HanyLFEKmoSCWeG5eepID9Pb7e0J/OXcdAazM9ThEXAULZ6SqiXQ6Si8dIde3teUlkEAOiR7dVBTieIEQIBGEAmqmzcBjaJbndKRUUNQjzGqCOBHGQwP08RFgFD2XK/nP9F10jLm3pebdjV2LdU1GO06saLBu0eWwCAC26CobRVByV4ghghEDAopaP5cKaqbGKRzjrMXBP5a0vPIncIi4ChzGVlkRT+F7Unz2jsiAI9vnS2jDFpWiwAICmctoyVL3QZDKUC28OAwSrV1TipaD5MlU3m+X1etbZ3pr1nUX+/Tx597iinocWIsAgYyurXhR8vLDn2LIq0ru6gvnB/vX76D5fqislnpGGhAIC4uAmAuoOYTARDhEBASgzGahxCnP4lut3JSbwVNYmGMysXTJWU/pPPhmJgk0mERcBQ1/Mg0VVhdPkiqeor/V7S2t6hmbWPakrJKP34tkvSsEgAwIAyumWME8SARGRL42KqcdLPTWCT6HanVFTUJDJGODM0EBYBuaIjKH3zfGn8pdJNPxtw+potL+ibjzyvR+64UpPPGpmGBQJAjor7pLFUoToIyJZwx2kMp8RSjZOpKhu3gU2yT0MjtEGiCIuAXPL7f5ee/J70+X3SiDP7nXr8ZJsuX7lFC6aXaOWC8jQtEACGuEydNGY8ku1w+oRcBUOEQMhiqeir0+fgD7ZapUw6mg9TZQPEjrAIyCVHn5e+c7F0zZelmYsHnH7zD57QEy8el5H4Cw8AYpWxbWNRAqBovYwIhpBm2VK14xTsFHjy5PUYnWxzCleHhmyqxkln82G+hwViQ1gE5JofXSsF3pL+aXvoX66j2LCrUV98oF6t7Z09Y4P9+EcASJlsOWmsvwAo2ilpQD9ieRBPddNkqnZOGUrVOHxfCWQnwiIg1zz1v9JDi6SP/16acGnUaTNrt6qxqe9DTonfp21LZqVyhQCQ3QZDMAT0Em8lj9tgJ5YAIlcls0KHahwA6UBYBOSa1relr5dJUxZI8/876rRJSzY5bowwkl6u/WDKlgcAWSUtwRAnjWFgqQ58CHZO8fu8am3vzOpTqQhxAKQaYRGQi+67Xnr18dDHUR44qCwCMGRF246ViWCIk8ZyhlPYk46tW7kg2VU7KxdMlcRWKwC5jbAIyDX166QHF0kdrafGvD5p7pqwB5ENuxr7ngTiMVp140V8wwNg8HATAHUHNrt/LrVnIBgiBBo0klndM9S2bmWqaTJVOwCQGoRFQK5ZPUVqPth3vGi8dMfesKHe3xQX5IdOB9n+bx/QaQX5aVosAMQgY6ePOdyXYChrZdN2rlT9bkwmN8FOppsmE+4AQPIRFgG5Zrlfzt+aGml5U9TLdr56XDd87wktmXOePvX+c1O2PABwhWAIEdjOFV2iJ2e5CXZomgwAQwthEZBrYqgsivSxe7ervqFJf/7iLI0YRnURgDRJRzBkPJLtcPqECIbSI5Ej2t2GQNkmHVu3knFyFgAg9xAWAbmmfp20cXH4Q1a+T5q3ZsCHnd0Hm1T9nW2qqSrTP1393hQvFEBOSkvFUJQAKFovI4KhhCSz4sfnzdPfXFSsh3YfUmt7Z9q/lkRE/q5N59YtAh8AQKwIi4Bc1PthTFaaulC64QeuLv2HH+/Q4y++Kf9pBXq9uYVvRAHEL5PBkFMAFO2UNKS8z89gkezqHrZuAQCyFWERkOt+dK108k1pUZ2Ulzfg9G9vfUHf+P3zYWPdx8zyjSyAqLItGIKk5Fb95OdJxpisD4HYzgUAwMAIi4Bct+d+6YF/kD78gPTeDww4fWbtVjU29T1ausTv07Yls1KxQgCDndP2V4KhlHEKgHKhz08sp3YR+AAA0L9YwiK62QJD0fnzpOFnStt/4CosOuQQFPU3DiAHRVYRtZ2MCIqk2IIigiEpviqgxqaAlq7fo7pXj/cZ/+mTr/V5j2Bn5kKiRI5oj/XULidO44RDAAAMjLAIGIryC6QZt0qPrZLeekU6fWK/04v9PsfKomK/LyXLA5DlnLaX7f651N7154TT6Yv9yr1gKN4QqOZXT4eFKdECoECww3E8XZJd8VNdUaLKc0YnFAIBAIDkYRsaMFSdOCStniJd/k/StV/ud+qGXY1aun5P2L/oFnrzVLugnG/IgaHMqfmz5LC9LBZDOxhKZj+gTEtHnx/+DgEAIHvQswhAyD1XS4d2hT4e4AGt9wOQlXTl5DP0P/9wafrWCiC13DSjzvNKxkgdbfG9xyAOhgZjCESfHwAAEAvCIgChB8MHF0kdrafGvD5p7poBH9yWPbhX//vkq7r/U5drxjmjU7xQAEnn6pSyJPCNlgqGZ3UwNBhDICeRAVAq+/wAAIChibAIQGgLmlNfkaLx0h17+730ndZ2Va1+TMGODuXn5elwcwsPF0C2Slcw5LS9zEX4nC7RQqHILbbZHgLFUgXkdBoaf04DAIBoCIsASMv9cj6ZyEjLmwa8vPa3+/T9P70UNubzerRywVQeRIBskYrj632jQ42se98zg9vL4q0MKvTmqcCTpxMt7Slfo5NUNIAGAABIRCxhEaehAUNVUWmUyqJSV5dvfPpwn7FAsEOrNu/noQXIlHQcXz/nq6GPIxtfJzkYSvVpYS3BTrUEO5O6Zil9IRB/zgIAgEyisggYqpwqDjzDpPn/7eqhb9KSTdHqkvRy7QeTtkwAUSR9e1nmTimLDIaysUcQlUAAAGCoS3plkTHmOkl3SfJI+qG1tjbKvBsl/UrSxdbaul7jEyQ9K2m5tfbrbt4TQIK6H/i6HzYl6czzXT8IFvt9amzq+1Ba7Pcla4UAug0UDDUflOp+FMMN0xMMxVsd5FQJFOxMfkjk93nV2t45YChFJRAAAEC4ASuLjDEeSc9LukZSg6Qdkm621j4bMW+kpE2SCiQtigiLHpDUKen/BgqLqCwCUuTRldKfaqXPPBkKjQawYVdjn8aww/Lz9NUbynkoApLJse9QAtIUDGVbdZDTaWErF0yVxFHwAAAAUvIriy6RdMBa+1LXzddKmq9QpVBvX5b0NUl3RiymWtJLkk66WRCAFLn0H6XHvy39ZbW04J4Bp3c/OHU/UEnSe88crvnTilO6TGDIc9V3KAYJHF8f7SStgYKhdFYHJXpamORc9UM4BAAAEJ2bsKhEUu8uuQ2SLu09wRhTIWm8tfY3xpg7e40Pl/RFhaqSwkKkiOtvl3S7JE2YMMH14gHE4LTRUuVt0pPfk65aKo2eNOAl1RUlPQ9UP3jsJX3l4X363d7XNWfq2aleLTA0uNleFpMoDaldhENuAqCl6/eo7tXjroKhRNEjCAAAIHu52Yb2d5KqrLWf6Hr9EUmXWGs/2/U6T9JWSbdaa18xxvxR0p3W2jpjzNclbbfWrjPGLJf0DtvQgAw6cVi6q1yq+LD0N6tjurS9o1Pzv7NNrx07qRGFXr3e3MJDGtBbFjWkdrNlLDKsSSWnLWKEQAAAAOkVyzY0N2HR5Qo1pq7qer1Ukqy1K7teF0l6UdI7XZeMk3Rc0jxJqyWN7xr3K9S3aJm19r+jvR9hEZBiP54rvfKYJBPzlpU1W57XNx95IWysuy8ID3TIaRnsO+QmGEqVZGwRAwAAQHoku2fRDkmTjTGTJDVKuknSLd2ftNY2Szqj15v/UV2VRZL+utf4coUqi6IGRQBSrH6d1LC964UNbYHZuDj00kVg9MsdDX3GAsEOrdq8nwc/5I7ICqLZy0Kvk9x3aEPHTK3a+zc61BJQcaFPNR1lUpy9hGLhMUYdDv+QRHUQAABA7hgwLLLWthtjFknaLMkj6V5r7TPGmBWS6qy1D6V6kQCSZMsKqb0lfCwYCI27CIu6G127HQcGPTc9hzZ8RuoMxnDTvtvLdpy/RJ97dnJPMHT1S2P1wM49YSFQza+eDqvaSUYwFC0AiqxMirU6iHAIAABgcHNTWSRr7cOSHo4YWxZl7lVRxpfHuDYAydbctzKo3/EIxX6fGh2CoWK/L5FVAdkpcmtZ80Gp7l716fTTb1DUNxh6sXi+hr+6RWfaN3XEnKHHzv60vrTjHAW63idVJ425rQyqrihR5TmjqQ4CAADIYa7CIgBDRFGp8+lLRaWuLq+pKtPS9XvCm+Qa6Z9nT07WCoHMcXWkfT+BjdcXPj9aMPTShQoE552a97wkJbe/UCzBkJPeJyECAAAg9xAWAblk9rK+TXhNXmjche6Hx+6Kg9OHF+j4yTb95ImX9a0tz+twEyekYZByqiKKRdF47Tj3sxr/1KpBFwwBAAAAkQiLgFzS3Zeou3qisEhqaZIK/a5vEVlx8C/rduuBpxp7Xjc2BbR0/Z6euUBW6lNF9I7rBtVWoYCmW7unULvO/aw+uuMcBYJ3nfpEgsEQJ40BAAAgUwiLgFxTvvBUaNTeJn33Mun3/yade7Xk8cZ8uydfOt5njBPSkFXcNKmOIjIYajPD9KuOK/V+7VKxOaZDdoy+0f4hPbx9glrbkxcMcdIYAAAAMomwCMhl+QXStf8lrb051Lj30n+M+RackIas5tik+keuL3/LjtC7trAnGPpa+0I91HmFpNsiZna6vmcy+gkBAAAAqURYBOS6sjnSpPdLf1gubbtLOnEoVH0xe9mpCqR+RDshbZQvXzNrt1IBgfRy1aTambWhhu3d3rUFWh78aFc4FB/6CQEAAGAwIiwCcp0x0nuukl7+kxR8NzTWfDBUjSENGBg5nZAmSScC7WoOtEuijxHSJMEm1cftCAUcq4gG5vd51dreGfb/AcEQAAAABivCIgChLWiRgoFQhcYAYVHkCWnFfp+aA216pzU8PKKPEZIusoqo9W3XVUSdVsqLqCL6z3Z3VURO1ULL510oiX5CAAAAGBoIiwCEHrZjGY8QeULapCWbHOfRxwhxc2hS3b7rZ8rvaAl9vr8m1Q7by37VcaVm5+0esIoo1m1khEMAAAAYCgiLAIQevp0etotK47pdtD5GxX5fXPdDjqtfp/YHPxsWDNm6H7n+Cyza9rIvRcyjvxAAAAAQQlgEINTMunevF0nyDAuNx8Gpj5Ex0uJZ7010pcgBOx66W+OfWqUz7VEdMWM10tOm4d1BURcT5VqnKiKn7WUEQwAAAEB0hEUATvUl6t7mY/KkYSOlsuvjul1kH6PThxfo+Mk23fv4K7pr6ws63NTCwzgkSRt2NYb1+fnnM3dp7qu18pk2yUjjdFS2XdHToQhOVUQbI4IigiEAAACgf8ZaO/CsNKqsrLR1dXWZXgaQ2159XLrveumcK6SmV071iZm9bMCG19F88f6n9cu68B5IPq9HKxdM5SE9R23Y1ai//Pq7+pzWqti8qUP2DJ1mWjTavOPqeqcm1UuDn9CDvcIhgiEAAAAgxBiz01pb6WYulUUA+jrnr6TJVdILvzs11nwwtFVNiisw+suBY33GOCEtd0RWENVUlWnXpnu0wtyj00ybJKnUvKlo/37htL3s/o4rNatXk+pv6SaNvOQmlRAMAQAAAAkhLALg7MjevmPBQGirWhxhUbST0DghbeiJDIauPm+sWp5aq19qrYqHvalD756hr9+/UDWeX+q0vLawa02U7WaR28u+pZtUWHmT7iYYAgAAAJKOsAiAs+bGKOMNzuMDiHZC2riiwrjuh+zgFAw9sLOxp7l5Y1NAJ7b/XLXeH4ZVEH3dfF8edTreM7KKKGALtGXi53XXkQqCIQAAACANCIsAOCsqDW09cxqPg9MJaZL0bmtQl/2/P+iNE62EAFnOTTD00ydf07y8v+gLBevC+hB1B0Xd8k2nOq1z3+qTnlF6p3OYzrRv6og5Qwdn1GjhvH9UfN2yAAAAAMSKsAiAs9nLQj2Kgr2qgTzDQuNxiDwhrdjv03njRmrLc0fU3HIqbFi6fk/YfGSHDbsaw8K+xqaAfvbka4psMTQv7y99qoiinqNgpHZPofI7WnqG2j2FGjH/GxrRtdVxXNcPAAAAAOlDWATAWXdfoi0rQlvP8jxS/jDpPVfFfcvqipKwEGhm7dY+c2h6nR0iq4jebWvvUxVmFQqHvpDffxVRtD5ELb6zddqcFad+jxWVKj+BE/cAAAAAJIexUf/JNzMqKyttXV1dppcBINIbz0g/mC35z5GC74R6GhWVhiqN4ny4n7RkU5/KlG4l/kIdampha1oaDLS9LJrIKiKpb7+hnnGFbzlr9xQqf/63CYYAAACANDHG7LTWVrqZS2URAHfOujD0YP/UT06NNR8MbVWT4nroj9b0WpIam1q6fmZrWio5bS/76ZOvOc5NpIrI+EZLBcOpIAIAAAAGAcIiAO692HfbmIKB0DaiOB78nZpeG6lPtRFb05LHzfYyJzH1Iork9Ulzvko4BAAAAAwShEUA3GtuiG18AE5Nr6NXGgU0s3YrR6fHwM3pZdEkUkWkiCqiRLYqAgAAAEg/wiIA7hWVhraeOY3HyanpdX+BUffPbE3rXyzbyyJRRQQAAADkNsIiAO7NXhbqURTsFeaYPOnqf0/aWzhtTXPC1rRw8W4vc/JF7zqqiAAAAIAcxmloAGJTv+7UUec+vxR4S3rP1dKxA0kLDCKDj/62S5X4fTm/NS2yiihWNxc+qX/WL3SmPaojZqzO0lFFy4bCeH3S3DWEQwAAAMAgEMtpaIRFABLz0xulA4+EjyU5ROhva1pvPq9HKxdMHfKBUWSY9nZLUCda2l1dG9lA/MaCx1Xr/aHyO1oGvpgqIgAAAGDQiiUsYhsagMQc3dd3LIET0pzk6ta0yFCopqpMkvr0InLL5/XoPyc9oytf+15PFdHp3jblB52CoohYiV5EAAAAQM4gLAKQmObGKOPxnZDmJBdPTXNqUP3FB+plJLW0d7q6h9/n1fBh+T2/Ft+64AVdvGeVpIBkpHE6KgWjXW2lovFUEQEAAAA5iLAIQGJScEKak6F+apqbBtWtLkMiKVRFtHzeheFf9+qI5uT9KRov3bHX9fsBAAAAGDoIiwAkxumENEkaN1VaPSVllSmxbk2T1GdLV7YESE5VRLGKrCKqqSpTtWebtLqrGfnIcdLbh93dzOsL/fcCAAAAkJNocA0gcb1PSBt1ttR6UmptDp+TgpOzYjk1rTA/L2z7ViabYTtVEb31btT9YGH8Pq9a2zvDQjLHr6V+nXOI54TG1QAAAMCQl/TT0Iwx10m6S5JH0g+ttbVR5t0o6VeSLrbW1hljrpFUK6lAUpukGmvt1v7ei7AIGAK+eb504lDf8RRvbXJ7alo3x2qcJIdHkcHQ1eeN1QM7G+M65r47FJIcqqQ8204FdkWlUusJqaXZ4S4OjauTHOIBAAAAyD5JDYuMMR5Jz0u6RlKDpB2SbrbWPhsxb6SkTQoFQ4u6wqIKSW9Yaw8ZY6ZI2myt7fdJjLAIGAKW+xV+QHs3Iy1vStnbRm7nkkIBi9tgxuf16IYZJXr0uaNJCZCc1hML12FWLFVEEo2rAQAAgBwUS1jkpmfRJZIOWGtf6rr5WknzJT0bMe/Lkr4m6c7uAWvtrl6ff0ZSoTFmmLW21c3iAAxSaWp6Hcnp1LSaqjKt2rzfVcVRINihnz35Wk/M1d0gu+7V430CJKf3iRxzalLtlmOD6mi2rKBxNQAAAICkcVNZdKOk66y1n+h6/RFJl1prF/WaUyHp3621Nxhj/ijpTmttncN9PmWt/YDDe9wu6XZJmjBhwoxXX301sa8KQGZFq3QZ8z6p/V2puTGtVS2JVvhEbNySN89IRgp22F5jkowJG4tFTFvieveIGnGW9M7r7t6ELWcAAABAzkp2ZZFxGOt5GjLG5ElaLenWfhZ0oaSvSrrW6fPW2nsk3SOFtqG5WBOAbNYdRvTuoeMbLb3+9Kk5zQdDgVLv+SniVHEUS1PpyD+Ugp19/5gKdjrNdBYZPsVURRQZxPUXFNG4GgAAAEAc3IRFDZLG93pdKql359qRkqZI+qMxRpLGSXrIGDOvq29RqaRfS/qotfbF5CwbQNYrXxgeTKye0ndOMBAKlNIQYFRXlISFMU7VRpEhTirE1BepdwVRd9jzyJeibDlzaFw956uEQwAAAABi5iYs2iFpsjFmkqRGSTdJuqX7k9baZklndL/uvQ3NGONXqOn1UmvttmQuHMAg09wQ23iKOVUbOZ1UlmiAFPeJa5EVRM0HpQ2fljrbo1xgaVwNAAAAICkGDIuste3GmEWSNkvySLrXWvuMMWaFpDpr7UP9XL5I0nsl/Ycx5j+6xq611h5JdOEABploTa89BdLqC9Pex0jqW20kSZXnjB4wQHLuWdR3LKbtZZGcmlZ3titqfEXjagAAAABJMmCD63SrrKy0dXV1A08EMLg4Nr12CD6ysAnzhl2NA558Fm3MdVAUueXMKVjr5vWF/zpm4a8ZAAAAgOwSS4NrwiIA6RMZiLSdlALH+87LtcbM0U6Pc1I0PvTrEdnLaCj/+gAAAABIGGERgMFhuV+uOgIN9cqZ1Zg/Gs4AAAhmSURBVFP6ryTqNtR/HQAAAACkTCxhUV6qFwMAURWVupvXfWraUFG/LhQQLfcPHBQVjZdkQj8TFAEAAABIAzenoQFAasxe5n77VfPBrmBlkG+9cjrlLBqaVgMAAADIAMIiAJnTHfa46WMknQpWmg+GApfe98hWTn2a3IRjXl8oEAMAAACANKNnEYDsEkuz52xvhB3L1yKFKomy9WsBAAAAMKjF0rOIyiIA2cWp2ijaVq3A8VNVSN3VRq89Kb3w+8yELvFWEUlsOQMAAACQNQiLAGSf8oXhAY/b08KCAanuXvWcsJbK7WqRwdDka6Wnf+6uF1EktpwBAAAAyCKERQCyXyyNsBWxtbb3SWq9w53ucCaeMadgqHdINZBs3z4HAAAAIKfRswjA4OC0xStaI2wn+YVSe8up13leyRipoy32MRm5DoYieX3S3DWEQwAAAADSip5FAIaeyK1pjs2j+wlxegdFktQZ7DvH7VgsQdH/b+9+Q++syziOvz85k7JCQxPzT67QIIJURAVRhMqchKvA2Ipaf6AEjaQnZj1I9kgtg3pSFA4MdGrUaER/tD/Uo9XcGumc5rRZ07FlQiZGsXX14Nw7nf12zphs9+/eue/3C8Y557szuODadf+5zvf+fp1FJEmSJGnO2CySNJ+mLYS98PEwGM3kOdxFpo/YgmbV8a+BZbfbHJIkSZI0V2wWSZpfC2cbAZx96cFrDv1y9StbcPqwTGkMvesj3e3EJkmSJElHic0iSf0yrYEEBz+ydiRrFtkYkiRJktRjNosk9d+0R9aOZDc0G0OSJEmSeszd0CRJkiRJknruleyG9qq2g5EkSZIkSdL8sFkkSZIkSZKkMZtFkiRJkiRJGrNZJEmSJEmSpDGbRZIkSZIkSRqzWSRJkiRJkqQxm0WSJEmSJEkas1kkSZIkSZKksVRV1zEcIMnfgGe6juMoOQV4vusg1BnzP1zmftjM/3CZ++Ey98Nm/ofL3A/bPOb/LVV16uF88ZhrFvVJkoer6qKu41A3zP9wmfthM//DZe6Hy9wPm/kfLnM/bH3Pv4+hSZIkSZIkacxmkSRJkiRJksZsFrXrO10HoE6Z/+Ey98Nm/ofL3A+XuR828z9c5n7Yep1/1yySJEmSJEnSmDOLJEmSJEmSNGazSJIkSZIkSWM2i1qS5OokTyTZnuSLXcej9iQ5K8mvk2xLsjXJ55vxW5M8m2RL8+earmNVO5LsSPJIk+eHm7E3JnkoyZPN68ldx6mjK8nbJ+p7S5IXk9xk7fdXkjVJ9iR5dGJsaq1n5JvNdcAfk1zYXeQ6UjNy/9Ukjzf5XZfkpGb8nCT/mjgGfLu7yHU0zMj/zGN9klua2n8iyfu6iVpHw4zc3z+R9x1JtjTj1n6PHOIebzDnfdcsakGS44A/Ae8FdgIbgZVV9VingakVSU4HTq+qzUleD2wCPgB8GHipqr7WaYBqXZIdwEVV9fzE2B3AC1V1W9MwPrmqbu4qRrWrOe4/C1wCfBJrv5eSXAG8BHyvqt7ZjE2t9ebG8XPANYz+X3yjqi7pKnYdmRm5vwr4VVXtTXI7QJP7c4Af7/+e5t+M/N/KlGN9kncAa4GLgTcDvwDOq6p9ixq0joppuV/w93cC/6iq1dZ+vxziHu8TDOS878yidlwMbK+qp6vqP8B9wPKOY1JLqmpXVW1u3v8T2Aac0W1UOgYsB+5u3t/N6OSi/no38FRVPdN1IGpPVf0WeGHB8KxaX87o5qKqagNwUnPhqTk0LfdV9WBV7W0+bgDOXPTAtChm1P4sy4H7qurfVfVnYDujewPNoUPlPkkY/Ti8dlGD0qI4xD3eYM77NovacQbw14nPO7F5MAjNLwoXAL9rhm5spiGu8TGkXivgwSSbknymGTutqnbB6GQDvKmz6LQYVnDgxaK1Pxyzat1rgWH5FPDTic9Lk/whyW+SXN5VUGrdtGO9tT8clwO7q+rJiTFrv4cW3OMN5rxvs6gdmTLm8349l+R1wA+Am6rqReBbwNuA84FdwJ0dhqd2XVZVFwLLgBuaKcsaiCSvBq4Fvt8MWfsCrwUGI8mXgb3APc3QLuDsqroA+AJwb5I3dBWfWjPrWG/tD8dKDvyhyNrvoSn3eDO/OmVsrmvfZlE7dgJnTXw+E3iuo1i0CJIcz+ggck9V/RCgqnZX1b6q+i/wXZyC3FtV9VzzugdYxyjXu/dPPW1e93QXoVq2DNhcVbvB2h+gWbXutcAAJFkFvB/4aDULgTaPH/29eb8JeAo4r7so1YZDHOut/QFIsgT4EHD//jFrv3+m3eMxoPO+zaJ2bATOTbK0+cV5BbC+45jUkuZ55buAbVX19YnxyWdUPwg8uvDfav4lObFZ9I4kJwJXMcr1emBV87VVwI+6iVCL4IBfFq39wZlV6+uBjze7o1zKaAHUXV0EqHYkuRq4Gbi2ql6eGD+1WfSeJG8FzgWe7iZKteUQx/r1wIokJyRZyij/v1/s+NS69wCPV9XO/QPWfr/MusdjQOf9JV0H0EfNrhg3Aj8HjgPWVNXWjsNSey4DPgY8sn/rTOBLwMok5zOafrgD+Gw34allpwHrRucTlgD3VtXPkmwEHkjyaeAvwHUdxqiWJHkto50vJ+v7Dmu/n5KsBa4ETkmyE/gKcBvTa/0njHZE2Q68zGiXPM2pGbm/BTgBeKg5B2yoquuBK4DVSfYC+4Drq+pwF0fWMWhG/q+cdqyvqq1JHgAeY/R44g3uhDa/puW+qu7i4LUKwdrvm1n3eIM576eZMStJkiRJkiT5GJokSZIkSZL+z2aRJEmSJEmSxmwWSZIkSZIkacxmkSRJkiRJksZsFkmSJEmSJGnMZpEkSZIkSZLGbBZJkiRJkiRp7H/BHRP20bOxngAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize= (20,5))\n",
    "ax.plot(avg_valids,marker='o',label='Val')\n",
    "ax.plot(avg_trainings,marker='o',label='Train')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richard/anaconda3/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/richard/anaconda3/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type LSTMEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(classifier, 'SiameseNN1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
