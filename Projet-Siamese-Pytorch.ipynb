{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Load data\n",
    "with open(\"Fulldata.txt\", \"rb\") as fp:   # Unpickling\n",
    "    df = pickle.load(fp)\n",
    "X = df[['left','right']]     \n",
    "Y = df['target']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperate to training, validation, and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 64)\n",
    "validation_size = int(len(X_train) * 0.1)\n",
    "training_size = len(X_train) - validation_size\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=validation_size,random_state= 64)\n",
    "Y_test = Y_test.values\n",
    "Y_train = Y_train.values\n",
    "Y_validation = Y_validation.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 14400\n",
      "Validation size: 1600\n",
      "test size: 4000\n"
     ]
    }
   ],
   "source": [
    "print('Training size:',X_train.shape[0])\n",
    "print('Validation size:',X_validation.shape[0])\n",
    "print('test size:',X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check shape\n",
    "assert X_train['left'].shape == X_train['right'].shape\n",
    "assert len(X_train['left']) == len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two help function\n",
    "def one_hot(s):\n",
    "    nb_digits=201\n",
    "    batch_size = s.shape[0]\n",
    "    seqlen = s.shape[1]\n",
    "    s_onehot = torch.FloatTensor(batch_size,seqlen,nb_digits)\n",
    "    s_onehot.zero_()\n",
    "    s_onehot.scatter_(2, s.unsqueeze(2), 1)\n",
    "    return s_onehot\n",
    "# def padding(data):\n",
    "#     left = [] \n",
    "#     maxlen= 50\n",
    "#     for i in range(data.shape[0]):\n",
    "#         diff = maxlen - len((data.iloc[i]['left']))\n",
    "#         if diff>=1:\n",
    "#             data.iloc[i]['left']+= [0]*diff\n",
    "#         left.append((data.iloc[i]['left']))\n",
    "#     right = [] \n",
    "#     maxlen= 50\n",
    "#     for i in range(data.shape[0]):\n",
    "#         diff = maxlen - len((data.iloc[i]['right']))\n",
    "#         if diff>=1:\n",
    "#             data.iloc[i]['right']+= [0]*diff\n",
    "#         right.append((data.iloc[i]['right']))\n",
    "#     return torch.tensor(np.array([right,left])).transpose(1,0)\n",
    "def padding(data):\n",
    "    left = [] \n",
    "    for i in range(data.shape[0]):\n",
    "        left.append((data.iloc[i]['left']))\n",
    "    right = [] \n",
    "    for i in range(data.shape[0]):\n",
    "        right.append((data.iloc[i]['right']))\n",
    "    return torch.tensor(np.array([right,left])).transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding and creat the loaders\n",
    "X_train = padding(X_train)\n",
    "Y_train = torch.FloatTensor(np.array(Y_train))\n",
    "train_dataset  = Data.TensorDataset(X_train,Y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32,shuffle=True)\n",
    "\n",
    "X_validation = padding(X_validation)\n",
    "Y_validation = torch.FloatTensor(np.array(Y_validation))\n",
    "val_dataset  = Data.TensorDataset(X_validation,Y_validation)\n",
    "\n",
    "X_test = padding(X_test)\n",
    "Y_test = torch.FloatTensor(np.array(Y_test))\n",
    "test_dataset  = Data.TensorDataset(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14400, 2, 50])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    \"\"\" Implements the network type integrated within the Siamese RNN architecture. \"\"\"\n",
    "    def __init__(self, opt, is_train=False):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.node_size = opt['node_size']\n",
    "        self.name = 'sim_encoder'\n",
    "        self.hidden_size= opt['hidden_size']\n",
    "        self.num_layers= opt['num_layers']\n",
    "        self.embedding_dim = opt['embedding_dim']\n",
    "        self.bidirection_lstm = opt['bidirection_lstm']\n",
    "        self.seqlen = opt['seqlen']\n",
    "        self.batch_size = opt['batch_size']\n",
    "        self.embedding_table = nn.Embedding(num_embeddings=self.node_size, embedding_dim=self.embedding_dim,\n",
    "                                          padding_idx=0, max_norm=None, scale_grad_by_freq=False, sparse=False)\n",
    "        self.lstm_rnn = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.hidden_size,\n",
    "                                num_layers=self.num_layers,bidirectional=self.bidirection_lstm)\n",
    "        if self.bidirection_lstm:\n",
    "            fc_size1= self.hidden_size*self.seqlen*2\n",
    "            fc_size2= 128\n",
    "        else:\n",
    "            fc_size1= self.hidden_size\n",
    "            fc_size2= 16\n",
    "        self.fc1= nn.Linear(fc_size1,fc_size2)\n",
    "        self.fc2= nn.Linear(fc_size2,2)\n",
    "        \n",
    "    def initialize_hidden_plus_cell(self, batch_size):\n",
    "        \"\"\" Re-initializes the hidden state, cell state, and the forget gate bias of the network. \"\"\"\n",
    "        if self.bidirection_lstm: \n",
    "            h0_size= self.num_layers*2\n",
    "        else:\n",
    "            h0_size= self.num_layers\n",
    "        zero_hidden = torch.randn(h0_size, batch_size, self.hidden_size)\n",
    "        zero_cell = torch.randn(h0_size, batch_size,self.hidden_size)\n",
    "        return zero_hidden, zero_cell\n",
    "\n",
    "    def forward(self, input_data, hidden, cell):\n",
    "        \"\"\" Performs a forward pass through the network. \"\"\"\n",
    "        output = self.embedding_table(input_data)\n",
    "        output, (hidden, cell) = self.lstm_rnn(output, (hidden, cell))\n",
    "        #print(output.shape)\n",
    "        #if self.bidirection_lstm:\n",
    "        #    output = nn.functional.relu(self.fc1(output.transpose(1,0).contiguous().view(self.batch_size,-1)))\n",
    "        #else:\n",
    "        #    output = nn.functional.relu(self.fc1(output[-1]))\n",
    "        #output = self.fc2(output) \n",
    "        return output[-1], hidden[-1], cell[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseClassifier(nn.Module):\n",
    "    \"\"\" Sentence similarity estimator implementing a siamese arcitecture. Uses pretrained word2vec embeddings. \n",
    "    Different to the paper, the weights are untied, to avoid exploding/ vanishing gradients. \"\"\"\n",
    "    def __init__(self, opt, is_train=False):\n",
    "        super(SiameseClassifier, self).__init__()\n",
    "        self.learning_rate= opt['learning_rate']\n",
    "        # Initialize network\n",
    "        self.encoder_a =  LSTMEncoder(opt, is_train)\n",
    "        # Initialize network parameters\n",
    "        self.initialize_parameters()\n",
    "        # Declare loss function\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        # Initialize network optimizers\n",
    "        self.optimizer_a = optim.Adam(self.encoder_a.parameters(), lr=self.learning_rate,\n",
    "                                      betas=(0.9, 0.999),weight_decay=0)\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\" Performs a single forward pass through the siamese architecture. \"\"\"\n",
    "        \n",
    "        # Obtain the input length (each batch consists of padded sentences)\n",
    "        input_length = self.batch_a.size(0)\n",
    "        \n",
    "        # Obtain sentence encodings from each encoder\n",
    "        hidden_a, cell_a = self.encoder_a.initialize_hidden_plus_cell(self.batch_size)\n",
    "        output_a, hidden_a, cell_a = self.encoder_a(self.batch_a, hidden_a, cell_a)\n",
    "\n",
    "        hidden_b, cell_b = self.encoder_a.initialize_hidden_plus_cell(self.batch_size)\n",
    "        output_b, hidden_b, cell_b = self.encoder_a(self.batch_b, hidden_b, cell_b)\n",
    "\n",
    "        # Format sentence encodings as 2D tensors\n",
    "        self.encoding_a = output_a.squeeze()\n",
    "        self.encoding_b = output_b.squeeze()\n",
    "\n",
    "        # Obtain similarity score predictions by calculating the Manhattan distance between sentence encodings\n",
    "        if self.batch_size == 1:\n",
    "            self.prediction = torch.exp(-torch.norm((self.encoding_a - self.encoding_b), 1))\n",
    "        else:\n",
    "            self.prediction = torch.exp(-torch.norm((self.encoding_a - self.encoding_b), 1, 1))\n",
    "            \n",
    "\n",
    "    def get_loss(self):\n",
    "        \"\"\" Calculates the MSE loss between the network predictions and the ground truth. \"\"\"\n",
    "        # Loss is the L1 norm of the difference between the obtained sentence encodings\n",
    "        self.loss = self.loss_function(self.prediction, self.labels)\n",
    "\n",
    "    def load_pretrained_parameters(self,pretrained_state_dict_path):\n",
    "        \"\"\" Loads the parameters learned during the pre-training on the SemEval data. \"\"\"\n",
    "        self.encoder_a.load_state_dict(torch.load(pretrained_state_dict_path))\n",
    "        print('Pretrained parameters have been successfully loaded into the encoder networks.')\n",
    "    \n",
    "    def save_lstm(self,path):\n",
    "        torch.save(self.encoder_a.state_dict(), path)\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\" Initializes network parameters. \"\"\"\n",
    "        state_dict = self.encoder_a.state_dict()\n",
    "        for key in state_dict.keys():\n",
    "            if '.weight' in key:\n",
    "                state_dict[key] = torch.nn.init.xavier_normal_((state_dict[key]),gain=1)\n",
    "            if '.bias' in key:\n",
    "                bias_length = state_dict[key].size()[0]\n",
    "                start, end = bias_length // 4, bias_length // 2\n",
    "                state_dict[key][start:end].fill_(2.5)\n",
    "        self.encoder_a.load_state_dict(state_dict)\n",
    "        \n",
    "    def initialize_parametersX(self):\n",
    "        for p in self.encoder_a.parameters():\n",
    "            nn.init.xavier_normal_(p)\n",
    "\n",
    "    def train_step(self, train_batch_a, train_batch_b, train_labels):\n",
    "        \"\"\" Optimizes the parameters of the active networks, i.e. performs a single training step. \"\"\"\n",
    "        # Get batches\n",
    "        self.batch_a = train_batch_a.transpose(0,1)\n",
    "        self.batch_b = train_batch_b.transpose(0,1)\n",
    "        self.labels = train_labels\n",
    "        self.batch_size = self.batch_a.size(1)\n",
    "        self.encoder_a.zero_grad() \n",
    "        self.forward()\n",
    "        self.get_loss()\n",
    "        #l2_reg = None\n",
    "        #for i in classifier.encoder_a.lstm_rnn.parameters():\n",
    "        #    if l2_reg is None:\n",
    "        #        l2_reg = W.norm(2)\n",
    "        #    else:\n",
    "        #        l2_reg = l2_reg + W.norm(2)\n",
    "        #self.loss += l2_reg\n",
    "        self.loss.backward()\n",
    "        clip_grad_norm(self.encoder_a.parameters(), 0.25)\n",
    "        self.optimizer_a.step()\n",
    "\n",
    "    def test_step(self, test_batch_a, test_batch_b, test_labels):\n",
    "        \"\"\" Performs a single test step. \"\"\"\n",
    "        self.batch_a = test_batch_a.transpose(0,1)\n",
    "        self.batch_b = test_batch_b.transpose(0,1)\n",
    "        self.labels = test_labels\n",
    "        self.batch_size = self.batch_a.size(1)\n",
    "        self.forward()\n",
    "        self.get_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate:  0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richard/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:89: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/Users/richard/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:49: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training Batch: 10 | Average loss: 0.5238\n",
      "Epoch: 0 | Training Batch: 20 | Average loss: 0.5247\n",
      "Epoch: 0 | Training Batch: 30 | Average loss: 0.4954\n",
      "Epoch: 0 | Training Batch: 40 | Average loss: 0.5197\n",
      "Epoch: 0 | Training Batch: 50 | Average loss: 0.4984\n",
      "Epoch: 0 | Training Batch: 60 | Average loss: 0.5084\n",
      "Epoch: 0 | Training Batch: 70 | Average loss: 0.4993\n",
      "Epoch: 0 | Training Batch: 80 | Average loss: 0.5115\n",
      "Epoch: 0 | Training Batch: 90 | Average loss: 0.4878\n",
      "Epoch: 0 | Training Batch: 100 | Average loss: 0.4995\n",
      "Epoch: 0 | Training Batch: 110 | Average loss: 0.4826\n",
      "Epoch: 0 | Training Batch: 120 | Average loss: 0.4706\n",
      "Epoch: 0 | Training Batch: 130 | Average loss: 0.4723\n",
      "Epoch: 0 | Training Batch: 140 | Average loss: 0.4770\n",
      "Epoch: 0 | Training Batch: 150 | Average loss: 0.4701\n",
      "Epoch: 0 | Training Batch: 160 | Average loss: 0.4766\n",
      "Epoch: 0 | Training Batch: 170 | Average loss: 0.4490\n",
      "Epoch: 0 | Training Batch: 180 | Average loss: 0.4493\n",
      "Epoch: 0 | Training Batch: 190 | Average loss: 0.4417\n",
      "Epoch: 0 | Training Batch: 200 | Average loss: 0.4213\n",
      "Epoch: 0 | Training Batch: 210 | Average loss: 0.4258\n",
      "Epoch: 0 | Training Batch: 220 | Average loss: 0.4030\n",
      "Average training batch loss at epoch 0: 0.4761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richard/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:79: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation fold accuracy at epoch 0: 0.3959\n",
      "Epoch: 1 | Training Batch: 10 | Average loss: 0.3707\n",
      "Epoch: 1 | Training Batch: 20 | Average loss: 0.3827\n",
      "Epoch: 1 | Training Batch: 30 | Average loss: 0.3594\n",
      "Epoch: 1 | Training Batch: 40 | Average loss: 0.3246\n",
      "Epoch: 1 | Training Batch: 50 | Average loss: 0.3354\n",
      "Epoch: 1 | Training Batch: 60 | Average loss: 0.3356\n",
      "Epoch: 1 | Training Batch: 70 | Average loss: 0.3492\n",
      "Epoch: 1 | Training Batch: 80 | Average loss: 0.3239\n",
      "Epoch: 1 | Training Batch: 90 | Average loss: 0.3032\n",
      "Epoch: 1 | Training Batch: 100 | Average loss: 0.3053\n",
      "Epoch: 1 | Training Batch: 110 | Average loss: 0.2910\n",
      "Epoch: 1 | Training Batch: 120 | Average loss: 0.2846\n",
      "Epoch: 1 | Training Batch: 130 | Average loss: 0.2750\n",
      "Epoch: 1 | Training Batch: 140 | Average loss: 0.2927\n",
      "Epoch: 1 | Training Batch: 150 | Average loss: 0.2763\n",
      "Epoch: 1 | Training Batch: 160 | Average loss: 0.2800\n",
      "Epoch: 1 | Training Batch: 170 | Average loss: 0.2735\n",
      "Epoch: 1 | Training Batch: 180 | Average loss: 0.2598\n",
      "Epoch: 1 | Training Batch: 190 | Average loss: 0.2824\n",
      "Epoch: 1 | Training Batch: 200 | Average loss: 0.2638\n",
      "Epoch: 1 | Training Batch: 210 | Average loss: 0.2663\n",
      "Epoch: 1 | Training Batch: 220 | Average loss: 0.2635\n",
      "Average training batch loss at epoch 1: 0.3901\n",
      "Average validation fold accuracy at epoch 1: 0.3345\n",
      "Epoch: 2 | Training Batch: 10 | Average loss: 0.2619\n",
      "Epoch: 2 | Training Batch: 20 | Average loss: 0.2608\n",
      "Epoch: 2 | Training Batch: 30 | Average loss: 0.2632\n",
      "Epoch: 2 | Training Batch: 40 | Average loss: 0.2582\n",
      "Epoch: 2 | Training Batch: 50 | Average loss: 0.2667\n",
      "Epoch: 2 | Training Batch: 60 | Average loss: 0.2615\n",
      "Epoch: 2 | Training Batch: 70 | Average loss: 0.2577\n",
      "Epoch: 2 | Training Batch: 80 | Average loss: 0.2512\n",
      "Epoch: 2 | Training Batch: 90 | Average loss: 0.2624\n",
      "Epoch: 2 | Training Batch: 100 | Average loss: 0.2625\n",
      "Epoch: 2 | Training Batch: 110 | Average loss: 0.2681\n",
      "Epoch: 2 | Training Batch: 120 | Average loss: 0.2611\n",
      "Epoch: 2 | Training Batch: 130 | Average loss: 0.2498\n",
      "Epoch: 2 | Training Batch: 140 | Average loss: 0.2551\n",
      "Epoch: 2 | Training Batch: 150 | Average loss: 0.2565\n",
      "Epoch: 2 | Training Batch: 160 | Average loss: 0.2542\n",
      "Epoch: 2 | Training Batch: 170 | Average loss: 0.2529\n",
      "Epoch: 2 | Training Batch: 180 | Average loss: 0.2600\n",
      "Epoch: 2 | Training Batch: 190 | Average loss: 0.2559\n",
      "Epoch: 2 | Training Batch: 200 | Average loss: 0.2567\n",
      "Epoch: 2 | Training Batch: 210 | Average loss: 0.2614\n",
      "Epoch: 2 | Training Batch: 220 | Average loss: 0.2567\n",
      "Average training batch loss at epoch 2: 0.3464\n",
      "Average validation fold accuracy at epoch 2: 0.3099\n",
      "Epoch: 3 | Training Batch: 10 | Average loss: 0.2480\n",
      "Epoch: 3 | Training Batch: 20 | Average loss: 0.2550\n",
      "Epoch: 3 | Training Batch: 30 | Average loss: 0.2531\n",
      "Epoch: 3 | Training Batch: 40 | Average loss: 0.2557\n",
      "Epoch: 3 | Training Batch: 50 | Average loss: 0.2512\n",
      "Epoch: 3 | Training Batch: 60 | Average loss: 0.2476\n",
      "Epoch: 3 | Training Batch: 70 | Average loss: 0.2579\n",
      "Epoch: 3 | Training Batch: 80 | Average loss: 0.2532\n",
      "Epoch: 3 | Training Batch: 90 | Average loss: 0.2567\n",
      "Epoch: 3 | Training Batch: 100 | Average loss: 0.2485\n",
      "Epoch: 3 | Training Batch: 110 | Average loss: 0.2560\n",
      "Epoch: 3 | Training Batch: 120 | Average loss: 0.2502\n",
      "Epoch: 3 | Training Batch: 130 | Average loss: 0.2542\n",
      "Epoch: 3 | Training Batch: 140 | Average loss: 0.2540\n",
      "Epoch: 3 | Training Batch: 150 | Average loss: 0.2546\n",
      "Epoch: 3 | Training Batch: 160 | Average loss: 0.2521\n",
      "Epoch: 3 | Training Batch: 170 | Average loss: 0.2531\n",
      "Epoch: 3 | Training Batch: 180 | Average loss: 0.2570\n",
      "Epoch: 3 | Training Batch: 190 | Average loss: 0.2533\n",
      "Epoch: 3 | Training Batch: 200 | Average loss: 0.2502\n",
      "Epoch: 3 | Training Batch: 210 | Average loss: 0.2541\n",
      "Epoch: 3 | Training Batch: 220 | Average loss: 0.2506\n",
      "Average training batch loss at epoch 3: 0.3230\n",
      "Average validation fold accuracy at epoch 3: 0.2963\n",
      "Epoch: 4 | Training Batch: 10 | Average loss: 0.2532\n",
      "Epoch: 4 | Training Batch: 20 | Average loss: 0.2550\n",
      "Epoch: 4 | Training Batch: 30 | Average loss: 0.2491\n",
      "Epoch: 4 | Training Batch: 40 | Average loss: 0.2523\n",
      "Epoch: 4 | Training Batch: 50 | Average loss: 0.2489\n",
      "Epoch: 4 | Training Batch: 60 | Average loss: 0.2448\n",
      "Epoch: 4 | Training Batch: 70 | Average loss: 0.2486\n",
      "Epoch: 4 | Training Batch: 80 | Average loss: 0.2470\n",
      "Epoch: 4 | Training Batch: 90 | Average loss: 0.2472\n",
      "Epoch: 4 | Training Batch: 100 | Average loss: 0.2497\n",
      "Epoch: 4 | Training Batch: 110 | Average loss: 0.2524\n",
      "Epoch: 4 | Training Batch: 120 | Average loss: 0.2482\n",
      "Epoch: 4 | Training Batch: 130 | Average loss: 0.2476\n",
      "Epoch: 4 | Training Batch: 140 | Average loss: 0.2518\n",
      "Epoch: 4 | Training Batch: 150 | Average loss: 0.2457\n",
      "Epoch: 4 | Training Batch: 160 | Average loss: 0.2467\n",
      "Epoch: 4 | Training Batch: 170 | Average loss: 0.2484\n",
      "Epoch: 4 | Training Batch: 180 | Average loss: 0.2526\n",
      "Epoch: 4 | Training Batch: 190 | Average loss: 0.2507\n",
      "Epoch: 4 | Training Batch: 200 | Average loss: 0.2472\n",
      "Epoch: 4 | Training Batch: 210 | Average loss: 0.2513\n",
      "Epoch: 4 | Training Batch: 220 | Average loss: 0.2521\n",
      "Average training batch loss at epoch 4: 0.3084\n",
      "Average validation fold accuracy at epoch 4: 0.2880\n",
      "Epoch: 5 | Training Batch: 10 | Average loss: 0.2508\n",
      "Epoch: 5 | Training Batch: 20 | Average loss: 0.2529\n",
      "Epoch: 5 | Training Batch: 30 | Average loss: 0.2469\n",
      "Epoch: 5 | Training Batch: 40 | Average loss: 0.2464\n",
      "Epoch: 5 | Training Batch: 50 | Average loss: 0.2449\n",
      "Epoch: 5 | Training Batch: 60 | Average loss: 0.2482\n",
      "Epoch: 5 | Training Batch: 70 | Average loss: 0.2467\n",
      "Epoch: 5 | Training Batch: 80 | Average loss: 0.2479\n",
      "Epoch: 5 | Training Batch: 90 | Average loss: 0.2452\n",
      "Epoch: 5 | Training Batch: 100 | Average loss: 0.2467\n",
      "Epoch: 5 | Training Batch: 110 | Average loss: 0.2479\n",
      "Epoch: 5 | Training Batch: 120 | Average loss: 0.2449\n",
      "Epoch: 5 | Training Batch: 130 | Average loss: 0.2505\n",
      "Epoch: 5 | Training Batch: 140 | Average loss: 0.2441\n",
      "Epoch: 5 | Training Batch: 150 | Average loss: 0.2480\n",
      "Epoch: 5 | Training Batch: 160 | Average loss: 0.2479\n",
      "Epoch: 5 | Training Batch: 170 | Average loss: 0.2513\n",
      "Epoch: 5 | Training Batch: 180 | Average loss: 0.2474\n",
      "Epoch: 5 | Training Batch: 190 | Average loss: 0.2482\n",
      "Epoch: 5 | Training Batch: 200 | Average loss: 0.2448\n",
      "Epoch: 5 | Training Batch: 210 | Average loss: 0.2462\n",
      "Epoch: 5 | Training Batch: 220 | Average loss: 0.2461\n",
      "Average training batch loss at epoch 5: 0.2982\n",
      "Average validation fold accuracy at epoch 5: 0.2826\n",
      "Epoch: 6 | Training Batch: 10 | Average loss: 0.2502\n",
      "Epoch: 6 | Training Batch: 20 | Average loss: 0.2497\n",
      "Epoch: 6 | Training Batch: 30 | Average loss: 0.2446\n",
      "Epoch: 6 | Training Batch: 40 | Average loss: 0.2493\n",
      "Epoch: 6 | Training Batch: 50 | Average loss: 0.2504\n",
      "Epoch: 6 | Training Batch: 60 | Average loss: 0.2440\n",
      "Epoch: 6 | Training Batch: 70 | Average loss: 0.2493\n",
      "Epoch: 6 | Training Batch: 80 | Average loss: 0.2476\n",
      "Epoch: 6 | Training Batch: 90 | Average loss: 0.2442\n",
      "Epoch: 6 | Training Batch: 100 | Average loss: 0.2469\n",
      "Epoch: 6 | Training Batch: 110 | Average loss: 0.2473\n",
      "Epoch: 6 | Training Batch: 120 | Average loss: 0.2489\n",
      "Epoch: 6 | Training Batch: 130 | Average loss: 0.2459\n",
      "Epoch: 6 | Training Batch: 140 | Average loss: 0.2417\n",
      "Epoch: 6 | Training Batch: 150 | Average loss: 0.2460\n",
      "Epoch: 6 | Training Batch: 160 | Average loss: 0.2457\n",
      "Epoch: 6 | Training Batch: 170 | Average loss: 0.2458\n",
      "Epoch: 6 | Training Batch: 180 | Average loss: 0.2444\n",
      "Epoch: 6 | Training Batch: 190 | Average loss: 0.2442\n",
      "Epoch: 6 | Training Batch: 200 | Average loss: 0.2443\n",
      "Epoch: 6 | Training Batch: 210 | Average loss: 0.2489\n",
      "Epoch: 6 | Training Batch: 220 | Average loss: 0.2483\n",
      "Average training batch loss at epoch 6: 0.2908\n",
      "Average validation fold accuracy at epoch 6: 0.2786\n",
      "Epoch: 7 | Training Batch: 10 | Average loss: 0.2417\n",
      "Epoch: 7 | Training Batch: 20 | Average loss: 0.2413\n",
      "Epoch: 7 | Training Batch: 30 | Average loss: 0.2415\n",
      "Epoch: 7 | Training Batch: 40 | Average loss: 0.2450\n",
      "Epoch: 7 | Training Batch: 50 | Average loss: 0.2444\n",
      "Epoch: 7 | Training Batch: 60 | Average loss: 0.2474\n",
      "Epoch: 7 | Training Batch: 70 | Average loss: 0.2470\n",
      "Epoch: 7 | Training Batch: 80 | Average loss: 0.2424\n",
      "Epoch: 7 | Training Batch: 90 | Average loss: 0.2447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Training Batch: 100 | Average loss: 0.2484\n",
      "Epoch: 7 | Training Batch: 110 | Average loss: 0.2440\n",
      "Epoch: 7 | Training Batch: 120 | Average loss: 0.2446\n",
      "Epoch: 7 | Training Batch: 130 | Average loss: 0.2481\n",
      "Epoch: 7 | Training Batch: 140 | Average loss: 0.2441\n",
      "Epoch: 7 | Training Batch: 150 | Average loss: 0.2424\n",
      "Epoch: 7 | Training Batch: 160 | Average loss: 0.2468\n",
      "Epoch: 7 | Training Batch: 170 | Average loss: 0.2465\n",
      "Epoch: 7 | Training Batch: 180 | Average loss: 0.2465\n",
      "Epoch: 7 | Training Batch: 190 | Average loss: 0.2441\n",
      "Epoch: 7 | Training Batch: 200 | Average loss: 0.2474\n",
      "Epoch: 7 | Training Batch: 210 | Average loss: 0.2431\n",
      "Epoch: 7 | Training Batch: 220 | Average loss: 0.2441\n",
      "Average training batch loss at epoch 7: 0.2851\n",
      "Average validation fold accuracy at epoch 7: 0.2757\n",
      "Epoch: 8 | Training Batch: 10 | Average loss: 0.2462\n",
      "Epoch: 8 | Training Batch: 20 | Average loss: 0.2448\n",
      "Epoch: 8 | Training Batch: 30 | Average loss: 0.2447\n",
      "Epoch: 8 | Training Batch: 40 | Average loss: 0.2459\n",
      "Epoch: 8 | Training Batch: 50 | Average loss: 0.2437\n",
      "Epoch: 8 | Training Batch: 60 | Average loss: 0.2464\n",
      "Epoch: 8 | Training Batch: 70 | Average loss: 0.2420\n",
      "Epoch: 8 | Training Batch: 80 | Average loss: 0.2437\n",
      "Epoch: 8 | Training Batch: 90 | Average loss: 0.2439\n",
      "Epoch: 8 | Training Batch: 100 | Average loss: 0.2457\n",
      "Epoch: 8 | Training Batch: 110 | Average loss: 0.2437\n",
      "Epoch: 8 | Training Batch: 120 | Average loss: 0.2417\n",
      "Epoch: 8 | Training Batch: 130 | Average loss: 0.2451\n",
      "Epoch: 8 | Training Batch: 140 | Average loss: 0.2436\n",
      "Epoch: 8 | Training Batch: 150 | Average loss: 0.2447\n",
      "Epoch: 8 | Training Batch: 160 | Average loss: 0.2378\n",
      "Epoch: 8 | Training Batch: 170 | Average loss: 0.2427\n",
      "Epoch: 8 | Training Batch: 180 | Average loss: 0.2461\n",
      "Epoch: 8 | Training Batch: 190 | Average loss: 0.2448\n",
      "Epoch: 8 | Training Batch: 200 | Average loss: 0.2425\n",
      "Epoch: 8 | Training Batch: 210 | Average loss: 0.2442\n",
      "Epoch: 8 | Training Batch: 220 | Average loss: 0.2468\n",
      "Average training batch loss at epoch 8: 0.2805\n",
      "Average validation fold accuracy at epoch 8: 0.2733\n",
      "Epoch: 9 | Training Batch: 10 | Average loss: 0.2458\n",
      "Epoch: 9 | Training Batch: 20 | Average loss: 0.2432\n",
      "Epoch: 9 | Training Batch: 30 | Average loss: 0.2447\n",
      "Epoch: 9 | Training Batch: 40 | Average loss: 0.2405\n",
      "Epoch: 9 | Training Batch: 50 | Average loss: 0.2422\n",
      "Epoch: 9 | Training Batch: 60 | Average loss: 0.2451\n",
      "Epoch: 9 | Training Batch: 70 | Average loss: 0.2457\n",
      "Epoch: 9 | Training Batch: 80 | Average loss: 0.2417\n",
      "Epoch: 9 | Training Batch: 90 | Average loss: 0.2410\n",
      "Epoch: 9 | Training Batch: 100 | Average loss: 0.2388\n",
      "Epoch: 9 | Training Batch: 110 | Average loss: 0.2409\n",
      "Epoch: 9 | Training Batch: 120 | Average loss: 0.2408\n",
      "Epoch: 9 | Training Batch: 130 | Average loss: 0.2439\n",
      "Epoch: 9 | Training Batch: 140 | Average loss: 0.2439\n",
      "Epoch: 9 | Training Batch: 150 | Average loss: 0.2435\n",
      "Epoch: 9 | Training Batch: 160 | Average loss: 0.2474\n",
      "Epoch: 9 | Training Batch: 170 | Average loss: 0.2392\n",
      "Epoch: 9 | Training Batch: 180 | Average loss: 0.2441\n",
      "Epoch: 9 | Training Batch: 190 | Average loss: 0.2462\n",
      "Epoch: 9 | Training Batch: 200 | Average loss: 0.2407\n",
      "Epoch: 9 | Training Batch: 210 | Average loss: 0.2440\n",
      "Epoch: 9 | Training Batch: 220 | Average loss: 0.2449\n",
      "Average training batch loss at epoch 9: 0.2768\n",
      "Average validation fold accuracy at epoch 9: 0.2713\n",
      "Epoch: 10 | Training Batch: 10 | Average loss: 0.2406\n",
      "Epoch: 10 | Training Batch: 20 | Average loss: 0.2447\n",
      "Epoch: 10 | Training Batch: 30 | Average loss: 0.2434\n",
      "Epoch: 10 | Training Batch: 40 | Average loss: 0.2448\n",
      "Epoch: 10 | Training Batch: 50 | Average loss: 0.2428\n",
      "Epoch: 10 | Training Batch: 60 | Average loss: 0.2405\n",
      "Epoch: 10 | Training Batch: 70 | Average loss: 0.2402\n",
      "Epoch: 10 | Training Batch: 80 | Average loss: 0.2460\n",
      "Epoch: 10 | Training Batch: 90 | Average loss: 0.2427\n",
      "Epoch: 10 | Training Batch: 100 | Average loss: 0.2413\n",
      "Epoch: 10 | Training Batch: 110 | Average loss: 0.2433\n",
      "Epoch: 10 | Training Batch: 120 | Average loss: 0.2450\n",
      "Epoch: 10 | Training Batch: 130 | Average loss: 0.2414\n",
      "Epoch: 10 | Training Batch: 140 | Average loss: 0.2391\n",
      "Epoch: 10 | Training Batch: 150 | Average loss: 0.2405\n",
      "Epoch: 10 | Training Batch: 160 | Average loss: 0.2451\n",
      "Epoch: 10 | Training Batch: 170 | Average loss: 0.2429\n",
      "Epoch: 10 | Training Batch: 180 | Average loss: 0.2431\n",
      "Epoch: 10 | Training Batch: 190 | Average loss: 0.2376\n",
      "Epoch: 10 | Training Batch: 200 | Average loss: 0.2391\n",
      "Epoch: 10 | Training Batch: 210 | Average loss: 0.2453\n",
      "Epoch: 10 | Training Batch: 220 | Average loss: 0.2408\n",
      "Average training batch loss at epoch 10: 0.2736\n",
      "Average validation fold accuracy at epoch 10: 0.2697\n",
      "Epoch: 11 | Training Batch: 10 | Average loss: 0.2395\n",
      "Epoch: 11 | Training Batch: 20 | Average loss: 0.2378\n",
      "Epoch: 11 | Training Batch: 30 | Average loss: 0.2411\n",
      "Epoch: 11 | Training Batch: 40 | Average loss: 0.2433\n",
      "Epoch: 11 | Training Batch: 50 | Average loss: 0.2396\n",
      "Epoch: 11 | Training Batch: 60 | Average loss: 0.2421\n",
      "Epoch: 11 | Training Batch: 70 | Average loss: 0.2371\n",
      "Epoch: 11 | Training Batch: 80 | Average loss: 0.2433\n",
      "Epoch: 11 | Training Batch: 90 | Average loss: 0.2425\n",
      "Epoch: 11 | Training Batch: 100 | Average loss: 0.2442\n",
      "Epoch: 11 | Training Batch: 110 | Average loss: 0.2397\n",
      "Epoch: 11 | Training Batch: 120 | Average loss: 0.2410\n",
      "Epoch: 11 | Training Batch: 130 | Average loss: 0.2442\n",
      "Epoch: 11 | Training Batch: 140 | Average loss: 0.2416\n",
      "Epoch: 11 | Training Batch: 150 | Average loss: 0.2431\n",
      "Epoch: 11 | Training Batch: 160 | Average loss: 0.2412\n",
      "Epoch: 11 | Training Batch: 170 | Average loss: 0.2408\n",
      "Epoch: 11 | Training Batch: 180 | Average loss: 0.2398\n",
      "Epoch: 11 | Training Batch: 190 | Average loss: 0.2420\n",
      "Epoch: 11 | Training Batch: 200 | Average loss: 0.2427\n",
      "Epoch: 11 | Training Batch: 210 | Average loss: 0.2400\n",
      "Epoch: 11 | Training Batch: 220 | Average loss: 0.2422\n",
      "Average training batch loss at epoch 11: 0.2710\n",
      "Average validation fold accuracy at epoch 11: 0.2684\n",
      "Epoch: 12 | Training Batch: 10 | Average loss: 0.2420\n",
      "Epoch: 12 | Training Batch: 20 | Average loss: 0.2418\n",
      "Epoch: 12 | Training Batch: 30 | Average loss: 0.2384\n",
      "Epoch: 12 | Training Batch: 40 | Average loss: 0.2394\n",
      "Epoch: 12 | Training Batch: 50 | Average loss: 0.2361\n",
      "Epoch: 12 | Training Batch: 60 | Average loss: 0.2360\n",
      "Epoch: 12 | Training Batch: 70 | Average loss: 0.2375\n",
      "Epoch: 12 | Training Batch: 80 | Average loss: 0.2413\n",
      "Epoch: 12 | Training Batch: 90 | Average loss: 0.2424\n",
      "Epoch: 12 | Training Batch: 100 | Average loss: 0.2398\n",
      "Epoch: 12 | Training Batch: 110 | Average loss: 0.2385\n",
      "Epoch: 12 | Training Batch: 120 | Average loss: 0.2400\n",
      "Epoch: 12 | Training Batch: 130 | Average loss: 0.2417\n",
      "Epoch: 12 | Training Batch: 140 | Average loss: 0.2414\n",
      "Epoch: 12 | Training Batch: 150 | Average loss: 0.2422\n",
      "Epoch: 12 | Training Batch: 160 | Average loss: 0.2449\n",
      "Epoch: 12 | Training Batch: 170 | Average loss: 0.2387\n",
      "Epoch: 12 | Training Batch: 180 | Average loss: 0.2398\n",
      "Epoch: 12 | Training Batch: 190 | Average loss: 0.2379\n",
      "Epoch: 12 | Training Batch: 200 | Average loss: 0.2412\n",
      "Epoch: 12 | Training Batch: 210 | Average loss: 0.2464\n",
      "Epoch: 12 | Training Batch: 220 | Average loss: 0.2423\n",
      "Average training batch loss at epoch 12: 0.2686\n",
      "Average validation fold accuracy at epoch 12: 0.2673\n",
      "Epoch: 13 | Training Batch: 10 | Average loss: 0.2403\n",
      "Epoch: 13 | Training Batch: 20 | Average loss: 0.2424\n",
      "Epoch: 13 | Training Batch: 30 | Average loss: 0.2406\n",
      "Epoch: 13 | Training Batch: 40 | Average loss: 0.2408\n",
      "Epoch: 13 | Training Batch: 50 | Average loss: 0.2393\n",
      "Epoch: 13 | Training Batch: 60 | Average loss: 0.2376\n",
      "Epoch: 13 | Training Batch: 70 | Average loss: 0.2416\n",
      "Epoch: 13 | Training Batch: 80 | Average loss: 0.2447\n",
      "Epoch: 13 | Training Batch: 90 | Average loss: 0.2413\n",
      "Epoch: 13 | Training Batch: 100 | Average loss: 0.2369\n",
      "Epoch: 13 | Training Batch: 110 | Average loss: 0.2409\n",
      "Epoch: 13 | Training Batch: 120 | Average loss: 0.2370\n",
      "Epoch: 13 | Training Batch: 130 | Average loss: 0.2404\n",
      "Epoch: 13 | Training Batch: 140 | Average loss: 0.2365\n",
      "Epoch: 13 | Training Batch: 150 | Average loss: 0.2429\n",
      "Epoch: 13 | Training Batch: 160 | Average loss: 0.2362\n",
      "Epoch: 13 | Training Batch: 170 | Average loss: 0.2381\n",
      "Epoch: 13 | Training Batch: 180 | Average loss: 0.2330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Training Batch: 190 | Average loss: 0.2396\n",
      "Epoch: 13 | Training Batch: 200 | Average loss: 0.2419\n",
      "Epoch: 13 | Training Batch: 210 | Average loss: 0.2418\n",
      "Epoch: 13 | Training Batch: 220 | Average loss: 0.2438\n",
      "Average training batch loss at epoch 13: 0.2666\n",
      "Average validation fold accuracy at epoch 13: 0.2664\n",
      "Epoch: 14 | Training Batch: 10 | Average loss: 0.2424\n",
      "Epoch: 14 | Training Batch: 20 | Average loss: 0.2412\n",
      "Epoch: 14 | Training Batch: 30 | Average loss: 0.2408\n",
      "Epoch: 14 | Training Batch: 40 | Average loss: 0.2392\n",
      "Epoch: 14 | Training Batch: 50 | Average loss: 0.2412\n",
      "Epoch: 14 | Training Batch: 60 | Average loss: 0.2373\n",
      "Epoch: 14 | Training Batch: 70 | Average loss: 0.2385\n",
      "Epoch: 14 | Training Batch: 80 | Average loss: 0.2429\n",
      "Epoch: 14 | Training Batch: 90 | Average loss: 0.2367\n",
      "Epoch: 14 | Training Batch: 100 | Average loss: 0.2381\n",
      "Epoch: 14 | Training Batch: 110 | Average loss: 0.2380\n",
      "Epoch: 14 | Training Batch: 120 | Average loss: 0.2407\n",
      "Epoch: 14 | Training Batch: 130 | Average loss: 0.2372\n",
      "Epoch: 14 | Training Batch: 140 | Average loss: 0.2399\n",
      "Epoch: 14 | Training Batch: 150 | Average loss: 0.2390\n",
      "Epoch: 14 | Training Batch: 160 | Average loss: 0.2382\n",
      "Epoch: 14 | Training Batch: 170 | Average loss: 0.2363\n",
      "Epoch: 14 | Training Batch: 180 | Average loss: 0.2434\n",
      "Epoch: 14 | Training Batch: 190 | Average loss: 0.2396\n",
      "Epoch: 14 | Training Batch: 200 | Average loss: 0.2403\n",
      "Epoch: 14 | Training Batch: 210 | Average loss: 0.2334\n",
      "Epoch: 14 | Training Batch: 220 | Average loss: 0.2403\n",
      "Average training batch loss at epoch 14: 0.2648\n",
      "Average validation fold accuracy at epoch 14: 0.2655\n",
      "Epoch: 15 | Training Batch: 10 | Average loss: 0.2391\n",
      "Epoch: 15 | Training Batch: 20 | Average loss: 0.2351\n",
      "Epoch: 15 | Training Batch: 30 | Average loss: 0.2426\n",
      "Epoch: 15 | Training Batch: 40 | Average loss: 0.2409\n",
      "Epoch: 15 | Training Batch: 50 | Average loss: 0.2376\n",
      "Epoch: 15 | Training Batch: 60 | Average loss: 0.2375\n",
      "Epoch: 15 | Training Batch: 70 | Average loss: 0.2361\n",
      "Epoch: 15 | Training Batch: 80 | Average loss: 0.2361\n",
      "Epoch: 15 | Training Batch: 90 | Average loss: 0.2392\n",
      "Epoch: 15 | Training Batch: 100 | Average loss: 0.2397\n",
      "Epoch: 15 | Training Batch: 110 | Average loss: 0.2386\n",
      "Epoch: 15 | Training Batch: 120 | Average loss: 0.2407\n",
      "Epoch: 15 | Training Batch: 130 | Average loss: 0.2357\n",
      "Epoch: 15 | Training Batch: 140 | Average loss: 0.2391\n",
      "Epoch: 15 | Training Batch: 150 | Average loss: 0.2397\n",
      "Epoch: 15 | Training Batch: 160 | Average loss: 0.2420\n",
      "Epoch: 15 | Training Batch: 170 | Average loss: 0.2450\n",
      "Epoch: 15 | Training Batch: 180 | Average loss: 0.2353\n",
      "Epoch: 15 | Training Batch: 190 | Average loss: 0.2366\n",
      "Epoch: 15 | Training Batch: 200 | Average loss: 0.2374\n",
      "Epoch: 15 | Training Batch: 210 | Average loss: 0.2399\n",
      "Epoch: 15 | Training Batch: 220 | Average loss: 0.2381\n",
      "Average training batch loss at epoch 15: 0.2631\n",
      "Average validation fold accuracy at epoch 15: 0.2648\n",
      "Epoch: 16 | Training Batch: 10 | Average loss: 0.2404\n",
      "Epoch: 16 | Training Batch: 20 | Average loss: 0.2388\n",
      "Epoch: 16 | Training Batch: 30 | Average loss: 0.2398\n",
      "Epoch: 16 | Training Batch: 40 | Average loss: 0.2384\n",
      "Epoch: 16 | Training Batch: 50 | Average loss: 0.2398\n",
      "Epoch: 16 | Training Batch: 60 | Average loss: 0.2362\n",
      "Epoch: 16 | Training Batch: 70 | Average loss: 0.2350\n",
      "Epoch: 16 | Training Batch: 80 | Average loss: 0.2353\n",
      "Epoch: 16 | Training Batch: 90 | Average loss: 0.2409\n",
      "Epoch: 16 | Training Batch: 100 | Average loss: 0.2374\n",
      "Epoch: 16 | Training Batch: 110 | Average loss: 0.2395\n",
      "Epoch: 16 | Training Batch: 120 | Average loss: 0.2333\n",
      "Epoch: 16 | Training Batch: 130 | Average loss: 0.2369\n",
      "Epoch: 16 | Training Batch: 140 | Average loss: 0.2363\n",
      "Epoch: 16 | Training Batch: 150 | Average loss: 0.2421\n",
      "Epoch: 16 | Training Batch: 160 | Average loss: 0.2396\n",
      "Epoch: 16 | Training Batch: 170 | Average loss: 0.2381\n",
      "Epoch: 16 | Training Batch: 180 | Average loss: 0.2392\n",
      "Epoch: 16 | Training Batch: 190 | Average loss: 0.2390\n",
      "Epoch: 16 | Training Batch: 200 | Average loss: 0.2368\n",
      "Epoch: 16 | Training Batch: 210 | Average loss: 0.2392\n",
      "Epoch: 16 | Training Batch: 220 | Average loss: 0.2432\n",
      "Average training batch loss at epoch 16: 0.2617\n",
      "Average validation fold accuracy at epoch 16: 0.2642\n",
      "Epoch: 17 | Training Batch: 10 | Average loss: 0.2326\n",
      "Epoch: 17 | Training Batch: 20 | Average loss: 0.2361\n",
      "Epoch: 17 | Training Batch: 30 | Average loss: 0.2398\n",
      "Epoch: 17 | Training Batch: 40 | Average loss: 0.2380\n",
      "Epoch: 17 | Training Batch: 50 | Average loss: 0.2384\n",
      "Epoch: 17 | Training Batch: 60 | Average loss: 0.2372\n",
      "Epoch: 17 | Training Batch: 70 | Average loss: 0.2406\n",
      "Epoch: 17 | Training Batch: 80 | Average loss: 0.2363\n",
      "Epoch: 17 | Training Batch: 90 | Average loss: 0.2400\n",
      "Epoch: 17 | Training Batch: 100 | Average loss: 0.2382\n",
      "Epoch: 17 | Training Batch: 110 | Average loss: 0.2333\n",
      "Epoch: 17 | Training Batch: 120 | Average loss: 0.2375\n",
      "Epoch: 17 | Training Batch: 130 | Average loss: 0.2387\n",
      "Epoch: 17 | Training Batch: 140 | Average loss: 0.2350\n",
      "Epoch: 17 | Training Batch: 150 | Average loss: 0.2383\n",
      "Epoch: 17 | Training Batch: 160 | Average loss: 0.2366\n",
      "Epoch: 17 | Training Batch: 170 | Average loss: 0.2354\n",
      "Epoch: 17 | Training Batch: 180 | Average loss: 0.2390\n",
      "Epoch: 17 | Training Batch: 190 | Average loss: 0.2370\n",
      "Epoch: 17 | Training Batch: 200 | Average loss: 0.2361\n",
      "Epoch: 17 | Training Batch: 210 | Average loss: 0.2377\n",
      "Epoch: 17 | Training Batch: 220 | Average loss: 0.2436\n",
      "Average training batch loss at epoch 17: 0.2603\n",
      "Average validation fold accuracy at epoch 17: 0.2636\n",
      "Epoch: 18 | Training Batch: 10 | Average loss: 0.2330\n",
      "Epoch: 18 | Training Batch: 20 | Average loss: 0.2385\n",
      "Epoch: 18 | Training Batch: 30 | Average loss: 0.2396\n",
      "Epoch: 18 | Training Batch: 40 | Average loss: 0.2400\n",
      "Epoch: 18 | Training Batch: 50 | Average loss: 0.2385\n",
      "Epoch: 18 | Training Batch: 60 | Average loss: 0.2362\n",
      "Epoch: 18 | Training Batch: 70 | Average loss: 0.2370\n",
      "Epoch: 18 | Training Batch: 80 | Average loss: 0.2358\n",
      "Epoch: 18 | Training Batch: 90 | Average loss: 0.2358\n",
      "Epoch: 18 | Training Batch: 100 | Average loss: 0.2363\n",
      "Epoch: 18 | Training Batch: 110 | Average loss: 0.2382\n",
      "Epoch: 18 | Training Batch: 120 | Average loss: 0.2357\n",
      "Epoch: 18 | Training Batch: 130 | Average loss: 0.2373\n",
      "Epoch: 18 | Training Batch: 140 | Average loss: 0.2377\n",
      "Epoch: 18 | Training Batch: 150 | Average loss: 0.2362\n",
      "Epoch: 18 | Training Batch: 160 | Average loss: 0.2393\n",
      "Epoch: 18 | Training Batch: 170 | Average loss: 0.2405\n",
      "Epoch: 18 | Training Batch: 180 | Average loss: 0.2374\n",
      "Epoch: 18 | Training Batch: 190 | Average loss: 0.2396\n",
      "Epoch: 18 | Training Batch: 200 | Average loss: 0.2375\n",
      "Epoch: 18 | Training Batch: 210 | Average loss: 0.2372\n",
      "Epoch: 18 | Training Batch: 220 | Average loss: 0.2368\n",
      "Average training batch loss at epoch 18: 0.2591\n",
      "Average validation fold accuracy at epoch 18: 0.2631\n",
      "Epoch: 19 | Training Batch: 10 | Average loss: 0.2355\n",
      "Epoch: 19 | Training Batch: 20 | Average loss: 0.2396\n",
      "Epoch: 19 | Training Batch: 30 | Average loss: 0.2360\n",
      "Epoch: 19 | Training Batch: 40 | Average loss: 0.2317\n",
      "Epoch: 19 | Training Batch: 50 | Average loss: 0.2382\n",
      "Epoch: 19 | Training Batch: 60 | Average loss: 0.2368\n",
      "Epoch: 19 | Training Batch: 70 | Average loss: 0.2343\n",
      "Epoch: 19 | Training Batch: 80 | Average loss: 0.2390\n",
      "Epoch: 19 | Training Batch: 90 | Average loss: 0.2380\n",
      "Epoch: 19 | Training Batch: 100 | Average loss: 0.2354\n",
      "Epoch: 19 | Training Batch: 110 | Average loss: 0.2339\n",
      "Epoch: 19 | Training Batch: 120 | Average loss: 0.2378\n",
      "Epoch: 19 | Training Batch: 130 | Average loss: 0.2365\n",
      "Epoch: 19 | Training Batch: 140 | Average loss: 0.2432\n",
      "Epoch: 19 | Training Batch: 150 | Average loss: 0.2361\n",
      "Epoch: 19 | Training Batch: 160 | Average loss: 0.2359\n",
      "Epoch: 19 | Training Batch: 170 | Average loss: 0.2357\n",
      "Epoch: 19 | Training Batch: 180 | Average loss: 0.2343\n",
      "Epoch: 19 | Training Batch: 190 | Average loss: 0.2388\n",
      "Epoch: 19 | Training Batch: 200 | Average loss: 0.2407\n",
      "Epoch: 19 | Training Batch: 210 | Average loss: 0.2363\n",
      "Epoch: 19 | Training Batch: 220 | Average loss: 0.2356\n",
      "Average training batch loss at epoch 19: 0.2580\n",
      "Average validation fold accuracy at epoch 19: 0.2627\n",
      "Epoch: 20 | Training Batch: 10 | Average loss: 0.2361\n",
      "Epoch: 20 | Training Batch: 20 | Average loss: 0.2366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Training Batch: 30 | Average loss: 0.2384\n",
      "Epoch: 20 | Training Batch: 40 | Average loss: 0.2358\n",
      "Epoch: 20 | Training Batch: 50 | Average loss: 0.2337\n",
      "Epoch: 20 | Training Batch: 60 | Average loss: 0.2359\n",
      "Epoch: 20 | Training Batch: 70 | Average loss: 0.2341\n",
      "Epoch: 20 | Training Batch: 80 | Average loss: 0.2393\n",
      "Epoch: 20 | Training Batch: 90 | Average loss: 0.2326\n",
      "Epoch: 20 | Training Batch: 100 | Average loss: 0.2361\n",
      "Epoch: 20 | Training Batch: 110 | Average loss: 0.2377\n",
      "Epoch: 20 | Training Batch: 120 | Average loss: 0.2344\n",
      "Epoch: 20 | Training Batch: 130 | Average loss: 0.2377\n",
      "Epoch: 20 | Training Batch: 140 | Average loss: 0.2367\n",
      "Epoch: 20 | Training Batch: 150 | Average loss: 0.2323\n",
      "Epoch: 20 | Training Batch: 160 | Average loss: 0.2389\n",
      "Epoch: 20 | Training Batch: 170 | Average loss: 0.2410\n",
      "Epoch: 20 | Training Batch: 180 | Average loss: 0.2383\n",
      "Epoch: 20 | Training Batch: 190 | Average loss: 0.2358\n",
      "Epoch: 20 | Training Batch: 200 | Average loss: 0.2313\n",
      "Epoch: 20 | Training Batch: 210 | Average loss: 0.2375\n",
      "Epoch: 20 | Training Batch: 220 | Average loss: 0.2365\n",
      "Average training batch loss at epoch 20: 0.2570\n",
      "Average validation fold accuracy at epoch 20: 0.2622\n",
      "Epoch: 21 | Training Batch: 10 | Average loss: 0.2369\n",
      "Epoch: 21 | Training Batch: 20 | Average loss: 0.2361\n",
      "Epoch: 21 | Training Batch: 30 | Average loss: 0.2378\n",
      "Epoch: 21 | Training Batch: 40 | Average loss: 0.2367\n",
      "Epoch: 21 | Training Batch: 50 | Average loss: 0.2375\n",
      "Epoch: 21 | Training Batch: 60 | Average loss: 0.2367\n",
      "Epoch: 21 | Training Batch: 70 | Average loss: 0.2315\n",
      "Epoch: 21 | Training Batch: 80 | Average loss: 0.2367\n",
      "Epoch: 21 | Training Batch: 90 | Average loss: 0.2359\n",
      "Epoch: 21 | Training Batch: 100 | Average loss: 0.2351\n",
      "Epoch: 21 | Training Batch: 110 | Average loss: 0.2352\n",
      "Epoch: 21 | Training Batch: 120 | Average loss: 0.2358\n",
      "Epoch: 21 | Training Batch: 130 | Average loss: 0.2358\n",
      "Epoch: 21 | Training Batch: 140 | Average loss: 0.2348\n",
      "Epoch: 21 | Training Batch: 150 | Average loss: 0.2386\n",
      "Epoch: 21 | Training Batch: 160 | Average loss: 0.2392\n",
      "Epoch: 21 | Training Batch: 170 | Average loss: 0.2348\n",
      "Epoch: 21 | Training Batch: 180 | Average loss: 0.2315\n",
      "Epoch: 21 | Training Batch: 190 | Average loss: 0.2379\n",
      "Epoch: 21 | Training Batch: 200 | Average loss: 0.2361\n",
      "Epoch: 21 | Training Batch: 210 | Average loss: 0.2332\n",
      "Epoch: 21 | Training Batch: 220 | Average loss: 0.2390\n",
      "Average training batch loss at epoch 21: 0.2560\n",
      "Average validation fold accuracy at epoch 21: 0.2618\n",
      "Epoch: 22 | Training Batch: 10 | Average loss: 0.2326\n",
      "Epoch: 22 | Training Batch: 20 | Average loss: 0.2364\n",
      "Epoch: 22 | Training Batch: 30 | Average loss: 0.2322\n",
      "Epoch: 22 | Training Batch: 40 | Average loss: 0.2312\n",
      "Epoch: 22 | Training Batch: 50 | Average loss: 0.2385\n",
      "Epoch: 22 | Training Batch: 60 | Average loss: 0.2410\n",
      "Epoch: 22 | Training Batch: 70 | Average loss: 0.2348\n",
      "Epoch: 22 | Training Batch: 80 | Average loss: 0.2413\n",
      "Epoch: 22 | Training Batch: 90 | Average loss: 0.2357\n",
      "Epoch: 22 | Training Batch: 100 | Average loss: 0.2337\n",
      "Epoch: 22 | Training Batch: 110 | Average loss: 0.2336\n",
      "Epoch: 22 | Training Batch: 120 | Average loss: 0.2357\n",
      "Epoch: 22 | Training Batch: 130 | Average loss: 0.2379\n",
      "Epoch: 22 | Training Batch: 140 | Average loss: 0.2368\n",
      "Epoch: 22 | Training Batch: 150 | Average loss: 0.2362\n",
      "Epoch: 22 | Training Batch: 160 | Average loss: 0.2364\n",
      "Epoch: 22 | Training Batch: 170 | Average loss: 0.2333\n",
      "Epoch: 22 | Training Batch: 180 | Average loss: 0.2344\n",
      "Epoch: 22 | Training Batch: 190 | Average loss: 0.2400\n",
      "Epoch: 22 | Training Batch: 200 | Average loss: 0.2369\n",
      "Epoch: 22 | Training Batch: 210 | Average loss: 0.2351\n",
      "Epoch: 22 | Training Batch: 220 | Average loss: 0.2374\n",
      "Average training batch loss at epoch 22: 0.2552\n",
      "Average validation fold accuracy at epoch 22: 0.2615\n",
      "Epoch: 23 | Training Batch: 10 | Average loss: 0.2390\n",
      "Epoch: 23 | Training Batch: 20 | Average loss: 0.2360\n",
      "Epoch: 23 | Training Batch: 30 | Average loss: 0.2350\n",
      "Epoch: 23 | Training Batch: 40 | Average loss: 0.2333\n",
      "Epoch: 23 | Training Batch: 50 | Average loss: 0.2361\n",
      "Epoch: 23 | Training Batch: 60 | Average loss: 0.2365\n",
      "Epoch: 23 | Training Batch: 70 | Average loss: 0.2339\n",
      "Epoch: 23 | Training Batch: 80 | Average loss: 0.2342\n",
      "Epoch: 23 | Training Batch: 90 | Average loss: 0.2324\n",
      "Epoch: 23 | Training Batch: 100 | Average loss: 0.2325\n",
      "Epoch: 23 | Training Batch: 110 | Average loss: 0.2357\n",
      "Epoch: 23 | Training Batch: 120 | Average loss: 0.2410\n",
      "Epoch: 23 | Training Batch: 130 | Average loss: 0.2403\n",
      "Epoch: 23 | Training Batch: 140 | Average loss: 0.2377\n",
      "Epoch: 23 | Training Batch: 150 | Average loss: 0.2394\n",
      "Epoch: 23 | Training Batch: 160 | Average loss: 0.2344\n",
      "Epoch: 23 | Training Batch: 170 | Average loss: 0.2367\n",
      "Epoch: 23 | Training Batch: 180 | Average loss: 0.2337\n",
      "Epoch: 23 | Training Batch: 190 | Average loss: 0.2329\n",
      "Epoch: 23 | Training Batch: 200 | Average loss: 0.2337\n",
      "Epoch: 23 | Training Batch: 210 | Average loss: 0.2359\n",
      "Epoch: 23 | Training Batch: 220 | Average loss: 0.2346\n",
      "Average training batch loss at epoch 23: 0.2543\n",
      "Average validation fold accuracy at epoch 23: 0.2612\n",
      "Epoch: 24 | Training Batch: 10 | Average loss: 0.2344\n",
      "Epoch: 24 | Training Batch: 20 | Average loss: 0.2342\n",
      "Epoch: 24 | Training Batch: 30 | Average loss: 0.2371\n",
      "Epoch: 24 | Training Batch: 40 | Average loss: 0.2363\n",
      "Epoch: 24 | Training Batch: 50 | Average loss: 0.2336\n",
      "Epoch: 24 | Training Batch: 60 | Average loss: 0.2314\n",
      "Epoch: 24 | Training Batch: 70 | Average loss: 0.2357\n",
      "Epoch: 24 | Training Batch: 80 | Average loss: 0.2379\n",
      "Epoch: 24 | Training Batch: 90 | Average loss: 0.2354\n",
      "Epoch: 24 | Training Batch: 100 | Average loss: 0.2377\n",
      "Epoch: 24 | Training Batch: 110 | Average loss: 0.2318\n",
      "Epoch: 24 | Training Batch: 120 | Average loss: 0.2358\n",
      "Epoch: 24 | Training Batch: 130 | Average loss: 0.2361\n",
      "Epoch: 24 | Training Batch: 140 | Average loss: 0.2353\n",
      "Epoch: 24 | Training Batch: 150 | Average loss: 0.2305\n",
      "Epoch: 24 | Training Batch: 160 | Average loss: 0.2403\n",
      "Epoch: 24 | Training Batch: 170 | Average loss: 0.2358\n",
      "Epoch: 24 | Training Batch: 180 | Average loss: 0.2353\n",
      "Epoch: 24 | Training Batch: 190 | Average loss: 0.2360\n",
      "Epoch: 24 | Training Batch: 200 | Average loss: 0.2355\n",
      "Epoch: 24 | Training Batch: 210 | Average loss: 0.2398\n",
      "Epoch: 24 | Training Batch: 220 | Average loss: 0.2343\n",
      "Average training batch loss at epoch 24: 0.2536\n",
      "Average validation fold accuracy at epoch 24: 0.2609\n",
      "Epoch: 25 | Training Batch: 10 | Average loss: 0.2342\n",
      "Epoch: 25 | Training Batch: 20 | Average loss: 0.2383\n",
      "Epoch: 25 | Training Batch: 30 | Average loss: 0.2337\n",
      "Epoch: 25 | Training Batch: 40 | Average loss: 0.2373\n",
      "Epoch: 25 | Training Batch: 50 | Average loss: 0.2372\n",
      "Epoch: 25 | Training Batch: 60 | Average loss: 0.2359\n",
      "Epoch: 25 | Training Batch: 70 | Average loss: 0.2351\n",
      "Epoch: 25 | Training Batch: 80 | Average loss: 0.2346\n",
      "Epoch: 25 | Training Batch: 90 | Average loss: 0.2376\n",
      "Epoch: 25 | Training Batch: 100 | Average loss: 0.2346\n",
      "Epoch: 25 | Training Batch: 110 | Average loss: 0.2342\n",
      "Epoch: 25 | Training Batch: 120 | Average loss: 0.2320\n",
      "Epoch: 25 | Training Batch: 130 | Average loss: 0.2359\n",
      "Epoch: 25 | Training Batch: 140 | Average loss: 0.2338\n",
      "Epoch: 25 | Training Batch: 150 | Average loss: 0.2397\n",
      "Epoch: 25 | Training Batch: 160 | Average loss: 0.2339\n",
      "Epoch: 25 | Training Batch: 170 | Average loss: 0.2354\n",
      "Epoch: 25 | Training Batch: 180 | Average loss: 0.2344\n",
      "Epoch: 25 | Training Batch: 190 | Average loss: 0.2362\n",
      "Epoch: 25 | Training Batch: 200 | Average loss: 0.2357\n",
      "Epoch: 25 | Training Batch: 210 | Average loss: 0.2372\n",
      "Epoch: 25 | Training Batch: 220 | Average loss: 0.2302\n",
      "Average training batch loss at epoch 25: 0.2529\n",
      "Average validation fold accuracy at epoch 25: 0.2606\n",
      "Epoch: 26 | Training Batch: 10 | Average loss: 0.2395\n",
      "Epoch: 26 | Training Batch: 20 | Average loss: 0.2320\n",
      "Epoch: 26 | Training Batch: 30 | Average loss: 0.2345\n",
      "Epoch: 26 | Training Batch: 40 | Average loss: 0.2378\n",
      "Epoch: 26 | Training Batch: 50 | Average loss: 0.2351\n",
      "Epoch: 26 | Training Batch: 60 | Average loss: 0.2386\n",
      "Epoch: 26 | Training Batch: 70 | Average loss: 0.2318\n",
      "Epoch: 26 | Training Batch: 80 | Average loss: 0.2349\n",
      "Epoch: 26 | Training Batch: 90 | Average loss: 0.2321\n",
      "Epoch: 26 | Training Batch: 100 | Average loss: 0.2325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | Training Batch: 110 | Average loss: 0.2356\n",
      "Epoch: 26 | Training Batch: 120 | Average loss: 0.2328\n",
      "Epoch: 26 | Training Batch: 130 | Average loss: 0.2340\n",
      "Epoch: 26 | Training Batch: 140 | Average loss: 0.2346\n",
      "Epoch: 26 | Training Batch: 150 | Average loss: 0.2352\n",
      "Epoch: 26 | Training Batch: 160 | Average loss: 0.2394\n",
      "Epoch: 26 | Training Batch: 170 | Average loss: 0.2370\n",
      "Epoch: 26 | Training Batch: 180 | Average loss: 0.2334\n",
      "Epoch: 26 | Training Batch: 190 | Average loss: 0.2366\n",
      "Epoch: 26 | Training Batch: 200 | Average loss: 0.2370\n",
      "Epoch: 26 | Training Batch: 210 | Average loss: 0.2324\n",
      "Epoch: 26 | Training Batch: 220 | Average loss: 0.2370\n",
      "Average training batch loss at epoch 26: 0.2522\n",
      "Average validation fold accuracy at epoch 26: 0.2604\n",
      "Epoch: 27 | Training Batch: 10 | Average loss: 0.2373\n",
      "Epoch: 27 | Training Batch: 20 | Average loss: 0.2375\n",
      "Epoch: 27 | Training Batch: 30 | Average loss: 0.2265\n",
      "Epoch: 27 | Training Batch: 40 | Average loss: 0.2336\n",
      "Epoch: 27 | Training Batch: 50 | Average loss: 0.2365\n",
      "Epoch: 27 | Training Batch: 60 | Average loss: 0.2371\n",
      "Epoch: 27 | Training Batch: 70 | Average loss: 0.2353\n",
      "Epoch: 27 | Training Batch: 80 | Average loss: 0.2295\n",
      "Epoch: 27 | Training Batch: 90 | Average loss: 0.2359\n",
      "Epoch: 27 | Training Batch: 100 | Average loss: 0.2343\n",
      "Epoch: 27 | Training Batch: 110 | Average loss: 0.2376\n",
      "Epoch: 27 | Training Batch: 120 | Average loss: 0.2325\n",
      "Epoch: 27 | Training Batch: 130 | Average loss: 0.2353\n",
      "Epoch: 27 | Training Batch: 140 | Average loss: 0.2355\n",
      "Epoch: 27 | Training Batch: 150 | Average loss: 0.2339\n",
      "Epoch: 27 | Training Batch: 160 | Average loss: 0.2307\n",
      "Epoch: 27 | Training Batch: 170 | Average loss: 0.2392\n",
      "Epoch: 27 | Training Batch: 180 | Average loss: 0.2379\n",
      "Epoch: 27 | Training Batch: 190 | Average loss: 0.2363\n",
      "Epoch: 27 | Training Batch: 200 | Average loss: 0.2380\n",
      "Epoch: 27 | Training Batch: 210 | Average loss: 0.2370\n",
      "Epoch: 27 | Training Batch: 220 | Average loss: 0.2348\n",
      "Average training batch loss at epoch 27: 0.2516\n",
      "Average validation fold accuracy at epoch 27: 0.2601\n",
      "Epoch: 28 | Training Batch: 10 | Average loss: 0.2365\n",
      "Epoch: 28 | Training Batch: 20 | Average loss: 0.2347\n",
      "Epoch: 28 | Training Batch: 30 | Average loss: 0.2326\n",
      "Epoch: 28 | Training Batch: 40 | Average loss: 0.2361\n",
      "Epoch: 28 | Training Batch: 50 | Average loss: 0.2351\n",
      "Epoch: 28 | Training Batch: 60 | Average loss: 0.2318\n",
      "Epoch: 28 | Training Batch: 70 | Average loss: 0.2359\n",
      "Epoch: 28 | Training Batch: 80 | Average loss: 0.2340\n",
      "Epoch: 28 | Training Batch: 90 | Average loss: 0.2316\n",
      "Epoch: 28 | Training Batch: 100 | Average loss: 0.2338\n",
      "Epoch: 28 | Training Batch: 110 | Average loss: 0.2353\n",
      "Epoch: 28 | Training Batch: 120 | Average loss: 0.2321\n",
      "Epoch: 28 | Training Batch: 130 | Average loss: 0.2367\n",
      "Epoch: 28 | Training Batch: 140 | Average loss: 0.2393\n",
      "Epoch: 28 | Training Batch: 150 | Average loss: 0.2356\n",
      "Epoch: 28 | Training Batch: 160 | Average loss: 0.2360\n",
      "Epoch: 28 | Training Batch: 170 | Average loss: 0.2338\n",
      "Epoch: 28 | Training Batch: 180 | Average loss: 0.2339\n",
      "Epoch: 28 | Training Batch: 190 | Average loss: 0.2345\n",
      "Epoch: 28 | Training Batch: 200 | Average loss: 0.2351\n",
      "Epoch: 28 | Training Batch: 210 | Average loss: 0.2340\n",
      "Epoch: 28 | Training Batch: 220 | Average loss: 0.2384\n",
      "Average training batch loss at epoch 28: 0.2510\n",
      "Average validation fold accuracy at epoch 28: 0.2599\n",
      "Epoch: 29 | Training Batch: 10 | Average loss: 0.2303\n",
      "Epoch: 29 | Training Batch: 20 | Average loss: 0.2349\n",
      "Epoch: 29 | Training Batch: 30 | Average loss: 0.2352\n",
      "Epoch: 29 | Training Batch: 40 | Average loss: 0.2320\n",
      "Epoch: 29 | Training Batch: 50 | Average loss: 0.2349\n",
      "Epoch: 29 | Training Batch: 60 | Average loss: 0.2380\n",
      "Epoch: 29 | Training Batch: 70 | Average loss: 0.2363\n",
      "Epoch: 29 | Training Batch: 80 | Average loss: 0.2369\n",
      "Epoch: 29 | Training Batch: 90 | Average loss: 0.2349\n",
      "Epoch: 29 | Training Batch: 100 | Average loss: 0.2359\n",
      "Epoch: 29 | Training Batch: 110 | Average loss: 0.2320\n",
      "Epoch: 29 | Training Batch: 120 | Average loss: 0.2345\n",
      "Epoch: 29 | Training Batch: 130 | Average loss: 0.2354\n",
      "Epoch: 29 | Training Batch: 140 | Average loss: 0.2339\n",
      "Epoch: 29 | Training Batch: 150 | Average loss: 0.2380\n",
      "Epoch: 29 | Training Batch: 160 | Average loss: 0.2331\n",
      "Epoch: 29 | Training Batch: 170 | Average loss: 0.2325\n",
      "Epoch: 29 | Training Batch: 180 | Average loss: 0.2364\n",
      "Epoch: 29 | Training Batch: 190 | Average loss: 0.2354\n",
      "Epoch: 29 | Training Batch: 200 | Average loss: 0.2323\n",
      "Epoch: 29 | Training Batch: 210 | Average loss: 0.2368\n",
      "Epoch: 29 | Training Batch: 220 | Average loss: 0.2347\n",
      "Average training batch loss at epoch 29: 0.2505\n",
      "Average validation fold accuracy at epoch 29: 0.2597\n",
      "Epoch: 30 | Training Batch: 10 | Average loss: 0.2351\n",
      "Epoch: 30 | Training Batch: 20 | Average loss: 0.2328\n",
      "Epoch: 30 | Training Batch: 30 | Average loss: 0.2369\n",
      "Epoch: 30 | Training Batch: 40 | Average loss: 0.2343\n",
      "Epoch: 30 | Training Batch: 50 | Average loss: 0.2341\n",
      "Epoch: 30 | Training Batch: 60 | Average loss: 0.2384\n",
      "Epoch: 30 | Training Batch: 70 | Average loss: 0.2352\n",
      "Epoch: 30 | Training Batch: 80 | Average loss: 0.2315\n",
      "Epoch: 30 | Training Batch: 90 | Average loss: 0.2338\n",
      "Epoch: 30 | Training Batch: 100 | Average loss: 0.2327\n",
      "Epoch: 30 | Training Batch: 110 | Average loss: 0.2323\n",
      "Epoch: 30 | Training Batch: 120 | Average loss: 0.2361\n",
      "Epoch: 30 | Training Batch: 130 | Average loss: 0.2358\n",
      "Epoch: 30 | Training Batch: 140 | Average loss: 0.2342\n",
      "Epoch: 30 | Training Batch: 150 | Average loss: 0.2348\n",
      "Epoch: 30 | Training Batch: 160 | Average loss: 0.2353\n",
      "Epoch: 30 | Training Batch: 170 | Average loss: 0.2339\n",
      "Epoch: 30 | Training Batch: 180 | Average loss: 0.2341\n",
      "Epoch: 30 | Training Batch: 190 | Average loss: 0.2311\n",
      "Epoch: 30 | Training Batch: 200 | Average loss: 0.2329\n",
      "Epoch: 30 | Training Batch: 210 | Average loss: 0.2396\n",
      "Epoch: 30 | Training Batch: 220 | Average loss: 0.2366\n",
      "Average training batch loss at epoch 30: 0.2500\n",
      "Average validation fold accuracy at epoch 30: 0.2595\n",
      "Epoch: 31 | Training Batch: 10 | Average loss: 0.2333\n",
      "Epoch: 31 | Training Batch: 20 | Average loss: 0.2344\n",
      "Epoch: 31 | Training Batch: 30 | Average loss: 0.2297\n",
      "Epoch: 31 | Training Batch: 40 | Average loss: 0.2329\n",
      "Epoch: 31 | Training Batch: 50 | Average loss: 0.2343\n",
      "Epoch: 31 | Training Batch: 60 | Average loss: 0.2379\n",
      "Epoch: 31 | Training Batch: 70 | Average loss: 0.2369\n",
      "Epoch: 31 | Training Batch: 80 | Average loss: 0.2355\n",
      "Epoch: 31 | Training Batch: 90 | Average loss: 0.2356\n",
      "Epoch: 31 | Training Batch: 100 | Average loss: 0.2350\n",
      "Epoch: 31 | Training Batch: 110 | Average loss: 0.2332\n",
      "Epoch: 31 | Training Batch: 120 | Average loss: 0.2329\n",
      "Epoch: 31 | Training Batch: 130 | Average loss: 0.2364\n",
      "Epoch: 31 | Training Batch: 140 | Average loss: 0.2319\n",
      "Epoch: 31 | Training Batch: 150 | Average loss: 0.2348\n",
      "Epoch: 31 | Training Batch: 160 | Average loss: 0.2343\n",
      "Epoch: 31 | Training Batch: 170 | Average loss: 0.2301\n",
      "Epoch: 31 | Training Batch: 180 | Average loss: 0.2376\n",
      "Epoch: 31 | Training Batch: 190 | Average loss: 0.2342\n",
      "Epoch: 31 | Training Batch: 200 | Average loss: 0.2371\n",
      "Epoch: 31 | Training Batch: 210 | Average loss: 0.2378\n",
      "Epoch: 31 | Training Batch: 220 | Average loss: 0.2331\n",
      "Average training batch loss at epoch 31: 0.2495\n",
      "Average validation fold accuracy at epoch 31: 0.2593\n",
      "Epoch: 32 | Training Batch: 10 | Average loss: 0.2300\n",
      "Epoch: 32 | Training Batch: 20 | Average loss: 0.2340\n",
      "Epoch: 32 | Training Batch: 30 | Average loss: 0.2349\n",
      "Epoch: 32 | Training Batch: 40 | Average loss: 0.2363\n",
      "Epoch: 32 | Training Batch: 50 | Average loss: 0.2336\n",
      "Epoch: 32 | Training Batch: 60 | Average loss: 0.2337\n",
      "Epoch: 32 | Training Batch: 70 | Average loss: 0.2354\n",
      "Epoch: 32 | Training Batch: 80 | Average loss: 0.2330\n",
      "Epoch: 32 | Training Batch: 90 | Average loss: 0.2380\n",
      "Epoch: 32 | Training Batch: 100 | Average loss: 0.2338\n",
      "Epoch: 32 | Training Batch: 110 | Average loss: 0.2327\n",
      "Epoch: 32 | Training Batch: 120 | Average loss: 0.2300\n",
      "Epoch: 32 | Training Batch: 130 | Average loss: 0.2338\n",
      "Epoch: 32 | Training Batch: 140 | Average loss: 0.2338\n",
      "Epoch: 32 | Training Batch: 150 | Average loss: 0.2343\n",
      "Epoch: 32 | Training Batch: 160 | Average loss: 0.2326\n",
      "Epoch: 32 | Training Batch: 170 | Average loss: 0.2355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32 | Training Batch: 180 | Average loss: 0.2358\n",
      "Epoch: 32 | Training Batch: 190 | Average loss: 0.2357\n",
      "Epoch: 32 | Training Batch: 200 | Average loss: 0.2332\n",
      "Epoch: 32 | Training Batch: 210 | Average loss: 0.2352\n",
      "Epoch: 32 | Training Batch: 220 | Average loss: 0.2367\n",
      "Average training batch loss at epoch 32: 0.2490\n",
      "Average validation fold accuracy at epoch 32: 0.2592\n",
      "Epoch: 33 | Training Batch: 10 | Average loss: 0.2320\n",
      "Epoch: 33 | Training Batch: 20 | Average loss: 0.2341\n",
      "Epoch: 33 | Training Batch: 30 | Average loss: 0.2329\n",
      "Epoch: 33 | Training Batch: 40 | Average loss: 0.2297\n",
      "Epoch: 33 | Training Batch: 50 | Average loss: 0.2317\n",
      "Epoch: 33 | Training Batch: 60 | Average loss: 0.2354\n",
      "Epoch: 33 | Training Batch: 70 | Average loss: 0.2347\n",
      "Epoch: 33 | Training Batch: 80 | Average loss: 0.2329\n",
      "Epoch: 33 | Training Batch: 90 | Average loss: 0.2362\n",
      "Epoch: 33 | Training Batch: 100 | Average loss: 0.2324\n",
      "Epoch: 33 | Training Batch: 110 | Average loss: 0.2334\n",
      "Epoch: 33 | Training Batch: 120 | Average loss: 0.2367\n",
      "Epoch: 33 | Training Batch: 130 | Average loss: 0.2351\n",
      "Epoch: 33 | Training Batch: 140 | Average loss: 0.2325\n",
      "Epoch: 33 | Training Batch: 150 | Average loss: 0.2333\n",
      "Epoch: 33 | Training Batch: 160 | Average loss: 0.2342\n",
      "Epoch: 33 | Training Batch: 170 | Average loss: 0.2320\n",
      "Epoch: 33 | Training Batch: 180 | Average loss: 0.2387\n",
      "Epoch: 33 | Training Batch: 190 | Average loss: 0.2346\n",
      "Epoch: 33 | Training Batch: 200 | Average loss: 0.2352\n",
      "Epoch: 33 | Training Batch: 210 | Average loss: 0.2319\n",
      "Epoch: 33 | Training Batch: 220 | Average loss: 0.2361\n",
      "Average training batch loss at epoch 33: 0.2486\n",
      "Average validation fold accuracy at epoch 33: 0.2590\n",
      "Epoch: 34 | Training Batch: 10 | Average loss: 0.2329\n",
      "Epoch: 34 | Training Batch: 20 | Average loss: 0.2319\n",
      "Epoch: 34 | Training Batch: 30 | Average loss: 0.2324\n",
      "Epoch: 34 | Training Batch: 40 | Average loss: 0.2350\n",
      "Epoch: 34 | Training Batch: 50 | Average loss: 0.2336\n",
      "Epoch: 34 | Training Batch: 60 | Average loss: 0.2343\n",
      "Epoch: 34 | Training Batch: 70 | Average loss: 0.2337\n",
      "Epoch: 34 | Training Batch: 80 | Average loss: 0.2354\n",
      "Epoch: 34 | Training Batch: 90 | Average loss: 0.2346\n",
      "Epoch: 34 | Training Batch: 100 | Average loss: 0.2321\n",
      "Epoch: 34 | Training Batch: 110 | Average loss: 0.2371\n",
      "Epoch: 34 | Training Batch: 120 | Average loss: 0.2304\n",
      "Epoch: 34 | Training Batch: 130 | Average loss: 0.2370\n",
      "Epoch: 34 | Training Batch: 140 | Average loss: 0.2345\n",
      "Epoch: 34 | Training Batch: 150 | Average loss: 0.2335\n",
      "Epoch: 34 | Training Batch: 160 | Average loss: 0.2338\n",
      "Epoch: 34 | Training Batch: 170 | Average loss: 0.2353\n",
      "Epoch: 34 | Training Batch: 180 | Average loss: 0.2361\n",
      "Epoch: 34 | Training Batch: 190 | Average loss: 0.2316\n",
      "Epoch: 34 | Training Batch: 200 | Average loss: 0.2336\n",
      "Epoch: 34 | Training Batch: 210 | Average loss: 0.2329\n",
      "Epoch: 34 | Training Batch: 220 | Average loss: 0.2322\n",
      "Average training batch loss at epoch 34: 0.2482\n",
      "Average validation fold accuracy at epoch 34: 0.2589\n",
      "Epoch: 35 | Training Batch: 10 | Average loss: 0.2331\n",
      "Epoch: 35 | Training Batch: 20 | Average loss: 0.2282\n",
      "Epoch: 35 | Training Batch: 30 | Average loss: 0.2350\n",
      "Epoch: 35 | Training Batch: 40 | Average loss: 0.2377\n",
      "Epoch: 35 | Training Batch: 50 | Average loss: 0.2307\n",
      "Epoch: 35 | Training Batch: 60 | Average loss: 0.2364\n",
      "Epoch: 35 | Training Batch: 70 | Average loss: 0.2356\n",
      "Epoch: 35 | Training Batch: 80 | Average loss: 0.2326\n",
      "Epoch: 35 | Training Batch: 90 | Average loss: 0.2337\n",
      "Epoch: 35 | Training Batch: 100 | Average loss: 0.2345\n",
      "Epoch: 35 | Training Batch: 110 | Average loss: 0.2347\n",
      "Epoch: 35 | Training Batch: 120 | Average loss: 0.2314\n",
      "Epoch: 35 | Training Batch: 130 | Average loss: 0.2322\n",
      "Epoch: 35 | Training Batch: 140 | Average loss: 0.2315\n",
      "Epoch: 35 | Training Batch: 150 | Average loss: 0.2308\n",
      "Epoch: 35 | Training Batch: 160 | Average loss: 0.2338\n",
      "Epoch: 35 | Training Batch: 170 | Average loss: 0.2308\n",
      "Epoch: 35 | Training Batch: 180 | Average loss: 0.2368\n",
      "Epoch: 35 | Training Batch: 190 | Average loss: 0.2353\n",
      "Epoch: 35 | Training Batch: 200 | Average loss: 0.2327\n",
      "Epoch: 35 | Training Batch: 210 | Average loss: 0.2378\n",
      "Epoch: 35 | Training Batch: 220 | Average loss: 0.2348\n",
      "Average training batch loss at epoch 35: 0.2478\n",
      "Average validation fold accuracy at epoch 35: 0.2587\n",
      "Epoch: 36 | Training Batch: 10 | Average loss: 0.2327\n",
      "Epoch: 36 | Training Batch: 20 | Average loss: 0.2311\n",
      "Epoch: 36 | Training Batch: 30 | Average loss: 0.2353\n",
      "Epoch: 36 | Training Batch: 40 | Average loss: 0.2393\n",
      "Epoch: 36 | Training Batch: 50 | Average loss: 0.2379\n",
      "Epoch: 36 | Training Batch: 60 | Average loss: 0.2323\n",
      "Epoch: 36 | Training Batch: 70 | Average loss: 0.2339\n",
      "Epoch: 36 | Training Batch: 80 | Average loss: 0.2312\n",
      "Epoch: 36 | Training Batch: 90 | Average loss: 0.2342\n",
      "Epoch: 36 | Training Batch: 100 | Average loss: 0.2319\n",
      "Epoch: 36 | Training Batch: 110 | Average loss: 0.2346\n",
      "Epoch: 36 | Training Batch: 120 | Average loss: 0.2310\n",
      "Epoch: 36 | Training Batch: 130 | Average loss: 0.2342\n",
      "Epoch: 36 | Training Batch: 140 | Average loss: 0.2373\n",
      "Epoch: 36 | Training Batch: 150 | Average loss: 0.2355\n",
      "Epoch: 36 | Training Batch: 160 | Average loss: 0.2328\n",
      "Epoch: 36 | Training Batch: 170 | Average loss: 0.2290\n",
      "Epoch: 36 | Training Batch: 180 | Average loss: 0.2349\n",
      "Epoch: 36 | Training Batch: 190 | Average loss: 0.2287\n",
      "Epoch: 36 | Training Batch: 200 | Average loss: 0.2371\n",
      "Epoch: 36 | Training Batch: 210 | Average loss: 0.2290\n",
      "Epoch: 36 | Training Batch: 220 | Average loss: 0.2349\n",
      "Average training batch loss at epoch 36: 0.2474\n",
      "Average validation fold accuracy at epoch 36: 0.2586\n",
      "Epoch: 37 | Training Batch: 10 | Average loss: 0.2312\n",
      "Epoch: 37 | Training Batch: 20 | Average loss: 0.2325\n",
      "Epoch: 37 | Training Batch: 30 | Average loss: 0.2331\n",
      "Epoch: 37 | Training Batch: 40 | Average loss: 0.2356\n",
      "Epoch: 37 | Training Batch: 50 | Average loss: 0.2343\n",
      "Epoch: 37 | Training Batch: 60 | Average loss: 0.2315\n",
      "Epoch: 37 | Training Batch: 70 | Average loss: 0.2320\n",
      "Epoch: 37 | Training Batch: 80 | Average loss: 0.2358\n",
      "Epoch: 37 | Training Batch: 90 | Average loss: 0.2313\n",
      "Epoch: 37 | Training Batch: 100 | Average loss: 0.2317\n",
      "Epoch: 37 | Training Batch: 110 | Average loss: 0.2385\n",
      "Epoch: 37 | Training Batch: 120 | Average loss: 0.2365\n",
      "Epoch: 37 | Training Batch: 130 | Average loss: 0.2343\n",
      "Epoch: 37 | Training Batch: 140 | Average loss: 0.2348\n",
      "Epoch: 37 | Training Batch: 150 | Average loss: 0.2364\n",
      "Epoch: 37 | Training Batch: 160 | Average loss: 0.2312\n",
      "Epoch: 37 | Training Batch: 170 | Average loss: 0.2349\n",
      "Epoch: 37 | Training Batch: 180 | Average loss: 0.2305\n",
      "Epoch: 37 | Training Batch: 190 | Average loss: 0.2319\n",
      "Epoch: 37 | Training Batch: 200 | Average loss: 0.2322\n",
      "Epoch: 37 | Training Batch: 210 | Average loss: 0.2347\n",
      "Epoch: 37 | Training Batch: 220 | Average loss: 0.2300\n",
      "Average training batch loss at epoch 37: 0.2470\n",
      "Average validation fold accuracy at epoch 37: 0.2585\n",
      "Epoch: 38 | Training Batch: 10 | Average loss: 0.2294\n",
      "Epoch: 38 | Training Batch: 20 | Average loss: 0.2318\n",
      "Epoch: 38 | Training Batch: 30 | Average loss: 0.2371\n",
      "Epoch: 38 | Training Batch: 40 | Average loss: 0.2331\n",
      "Epoch: 38 | Training Batch: 50 | Average loss: 0.2301\n",
      "Epoch: 38 | Training Batch: 60 | Average loss: 0.2335\n",
      "Epoch: 38 | Training Batch: 70 | Average loss: 0.2314\n",
      "Epoch: 38 | Training Batch: 80 | Average loss: 0.2342\n",
      "Epoch: 38 | Training Batch: 90 | Average loss: 0.2323\n",
      "Epoch: 38 | Training Batch: 100 | Average loss: 0.2331\n",
      "Epoch: 38 | Training Batch: 110 | Average loss: 0.2342\n",
      "Epoch: 38 | Training Batch: 120 | Average loss: 0.2345\n",
      "Epoch: 38 | Training Batch: 130 | Average loss: 0.2356\n",
      "Epoch: 38 | Training Batch: 140 | Average loss: 0.2340\n",
      "Epoch: 38 | Training Batch: 150 | Average loss: 0.2319\n",
      "Epoch: 38 | Training Batch: 160 | Average loss: 0.2321\n",
      "Epoch: 38 | Training Batch: 170 | Average loss: 0.2338\n",
      "Epoch: 38 | Training Batch: 180 | Average loss: 0.2358\n",
      "Epoch: 38 | Training Batch: 190 | Average loss: 0.2377\n",
      "Epoch: 38 | Training Batch: 200 | Average loss: 0.2329\n",
      "Epoch: 38 | Training Batch: 210 | Average loss: 0.2322\n",
      "Epoch: 38 | Training Batch: 220 | Average loss: 0.2321\n",
      "Average training batch loss at epoch 38: 0.2466\n",
      "Average validation fold accuracy at epoch 38: 0.2584\n",
      "Epoch: 39 | Training Batch: 10 | Average loss: 0.2294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39 | Training Batch: 20 | Average loss: 0.2345\n",
      "Epoch: 39 | Training Batch: 30 | Average loss: 0.2319\n",
      "Epoch: 39 | Training Batch: 40 | Average loss: 0.2319\n",
      "Epoch: 39 | Training Batch: 50 | Average loss: 0.2322\n",
      "Epoch: 39 | Training Batch: 60 | Average loss: 0.2369\n",
      "Epoch: 39 | Training Batch: 70 | Average loss: 0.2301\n",
      "Epoch: 39 | Training Batch: 80 | Average loss: 0.2377\n",
      "Epoch: 39 | Training Batch: 90 | Average loss: 0.2323\n",
      "Epoch: 39 | Training Batch: 100 | Average loss: 0.2368\n",
      "Epoch: 39 | Training Batch: 110 | Average loss: 0.2286\n",
      "Epoch: 39 | Training Batch: 120 | Average loss: 0.2337\n",
      "Epoch: 39 | Training Batch: 130 | Average loss: 0.2343\n",
      "Epoch: 39 | Training Batch: 140 | Average loss: 0.2384\n",
      "Epoch: 39 | Training Batch: 150 | Average loss: 0.2257\n",
      "Epoch: 39 | Training Batch: 160 | Average loss: 0.2306\n",
      "Epoch: 39 | Training Batch: 170 | Average loss: 0.2318\n",
      "Epoch: 39 | Training Batch: 180 | Average loss: 0.2335\n",
      "Epoch: 39 | Training Batch: 190 | Average loss: 0.2349\n",
      "Epoch: 39 | Training Batch: 200 | Average loss: 0.2326\n",
      "Epoch: 39 | Training Batch: 210 | Average loss: 0.2355\n",
      "Epoch: 39 | Training Batch: 220 | Average loss: 0.2368\n",
      "Average training batch loss at epoch 39: 0.2463\n",
      "Average validation fold accuracy at epoch 39: 0.2583\n",
      "Epoch: 40 | Training Batch: 10 | Average loss: 0.2332\n",
      "Epoch: 40 | Training Batch: 20 | Average loss: 0.2335\n",
      "Epoch: 40 | Training Batch: 30 | Average loss: 0.2346\n",
      "Epoch: 40 | Training Batch: 40 | Average loss: 0.2329\n",
      "Epoch: 40 | Training Batch: 50 | Average loss: 0.2336\n",
      "Epoch: 40 | Training Batch: 60 | Average loss: 0.2347\n",
      "Epoch: 40 | Training Batch: 70 | Average loss: 0.2350\n",
      "Epoch: 40 | Training Batch: 80 | Average loss: 0.2390\n",
      "Epoch: 40 | Training Batch: 90 | Average loss: 0.2279\n",
      "Epoch: 40 | Training Batch: 100 | Average loss: 0.2318\n",
      "Epoch: 40 | Training Batch: 110 | Average loss: 0.2291\n",
      "Epoch: 40 | Training Batch: 120 | Average loss: 0.2296\n",
      "Epoch: 40 | Training Batch: 130 | Average loss: 0.2347\n",
      "Epoch: 40 | Training Batch: 140 | Average loss: 0.2337\n",
      "Epoch: 40 | Training Batch: 150 | Average loss: 0.2370\n",
      "Epoch: 40 | Training Batch: 160 | Average loss: 0.2295\n",
      "Epoch: 40 | Training Batch: 170 | Average loss: 0.2362\n",
      "Epoch: 40 | Training Batch: 180 | Average loss: 0.2250\n",
      "Epoch: 40 | Training Batch: 190 | Average loss: 0.2342\n",
      "Epoch: 40 | Training Batch: 200 | Average loss: 0.2322\n",
      "Epoch: 40 | Training Batch: 210 | Average loss: 0.2324\n",
      "Epoch: 40 | Training Batch: 220 | Average loss: 0.2326\n",
      "Average training batch loss at epoch 40: 0.2460\n",
      "Average validation fold accuracy at epoch 40: 0.2581\n",
      "Epoch: 41 | Training Batch: 10 | Average loss: 0.2399\n",
      "Epoch: 41 | Training Batch: 20 | Average loss: 0.2321\n",
      "Epoch: 41 | Training Batch: 30 | Average loss: 0.2323\n",
      "Epoch: 41 | Training Batch: 40 | Average loss: 0.2334\n",
      "Epoch: 41 | Training Batch: 50 | Average loss: 0.2317\n",
      "Epoch: 41 | Training Batch: 60 | Average loss: 0.2293\n",
      "Epoch: 41 | Training Batch: 70 | Average loss: 0.2349\n",
      "Epoch: 41 | Training Batch: 80 | Average loss: 0.2298\n",
      "Epoch: 41 | Training Batch: 90 | Average loss: 0.2329\n",
      "Epoch: 41 | Training Batch: 100 | Average loss: 0.2335\n",
      "Epoch: 41 | Training Batch: 110 | Average loss: 0.2297\n",
      "Epoch: 41 | Training Batch: 120 | Average loss: 0.2330\n",
      "Epoch: 41 | Training Batch: 130 | Average loss: 0.2350\n",
      "Epoch: 41 | Training Batch: 140 | Average loss: 0.2276\n",
      "Epoch: 41 | Training Batch: 150 | Average loss: 0.2338\n",
      "Epoch: 41 | Training Batch: 160 | Average loss: 0.2344\n",
      "Epoch: 41 | Training Batch: 170 | Average loss: 0.2330\n",
      "Epoch: 41 | Training Batch: 180 | Average loss: 0.2336\n",
      "Epoch: 41 | Training Batch: 190 | Average loss: 0.2316\n",
      "Epoch: 41 | Training Batch: 200 | Average loss: 0.2291\n",
      "Epoch: 41 | Training Batch: 210 | Average loss: 0.2352\n",
      "Epoch: 41 | Training Batch: 220 | Average loss: 0.2350\n",
      "Average training batch loss at epoch 41: 0.2457\n",
      "Average validation fold accuracy at epoch 41: 0.2580\n",
      "Epoch: 42 | Training Batch: 10 | Average loss: 0.2333\n",
      "Epoch: 42 | Training Batch: 20 | Average loss: 0.2315\n",
      "Epoch: 42 | Training Batch: 30 | Average loss: 0.2407\n",
      "Epoch: 42 | Training Batch: 40 | Average loss: 0.2350\n",
      "Epoch: 42 | Training Batch: 50 | Average loss: 0.2344\n",
      "Epoch: 42 | Training Batch: 60 | Average loss: 0.2286\n",
      "Epoch: 42 | Training Batch: 70 | Average loss: 0.2352\n",
      "Epoch: 42 | Training Batch: 80 | Average loss: 0.2291\n",
      "Epoch: 42 | Training Batch: 90 | Average loss: 0.2275\n",
      "Epoch: 42 | Training Batch: 100 | Average loss: 0.2350\n",
      "Epoch: 42 | Training Batch: 110 | Average loss: 0.2306\n",
      "Epoch: 42 | Training Batch: 120 | Average loss: 0.2375\n",
      "Epoch: 42 | Training Batch: 130 | Average loss: 0.2290\n",
      "Epoch: 42 | Training Batch: 140 | Average loss: 0.2307\n",
      "Epoch: 42 | Training Batch: 150 | Average loss: 0.2305\n",
      "Epoch: 42 | Training Batch: 160 | Average loss: 0.2328\n",
      "Epoch: 42 | Training Batch: 170 | Average loss: 0.2337\n",
      "Epoch: 42 | Training Batch: 180 | Average loss: 0.2315\n",
      "Epoch: 42 | Training Batch: 190 | Average loss: 0.2327\n",
      "Epoch: 42 | Training Batch: 200 | Average loss: 0.2361\n",
      "Epoch: 42 | Training Batch: 210 | Average loss: 0.2348\n",
      "Epoch: 42 | Training Batch: 220 | Average loss: 0.2313\n",
      "Average training batch loss at epoch 42: 0.2454\n",
      "Average validation fold accuracy at epoch 42: 0.2580\n",
      "Epoch: 43 | Training Batch: 10 | Average loss: 0.2317\n",
      "Epoch: 43 | Training Batch: 20 | Average loss: 0.2307\n",
      "Epoch: 43 | Training Batch: 30 | Average loss: 0.2317\n",
      "Epoch: 43 | Training Batch: 40 | Average loss: 0.2321\n",
      "Epoch: 43 | Training Batch: 50 | Average loss: 0.2326\n",
      "Epoch: 43 | Training Batch: 60 | Average loss: 0.2336\n",
      "Epoch: 43 | Training Batch: 70 | Average loss: 0.2338\n",
      "Epoch: 43 | Training Batch: 80 | Average loss: 0.2361\n",
      "Epoch: 43 | Training Batch: 90 | Average loss: 0.2313\n",
      "Epoch: 43 | Training Batch: 100 | Average loss: 0.2330\n",
      "Epoch: 43 | Training Batch: 110 | Average loss: 0.2316\n",
      "Epoch: 43 | Training Batch: 120 | Average loss: 0.2298\n",
      "Epoch: 43 | Training Batch: 130 | Average loss: 0.2323\n",
      "Epoch: 43 | Training Batch: 140 | Average loss: 0.2335\n",
      "Epoch: 43 | Training Batch: 150 | Average loss: 0.2344\n",
      "Epoch: 43 | Training Batch: 160 | Average loss: 0.2332\n",
      "Epoch: 43 | Training Batch: 170 | Average loss: 0.2317\n",
      "Epoch: 43 | Training Batch: 180 | Average loss: 0.2303\n",
      "Epoch: 43 | Training Batch: 190 | Average loss: 0.2373\n",
      "Epoch: 43 | Training Batch: 200 | Average loss: 0.2363\n",
      "Epoch: 43 | Training Batch: 210 | Average loss: 0.2311\n",
      "Epoch: 43 | Training Batch: 220 | Average loss: 0.2316\n",
      "Average training batch loss at epoch 43: 0.2451\n",
      "Average validation fold accuracy at epoch 43: 0.2579\n",
      "Epoch: 44 | Training Batch: 10 | Average loss: 0.2302\n",
      "Epoch: 44 | Training Batch: 20 | Average loss: 0.2326\n",
      "Epoch: 44 | Training Batch: 30 | Average loss: 0.2330\n",
      "Epoch: 44 | Training Batch: 40 | Average loss: 0.2297\n",
      "Epoch: 44 | Training Batch: 50 | Average loss: 0.2281\n",
      "Epoch: 44 | Training Batch: 60 | Average loss: 0.2295\n",
      "Epoch: 44 | Training Batch: 70 | Average loss: 0.2373\n",
      "Epoch: 44 | Training Batch: 80 | Average loss: 0.2280\n",
      "Epoch: 44 | Training Batch: 90 | Average loss: 0.2308\n",
      "Epoch: 44 | Training Batch: 100 | Average loss: 0.2358\n",
      "Epoch: 44 | Training Batch: 110 | Average loss: 0.2291\n",
      "Epoch: 44 | Training Batch: 120 | Average loss: 0.2324\n",
      "Epoch: 44 | Training Batch: 130 | Average loss: 0.2395\n",
      "Epoch: 44 | Training Batch: 140 | Average loss: 0.2333\n",
      "Epoch: 44 | Training Batch: 150 | Average loss: 0.2358\n",
      "Epoch: 44 | Training Batch: 160 | Average loss: 0.2361\n",
      "Epoch: 44 | Training Batch: 170 | Average loss: 0.2311\n",
      "Epoch: 44 | Training Batch: 180 | Average loss: 0.2326\n",
      "Epoch: 44 | Training Batch: 190 | Average loss: 0.2375\n",
      "Epoch: 44 | Training Batch: 200 | Average loss: 0.2325\n",
      "Epoch: 44 | Training Batch: 210 | Average loss: 0.2343\n",
      "Epoch: 44 | Training Batch: 220 | Average loss: 0.2300\n",
      "Average training batch loss at epoch 44: 0.2448\n",
      "Average validation fold accuracy at epoch 44: 0.2578\n",
      "Epoch: 45 | Training Batch: 10 | Average loss: 0.2327\n",
      "Epoch: 45 | Training Batch: 20 | Average loss: 0.2309\n",
      "Epoch: 45 | Training Batch: 30 | Average loss: 0.2350\n",
      "Epoch: 45 | Training Batch: 40 | Average loss: 0.2364\n",
      "Epoch: 45 | Training Batch: 50 | Average loss: 0.2345\n",
      "Epoch: 45 | Training Batch: 60 | Average loss: 0.2295\n",
      "Epoch: 45 | Training Batch: 70 | Average loss: 0.2276\n",
      "Epoch: 45 | Training Batch: 80 | Average loss: 0.2298\n",
      "Epoch: 45 | Training Batch: 90 | Average loss: 0.2322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45 | Training Batch: 100 | Average loss: 0.2330\n",
      "Epoch: 45 | Training Batch: 110 | Average loss: 0.2290\n",
      "Epoch: 45 | Training Batch: 120 | Average loss: 0.2322\n",
      "Epoch: 45 | Training Batch: 130 | Average loss: 0.2322\n",
      "Epoch: 45 | Training Batch: 140 | Average loss: 0.2367\n",
      "Epoch: 45 | Training Batch: 150 | Average loss: 0.2329\n",
      "Epoch: 45 | Training Batch: 160 | Average loss: 0.2349\n",
      "Epoch: 45 | Training Batch: 170 | Average loss: 0.2352\n",
      "Epoch: 45 | Training Batch: 180 | Average loss: 0.2319\n",
      "Epoch: 45 | Training Batch: 190 | Average loss: 0.2348\n",
      "Epoch: 45 | Training Batch: 200 | Average loss: 0.2338\n",
      "Epoch: 45 | Training Batch: 210 | Average loss: 0.2310\n",
      "Epoch: 45 | Training Batch: 220 | Average loss: 0.2323\n",
      "Average training batch loss at epoch 45: 0.2445\n",
      "Average validation fold accuracy at epoch 45: 0.2577\n",
      "Epoch: 46 | Training Batch: 10 | Average loss: 0.2292\n",
      "Epoch: 46 | Training Batch: 20 | Average loss: 0.2339\n",
      "Epoch: 46 | Training Batch: 30 | Average loss: 0.2322\n",
      "Epoch: 46 | Training Batch: 40 | Average loss: 0.2301\n",
      "Epoch: 46 | Training Batch: 50 | Average loss: 0.2330\n",
      "Epoch: 46 | Training Batch: 60 | Average loss: 0.2314\n",
      "Epoch: 46 | Training Batch: 70 | Average loss: 0.2276\n",
      "Epoch: 46 | Training Batch: 80 | Average loss: 0.2347\n",
      "Epoch: 46 | Training Batch: 90 | Average loss: 0.2321\n",
      "Epoch: 46 | Training Batch: 100 | Average loss: 0.2328\n",
      "Epoch: 46 | Training Batch: 110 | Average loss: 0.2339\n",
      "Epoch: 46 | Training Batch: 120 | Average loss: 0.2350\n",
      "Epoch: 46 | Training Batch: 130 | Average loss: 0.2302\n",
      "Epoch: 46 | Training Batch: 140 | Average loss: 0.2333\n",
      "Epoch: 46 | Training Batch: 150 | Average loss: 0.2285\n",
      "Epoch: 46 | Training Batch: 160 | Average loss: 0.2338\n",
      "Epoch: 46 | Training Batch: 170 | Average loss: 0.2324\n",
      "Epoch: 46 | Training Batch: 180 | Average loss: 0.2327\n",
      "Epoch: 46 | Training Batch: 190 | Average loss: 0.2364\n",
      "Epoch: 46 | Training Batch: 200 | Average loss: 0.2313\n",
      "Epoch: 46 | Training Batch: 210 | Average loss: 0.2357\n",
      "Epoch: 46 | Training Batch: 220 | Average loss: 0.2338\n",
      "Average training batch loss at epoch 46: 0.2443\n",
      "Average validation fold accuracy at epoch 46: 0.2576\n",
      "Epoch: 47 | Training Batch: 10 | Average loss: 0.2336\n",
      "Epoch: 47 | Training Batch: 20 | Average loss: 0.2333\n",
      "Epoch: 47 | Training Batch: 30 | Average loss: 0.2325\n",
      "Epoch: 47 | Training Batch: 40 | Average loss: 0.2362\n",
      "Epoch: 47 | Training Batch: 50 | Average loss: 0.2296\n",
      "Epoch: 47 | Training Batch: 60 | Average loss: 0.2262\n",
      "Epoch: 47 | Training Batch: 70 | Average loss: 0.2303\n",
      "Epoch: 47 | Training Batch: 80 | Average loss: 0.2304\n",
      "Epoch: 47 | Training Batch: 90 | Average loss: 0.2384\n",
      "Epoch: 47 | Training Batch: 100 | Average loss: 0.2301\n",
      "Epoch: 47 | Training Batch: 110 | Average loss: 0.2322\n",
      "Epoch: 47 | Training Batch: 120 | Average loss: 0.2350\n",
      "Epoch: 47 | Training Batch: 130 | Average loss: 0.2327\n",
      "Epoch: 47 | Training Batch: 140 | Average loss: 0.2342\n",
      "Epoch: 47 | Training Batch: 150 | Average loss: 0.2292\n",
      "Epoch: 47 | Training Batch: 160 | Average loss: 0.2355\n",
      "Epoch: 47 | Training Batch: 170 | Average loss: 0.2330\n",
      "Epoch: 47 | Training Batch: 180 | Average loss: 0.2322\n",
      "Epoch: 47 | Training Batch: 190 | Average loss: 0.2321\n",
      "Epoch: 47 | Training Batch: 200 | Average loss: 0.2273\n",
      "Epoch: 47 | Training Batch: 210 | Average loss: 0.2363\n",
      "Epoch: 47 | Training Batch: 220 | Average loss: 0.2324\n",
      "Average training batch loss at epoch 47: 0.2440\n",
      "Average validation fold accuracy at epoch 47: 0.2575\n",
      "Epoch: 48 | Training Batch: 10 | Average loss: 0.2308\n",
      "Epoch: 48 | Training Batch: 20 | Average loss: 0.2309\n",
      "Epoch: 48 | Training Batch: 30 | Average loss: 0.2285\n",
      "Epoch: 48 | Training Batch: 40 | Average loss: 0.2374\n",
      "Epoch: 48 | Training Batch: 50 | Average loss: 0.2320\n",
      "Epoch: 48 | Training Batch: 60 | Average loss: 0.2303\n",
      "Epoch: 48 | Training Batch: 70 | Average loss: 0.2326\n",
      "Epoch: 48 | Training Batch: 80 | Average loss: 0.2359\n",
      "Epoch: 48 | Training Batch: 90 | Average loss: 0.2374\n",
      "Epoch: 48 | Training Batch: 100 | Average loss: 0.2337\n",
      "Epoch: 48 | Training Batch: 110 | Average loss: 0.2319\n",
      "Epoch: 48 | Training Batch: 120 | Average loss: 0.2324\n",
      "Epoch: 48 | Training Batch: 130 | Average loss: 0.2279\n",
      "Epoch: 48 | Training Batch: 140 | Average loss: 0.2284\n",
      "Epoch: 48 | Training Batch: 150 | Average loss: 0.2337\n",
      "Epoch: 48 | Training Batch: 160 | Average loss: 0.2322\n",
      "Epoch: 48 | Training Batch: 170 | Average loss: 0.2340\n",
      "Epoch: 48 | Training Batch: 180 | Average loss: 0.2324\n",
      "Epoch: 48 | Training Batch: 190 | Average loss: 0.2336\n",
      "Epoch: 48 | Training Batch: 200 | Average loss: 0.2364\n",
      "Epoch: 48 | Training Batch: 210 | Average loss: 0.2320\n",
      "Epoch: 48 | Training Batch: 220 | Average loss: 0.2305\n",
      "Average training batch loss at epoch 48: 0.2438\n",
      "Average validation fold accuracy at epoch 48: 0.2575\n",
      "Epoch: 49 | Training Batch: 10 | Average loss: 0.2343\n",
      "Epoch: 49 | Training Batch: 20 | Average loss: 0.2360\n",
      "Epoch: 49 | Training Batch: 30 | Average loss: 0.2303\n",
      "Epoch: 49 | Training Batch: 40 | Average loss: 0.2330\n",
      "Epoch: 49 | Training Batch: 50 | Average loss: 0.2357\n",
      "Epoch: 49 | Training Batch: 60 | Average loss: 0.2340\n",
      "Epoch: 49 | Training Batch: 70 | Average loss: 0.2301\n",
      "Epoch: 49 | Training Batch: 80 | Average loss: 0.2286\n",
      "Epoch: 49 | Training Batch: 90 | Average loss: 0.2344\n",
      "Epoch: 49 | Training Batch: 100 | Average loss: 0.2344\n",
      "Epoch: 49 | Training Batch: 110 | Average loss: 0.2308\n",
      "Epoch: 49 | Training Batch: 120 | Average loss: 0.2364\n",
      "Epoch: 49 | Training Batch: 130 | Average loss: 0.2322\n",
      "Epoch: 49 | Training Batch: 140 | Average loss: 0.2307\n",
      "Epoch: 49 | Training Batch: 150 | Average loss: 0.2309\n",
      "Epoch: 49 | Training Batch: 160 | Average loss: 0.2339\n",
      "Epoch: 49 | Training Batch: 170 | Average loss: 0.2318\n",
      "Epoch: 49 | Training Batch: 180 | Average loss: 0.2307\n",
      "Epoch: 49 | Training Batch: 190 | Average loss: 0.2312\n",
      "Epoch: 49 | Training Batch: 200 | Average loss: 0.2292\n",
      "Epoch: 49 | Training Batch: 210 | Average loss: 0.2302\n",
      "Epoch: 49 | Training Batch: 220 | Average loss: 0.2314\n",
      "Average training batch loss at epoch 49: 0.2436\n",
      "Average validation fold accuracy at epoch 49: 0.2574\n",
      "Epoch: 50 | Training Batch: 10 | Average loss: 0.2361\n",
      "Epoch: 50 | Training Batch: 20 | Average loss: 0.2303\n",
      "Epoch: 50 | Training Batch: 30 | Average loss: 0.2300\n",
      "Epoch: 50 | Training Batch: 40 | Average loss: 0.2332\n",
      "Epoch: 50 | Training Batch: 50 | Average loss: 0.2291\n",
      "Epoch: 50 | Training Batch: 60 | Average loss: 0.2279\n",
      "Epoch: 50 | Training Batch: 70 | Average loss: 0.2332\n",
      "Epoch: 50 | Training Batch: 80 | Average loss: 0.2340\n",
      "Epoch: 50 | Training Batch: 90 | Average loss: 0.2349\n",
      "Epoch: 50 | Training Batch: 100 | Average loss: 0.2342\n",
      "Epoch: 50 | Training Batch: 110 | Average loss: 0.2325\n",
      "Epoch: 50 | Training Batch: 120 | Average loss: 0.2289\n",
      "Epoch: 50 | Training Batch: 130 | Average loss: 0.2329\n",
      "Epoch: 50 | Training Batch: 140 | Average loss: 0.2323\n",
      "Epoch: 50 | Training Batch: 150 | Average loss: 0.2260\n",
      "Epoch: 50 | Training Batch: 160 | Average loss: 0.2327\n",
      "Epoch: 50 | Training Batch: 170 | Average loss: 0.2324\n",
      "Epoch: 50 | Training Batch: 180 | Average loss: 0.2310\n",
      "Epoch: 50 | Training Batch: 190 | Average loss: 0.2347\n",
      "Epoch: 50 | Training Batch: 200 | Average loss: 0.2353\n",
      "Epoch: 50 | Training Batch: 210 | Average loss: 0.2334\n",
      "Epoch: 50 | Training Batch: 220 | Average loss: 0.2346\n",
      "Average training batch loss at epoch 50: 0.2433\n",
      "Average validation fold accuracy at epoch 50: 0.2573\n",
      "Epoch: 51 | Training Batch: 10 | Average loss: 0.2325\n",
      "Epoch: 51 | Training Batch: 20 | Average loss: 0.2284\n",
      "Epoch: 51 | Training Batch: 30 | Average loss: 0.2299\n",
      "Epoch: 51 | Training Batch: 40 | Average loss: 0.2333\n",
      "Epoch: 51 | Training Batch: 50 | Average loss: 0.2338\n",
      "Epoch: 51 | Training Batch: 60 | Average loss: 0.2355\n",
      "Epoch: 51 | Training Batch: 70 | Average loss: 0.2308\n",
      "Epoch: 51 | Training Batch: 80 | Average loss: 0.2295\n",
      "Epoch: 51 | Training Batch: 90 | Average loss: 0.2296\n",
      "Epoch: 51 | Training Batch: 100 | Average loss: 0.2362\n",
      "Epoch: 51 | Training Batch: 110 | Average loss: 0.2352\n",
      "Epoch: 51 | Training Batch: 120 | Average loss: 0.2337\n",
      "Epoch: 51 | Training Batch: 130 | Average loss: 0.2285\n",
      "Epoch: 51 | Training Batch: 140 | Average loss: 0.2324\n",
      "Epoch: 51 | Training Batch: 150 | Average loss: 0.2361\n",
      "Epoch: 51 | Training Batch: 160 | Average loss: 0.2264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51 | Training Batch: 170 | Average loss: 0.2330\n",
      "Epoch: 51 | Training Batch: 180 | Average loss: 0.2288\n",
      "Epoch: 51 | Training Batch: 190 | Average loss: 0.2339\n",
      "Epoch: 51 | Training Batch: 200 | Average loss: 0.2317\n",
      "Epoch: 51 | Training Batch: 210 | Average loss: 0.2330\n",
      "Epoch: 51 | Training Batch: 220 | Average loss: 0.2363\n",
      "Average training batch loss at epoch 51: 0.2431\n",
      "Average validation fold accuracy at epoch 51: 0.2573\n",
      "Epoch: 52 | Training Batch: 10 | Average loss: 0.2293\n",
      "Epoch: 52 | Training Batch: 20 | Average loss: 0.2301\n",
      "Epoch: 52 | Training Batch: 30 | Average loss: 0.2318\n",
      "Epoch: 52 | Training Batch: 40 | Average loss: 0.2346\n",
      "Epoch: 52 | Training Batch: 50 | Average loss: 0.2288\n",
      "Epoch: 52 | Training Batch: 60 | Average loss: 0.2312\n",
      "Epoch: 52 | Training Batch: 70 | Average loss: 0.2303\n",
      "Epoch: 52 | Training Batch: 80 | Average loss: 0.2316\n",
      "Epoch: 52 | Training Batch: 90 | Average loss: 0.2274\n",
      "Epoch: 52 | Training Batch: 100 | Average loss: 0.2387\n",
      "Epoch: 52 | Training Batch: 110 | Average loss: 0.2300\n",
      "Epoch: 52 | Training Batch: 120 | Average loss: 0.2343\n",
      "Epoch: 52 | Training Batch: 130 | Average loss: 0.2342\n",
      "Epoch: 52 | Training Batch: 140 | Average loss: 0.2272\n",
      "Epoch: 52 | Training Batch: 150 | Average loss: 0.2334\n",
      "Epoch: 52 | Training Batch: 160 | Average loss: 0.2323\n",
      "Epoch: 52 | Training Batch: 170 | Average loss: 0.2347\n",
      "Epoch: 52 | Training Batch: 180 | Average loss: 0.2347\n",
      "Epoch: 52 | Training Batch: 190 | Average loss: 0.2348\n",
      "Epoch: 52 | Training Batch: 200 | Average loss: 0.2312\n",
      "Epoch: 52 | Training Batch: 210 | Average loss: 0.2346\n",
      "Epoch: 52 | Training Batch: 220 | Average loss: 0.2313\n",
      "Average training batch loss at epoch 52: 0.2429\n",
      "Average validation fold accuracy at epoch 52: 0.2572\n",
      "Epoch: 53 | Training Batch: 10 | Average loss: 0.2339\n",
      "Epoch: 53 | Training Batch: 20 | Average loss: 0.2308\n",
      "Epoch: 53 | Training Batch: 30 | Average loss: 0.2280\n",
      "Epoch: 53 | Training Batch: 40 | Average loss: 0.2282\n",
      "Epoch: 53 | Training Batch: 50 | Average loss: 0.2316\n",
      "Epoch: 53 | Training Batch: 60 | Average loss: 0.2362\n",
      "Epoch: 53 | Training Batch: 70 | Average loss: 0.2309\n",
      "Epoch: 53 | Training Batch: 80 | Average loss: 0.2301\n",
      "Epoch: 53 | Training Batch: 90 | Average loss: 0.2291\n",
      "Epoch: 53 | Training Batch: 100 | Average loss: 0.2337\n",
      "Epoch: 53 | Training Batch: 110 | Average loss: 0.2338\n",
      "Epoch: 53 | Training Batch: 120 | Average loss: 0.2325\n",
      "Epoch: 53 | Training Batch: 130 | Average loss: 0.2312\n",
      "Epoch: 53 | Training Batch: 140 | Average loss: 0.2349\n",
      "Epoch: 53 | Training Batch: 150 | Average loss: 0.2300\n",
      "Epoch: 53 | Training Batch: 160 | Average loss: 0.2315\n",
      "Epoch: 53 | Training Batch: 170 | Average loss: 0.2349\n",
      "Epoch: 53 | Training Batch: 180 | Average loss: 0.2363\n",
      "Epoch: 53 | Training Batch: 190 | Average loss: 0.2287\n",
      "Epoch: 53 | Training Batch: 200 | Average loss: 0.2308\n",
      "Epoch: 53 | Training Batch: 210 | Average loss: 0.2337\n",
      "Epoch: 53 | Training Batch: 220 | Average loss: 0.2352\n",
      "Average training batch loss at epoch 53: 0.2427\n",
      "Average validation fold accuracy at epoch 53: 0.2571\n",
      "Epoch: 54 | Training Batch: 10 | Average loss: 0.2303\n",
      "Epoch: 54 | Training Batch: 20 | Average loss: 0.2343\n",
      "Epoch: 54 | Training Batch: 30 | Average loss: 0.2323\n",
      "Epoch: 54 | Training Batch: 40 | Average loss: 0.2281\n",
      "Epoch: 54 | Training Batch: 50 | Average loss: 0.2284\n",
      "Epoch: 54 | Training Batch: 60 | Average loss: 0.2291\n",
      "Epoch: 54 | Training Batch: 70 | Average loss: 0.2351\n",
      "Epoch: 54 | Training Batch: 80 | Average loss: 0.2356\n",
      "Epoch: 54 | Training Batch: 90 | Average loss: 0.2297\n",
      "Epoch: 54 | Training Batch: 100 | Average loss: 0.2358\n",
      "Epoch: 54 | Training Batch: 110 | Average loss: 0.2278\n",
      "Epoch: 54 | Training Batch: 120 | Average loss: 0.2308\n",
      "Epoch: 54 | Training Batch: 130 | Average loss: 0.2341\n",
      "Epoch: 54 | Training Batch: 140 | Average loss: 0.2303\n",
      "Epoch: 54 | Training Batch: 150 | Average loss: 0.2305\n",
      "Epoch: 54 | Training Batch: 160 | Average loss: 0.2292\n",
      "Epoch: 54 | Training Batch: 170 | Average loss: 0.2328\n",
      "Epoch: 54 | Training Batch: 180 | Average loss: 0.2310\n",
      "Epoch: 54 | Training Batch: 190 | Average loss: 0.2333\n",
      "Epoch: 54 | Training Batch: 200 | Average loss: 0.2319\n",
      "Epoch: 54 | Training Batch: 210 | Average loss: 0.2397\n",
      "Epoch: 54 | Training Batch: 220 | Average loss: 0.2324\n",
      "Average training batch loss at epoch 54: 0.2425\n",
      "Average validation fold accuracy at epoch 54: 0.2571\n",
      "Epoch: 55 | Training Batch: 10 | Average loss: 0.2334\n",
      "Epoch: 55 | Training Batch: 20 | Average loss: 0.2317\n",
      "Epoch: 55 | Training Batch: 30 | Average loss: 0.2311\n",
      "Epoch: 55 | Training Batch: 40 | Average loss: 0.2324\n",
      "Epoch: 55 | Training Batch: 50 | Average loss: 0.2297\n",
      "Epoch: 55 | Training Batch: 60 | Average loss: 0.2337\n",
      "Epoch: 55 | Training Batch: 70 | Average loss: 0.2311\n",
      "Epoch: 55 | Training Batch: 80 | Average loss: 0.2322\n",
      "Epoch: 55 | Training Batch: 90 | Average loss: 0.2336\n",
      "Epoch: 55 | Training Batch: 100 | Average loss: 0.2307\n",
      "Epoch: 55 | Training Batch: 110 | Average loss: 0.2306\n",
      "Epoch: 55 | Training Batch: 120 | Average loss: 0.2329\n",
      "Epoch: 55 | Training Batch: 130 | Average loss: 0.2357\n",
      "Epoch: 55 | Training Batch: 140 | Average loss: 0.2286\n",
      "Epoch: 55 | Training Batch: 150 | Average loss: 0.2303\n",
      "Epoch: 55 | Training Batch: 160 | Average loss: 0.2336\n",
      "Epoch: 55 | Training Batch: 170 | Average loss: 0.2325\n",
      "Epoch: 55 | Training Batch: 180 | Average loss: 0.2282\n",
      "Epoch: 55 | Training Batch: 190 | Average loss: 0.2314\n",
      "Epoch: 55 | Training Batch: 200 | Average loss: 0.2308\n",
      "Epoch: 55 | Training Batch: 210 | Average loss: 0.2289\n",
      "Epoch: 55 | Training Batch: 220 | Average loss: 0.2372\n",
      "Average training batch loss at epoch 55: 0.2423\n",
      "Average validation fold accuracy at epoch 55: 0.2570\n",
      "Epoch: 56 | Training Batch: 10 | Average loss: 0.2328\n",
      "Epoch: 56 | Training Batch: 20 | Average loss: 0.2312\n",
      "Epoch: 56 | Training Batch: 30 | Average loss: 0.2330\n",
      "Epoch: 56 | Training Batch: 40 | Average loss: 0.2229\n",
      "Epoch: 56 | Training Batch: 50 | Average loss: 0.2338\n",
      "Epoch: 56 | Training Batch: 60 | Average loss: 0.2317\n",
      "Epoch: 56 | Training Batch: 70 | Average loss: 0.2299\n",
      "Epoch: 56 | Training Batch: 80 | Average loss: 0.2300\n",
      "Epoch: 56 | Training Batch: 90 | Average loss: 0.2284\n",
      "Epoch: 56 | Training Batch: 100 | Average loss: 0.2348\n",
      "Epoch: 56 | Training Batch: 110 | Average loss: 0.2374\n",
      "Epoch: 56 | Training Batch: 120 | Average loss: 0.2351\n",
      "Epoch: 56 | Training Batch: 130 | Average loss: 0.2371\n",
      "Epoch: 56 | Training Batch: 140 | Average loss: 0.2313\n",
      "Epoch: 56 | Training Batch: 150 | Average loss: 0.2322\n",
      "Epoch: 56 | Training Batch: 160 | Average loss: 0.2322\n",
      "Epoch: 56 | Training Batch: 170 | Average loss: 0.2303\n",
      "Epoch: 56 | Training Batch: 180 | Average loss: 0.2304\n",
      "Epoch: 56 | Training Batch: 190 | Average loss: 0.2326\n",
      "Epoch: 56 | Training Batch: 200 | Average loss: 0.2274\n",
      "Epoch: 56 | Training Batch: 210 | Average loss: 0.2340\n",
      "Epoch: 56 | Training Batch: 220 | Average loss: 0.2316\n",
      "Average training batch loss at epoch 56: 0.2421\n",
      "Average validation fold accuracy at epoch 56: 0.2570\n",
      "Epoch: 57 | Training Batch: 10 | Average loss: 0.2319\n",
      "Epoch: 57 | Training Batch: 20 | Average loss: 0.2305\n",
      "Epoch: 57 | Training Batch: 30 | Average loss: 0.2322\n",
      "Epoch: 57 | Training Batch: 40 | Average loss: 0.2341\n",
      "Epoch: 57 | Training Batch: 50 | Average loss: 0.2326\n",
      "Epoch: 57 | Training Batch: 60 | Average loss: 0.2325\n",
      "Epoch: 57 | Training Batch: 70 | Average loss: 0.2336\n",
      "Epoch: 57 | Training Batch: 80 | Average loss: 0.2331\n",
      "Epoch: 57 | Training Batch: 90 | Average loss: 0.2342\n",
      "Epoch: 57 | Training Batch: 100 | Average loss: 0.2292\n",
      "Epoch: 57 | Training Batch: 110 | Average loss: 0.2332\n",
      "Epoch: 57 | Training Batch: 120 | Average loss: 0.2306\n",
      "Epoch: 57 | Training Batch: 130 | Average loss: 0.2300\n",
      "Epoch: 57 | Training Batch: 140 | Average loss: 0.2305\n",
      "Epoch: 57 | Training Batch: 150 | Average loss: 0.2327\n",
      "Epoch: 57 | Training Batch: 160 | Average loss: 0.2249\n",
      "Epoch: 57 | Training Batch: 170 | Average loss: 0.2317\n",
      "Epoch: 57 | Training Batch: 180 | Average loss: 0.2299\n",
      "Epoch: 57 | Training Batch: 190 | Average loss: 0.2277\n",
      "Epoch: 57 | Training Batch: 200 | Average loss: 0.2342\n",
      "Epoch: 57 | Training Batch: 210 | Average loss: 0.2369\n",
      "Epoch: 57 | Training Batch: 220 | Average loss: 0.2325\n",
      "Average training batch loss at epoch 57: 0.2420\n",
      "Average validation fold accuracy at epoch 57: 0.2569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58 | Training Batch: 10 | Average loss: 0.2330\n",
      "Epoch: 58 | Training Batch: 20 | Average loss: 0.2281\n",
      "Epoch: 58 | Training Batch: 30 | Average loss: 0.2324\n",
      "Epoch: 58 | Training Batch: 40 | Average loss: 0.2325\n",
      "Epoch: 58 | Training Batch: 50 | Average loss: 0.2321\n",
      "Epoch: 58 | Training Batch: 60 | Average loss: 0.2304\n",
      "Epoch: 58 | Training Batch: 70 | Average loss: 0.2341\n",
      "Epoch: 58 | Training Batch: 80 | Average loss: 0.2325\n",
      "Epoch: 58 | Training Batch: 90 | Average loss: 0.2304\n",
      "Epoch: 58 | Training Batch: 100 | Average loss: 0.2285\n",
      "Epoch: 58 | Training Batch: 110 | Average loss: 0.2304\n",
      "Epoch: 58 | Training Batch: 120 | Average loss: 0.2319\n",
      "Epoch: 58 | Training Batch: 130 | Average loss: 0.2267\n",
      "Epoch: 58 | Training Batch: 140 | Average loss: 0.2315\n",
      "Epoch: 58 | Training Batch: 150 | Average loss: 0.2321\n",
      "Epoch: 58 | Training Batch: 160 | Average loss: 0.2376\n",
      "Epoch: 58 | Training Batch: 170 | Average loss: 0.2327\n",
      "Epoch: 58 | Training Batch: 180 | Average loss: 0.2331\n",
      "Epoch: 58 | Training Batch: 190 | Average loss: 0.2317\n",
      "Epoch: 58 | Training Batch: 200 | Average loss: 0.2321\n",
      "Epoch: 58 | Training Batch: 210 | Average loss: 0.2388\n",
      "Epoch: 58 | Training Batch: 220 | Average loss: 0.2290\n",
      "Average training batch loss at epoch 58: 0.2418\n",
      "Average validation fold accuracy at epoch 58: 0.2569\n",
      "Epoch: 59 | Training Batch: 10 | Average loss: 0.2319\n",
      "Epoch: 59 | Training Batch: 20 | Average loss: 0.2333\n",
      "Epoch: 59 | Training Batch: 30 | Average loss: 0.2317\n",
      "Epoch: 59 | Training Batch: 40 | Average loss: 0.2297\n",
      "Epoch: 59 | Training Batch: 50 | Average loss: 0.2308\n",
      "Epoch: 59 | Training Batch: 60 | Average loss: 0.2323\n",
      "Epoch: 59 | Training Batch: 70 | Average loss: 0.2295\n",
      "Epoch: 59 | Training Batch: 80 | Average loss: 0.2301\n",
      "Epoch: 59 | Training Batch: 90 | Average loss: 0.2334\n",
      "Epoch: 59 | Training Batch: 100 | Average loss: 0.2321\n",
      "Epoch: 59 | Training Batch: 110 | Average loss: 0.2340\n",
      "Epoch: 59 | Training Batch: 120 | Average loss: 0.2283\n",
      "Epoch: 59 | Training Batch: 130 | Average loss: 0.2323\n",
      "Epoch: 59 | Training Batch: 140 | Average loss: 0.2359\n",
      "Epoch: 59 | Training Batch: 150 | Average loss: 0.2314\n",
      "Epoch: 59 | Training Batch: 160 | Average loss: 0.2327\n",
      "Epoch: 59 | Training Batch: 170 | Average loss: 0.2335\n",
      "Epoch: 59 | Training Batch: 180 | Average loss: 0.2330\n",
      "Epoch: 59 | Training Batch: 190 | Average loss: 0.2305\n",
      "Epoch: 59 | Training Batch: 200 | Average loss: 0.2277\n",
      "Epoch: 59 | Training Batch: 210 | Average loss: 0.2297\n",
      "Epoch: 59 | Training Batch: 220 | Average loss: 0.2324\n",
      "Average training batch loss at epoch 59: 0.2416\n",
      "Average validation fold accuracy at epoch 59: 0.2568\n",
      "Epoch: 60 | Training Batch: 10 | Average loss: 0.2321\n",
      "Epoch: 60 | Training Batch: 20 | Average loss: 0.2319\n",
      "Epoch: 60 | Training Batch: 30 | Average loss: 0.2282\n",
      "Epoch: 60 | Training Batch: 40 | Average loss: 0.2376\n",
      "Epoch: 60 | Training Batch: 50 | Average loss: 0.2362\n",
      "Epoch: 60 | Training Batch: 60 | Average loss: 0.2289\n",
      "Epoch: 60 | Training Batch: 70 | Average loss: 0.2318\n",
      "Epoch: 60 | Training Batch: 80 | Average loss: 0.2310\n",
      "Epoch: 60 | Training Batch: 90 | Average loss: 0.2295\n",
      "Epoch: 60 | Training Batch: 100 | Average loss: 0.2317\n",
      "Epoch: 60 | Training Batch: 110 | Average loss: 0.2317\n",
      "Epoch: 60 | Training Batch: 120 | Average loss: 0.2343\n",
      "Epoch: 60 | Training Batch: 130 | Average loss: 0.2329\n",
      "Epoch: 60 | Training Batch: 140 | Average loss: 0.2370\n",
      "Epoch: 60 | Training Batch: 150 | Average loss: 0.2318\n",
      "Epoch: 60 | Training Batch: 160 | Average loss: 0.2307\n",
      "Epoch: 60 | Training Batch: 170 | Average loss: 0.2315\n",
      "Epoch: 60 | Training Batch: 180 | Average loss: 0.2286\n",
      "Epoch: 60 | Training Batch: 190 | Average loss: 0.2292\n",
      "Epoch: 60 | Training Batch: 200 | Average loss: 0.2272\n",
      "Epoch: 60 | Training Batch: 210 | Average loss: 0.2309\n",
      "Epoch: 60 | Training Batch: 220 | Average loss: 0.2318\n",
      "Average training batch loss at epoch 60: 0.2415\n",
      "Average validation fold accuracy at epoch 60: 0.2568\n",
      "Epoch: 61 | Training Batch: 10 | Average loss: 0.2301\n",
      "Epoch: 61 | Training Batch: 20 | Average loss: 0.2317\n",
      "Epoch: 61 | Training Batch: 30 | Average loss: 0.2345\n",
      "Epoch: 61 | Training Batch: 40 | Average loss: 0.2329\n",
      "Epoch: 61 | Training Batch: 50 | Average loss: 0.2318\n",
      "Epoch: 61 | Training Batch: 60 | Average loss: 0.2288\n",
      "Epoch: 61 | Training Batch: 70 | Average loss: 0.2307\n",
      "Epoch: 61 | Training Batch: 80 | Average loss: 0.2371\n",
      "Epoch: 61 | Training Batch: 90 | Average loss: 0.2276\n",
      "Epoch: 61 | Training Batch: 100 | Average loss: 0.2328\n",
      "Epoch: 61 | Training Batch: 110 | Average loss: 0.2290\n",
      "Epoch: 61 | Training Batch: 120 | Average loss: 0.2298\n",
      "Epoch: 61 | Training Batch: 130 | Average loss: 0.2363\n",
      "Epoch: 61 | Training Batch: 140 | Average loss: 0.2302\n",
      "Epoch: 61 | Training Batch: 150 | Average loss: 0.2305\n",
      "Epoch: 61 | Training Batch: 160 | Average loss: 0.2298\n",
      "Epoch: 61 | Training Batch: 170 | Average loss: 0.2270\n",
      "Epoch: 61 | Training Batch: 180 | Average loss: 0.2322\n",
      "Epoch: 61 | Training Batch: 190 | Average loss: 0.2300\n",
      "Epoch: 61 | Training Batch: 200 | Average loss: 0.2349\n",
      "Epoch: 61 | Training Batch: 210 | Average loss: 0.2360\n",
      "Epoch: 61 | Training Batch: 220 | Average loss: 0.2303\n",
      "Average training batch loss at epoch 61: 0.2413\n",
      "Average validation fold accuracy at epoch 61: 0.2567\n",
      "Epoch: 62 | Training Batch: 10 | Average loss: 0.2293\n",
      "Epoch: 62 | Training Batch: 20 | Average loss: 0.2305\n",
      "Epoch: 62 | Training Batch: 30 | Average loss: 0.2283\n",
      "Epoch: 62 | Training Batch: 40 | Average loss: 0.2328\n",
      "Epoch: 62 | Training Batch: 50 | Average loss: 0.2322\n",
      "Epoch: 62 | Training Batch: 60 | Average loss: 0.2269\n",
      "Epoch: 62 | Training Batch: 70 | Average loss: 0.2329\n",
      "Epoch: 62 | Training Batch: 80 | Average loss: 0.2334\n",
      "Epoch: 62 | Training Batch: 90 | Average loss: 0.2314\n",
      "Epoch: 62 | Training Batch: 100 | Average loss: 0.2293\n",
      "Epoch: 62 | Training Batch: 110 | Average loss: 0.2358\n",
      "Epoch: 62 | Training Batch: 120 | Average loss: 0.2305\n",
      "Epoch: 62 | Training Batch: 130 | Average loss: 0.2330\n",
      "Epoch: 62 | Training Batch: 140 | Average loss: 0.2258\n",
      "Epoch: 62 | Training Batch: 150 | Average loss: 0.2311\n",
      "Epoch: 62 | Training Batch: 160 | Average loss: 0.2354\n",
      "Epoch: 62 | Training Batch: 170 | Average loss: 0.2311\n",
      "Epoch: 62 | Training Batch: 180 | Average loss: 0.2339\n",
      "Epoch: 62 | Training Batch: 190 | Average loss: 0.2322\n",
      "Epoch: 62 | Training Batch: 200 | Average loss: 0.2322\n",
      "Epoch: 62 | Training Batch: 210 | Average loss: 0.2354\n",
      "Epoch: 62 | Training Batch: 220 | Average loss: 0.2289\n",
      "Average training batch loss at epoch 62: 0.2412\n",
      "Average validation fold accuracy at epoch 62: 0.2567\n",
      "Epoch: 63 | Training Batch: 10 | Average loss: 0.2375\n",
      "Epoch: 63 | Training Batch: 20 | Average loss: 0.2238\n",
      "Epoch: 63 | Training Batch: 30 | Average loss: 0.2314\n",
      "Epoch: 63 | Training Batch: 40 | Average loss: 0.2291\n",
      "Epoch: 63 | Training Batch: 50 | Average loss: 0.2288\n",
      "Epoch: 63 | Training Batch: 60 | Average loss: 0.2290\n",
      "Epoch: 63 | Training Batch: 70 | Average loss: 0.2337\n",
      "Epoch: 63 | Training Batch: 80 | Average loss: 0.2305\n",
      "Epoch: 63 | Training Batch: 90 | Average loss: 0.2283\n",
      "Epoch: 63 | Training Batch: 100 | Average loss: 0.2321\n",
      "Epoch: 63 | Training Batch: 110 | Average loss: 0.2294\n",
      "Epoch: 63 | Training Batch: 120 | Average loss: 0.2325\n",
      "Epoch: 63 | Training Batch: 130 | Average loss: 0.2334\n",
      "Epoch: 63 | Training Batch: 140 | Average loss: 0.2329\n",
      "Epoch: 63 | Training Batch: 150 | Average loss: 0.2289\n",
      "Epoch: 63 | Training Batch: 160 | Average loss: 0.2291\n",
      "Epoch: 63 | Training Batch: 170 | Average loss: 0.2379\n",
      "Epoch: 63 | Training Batch: 180 | Average loss: 0.2330\n",
      "Epoch: 63 | Training Batch: 190 | Average loss: 0.2325\n",
      "Epoch: 63 | Training Batch: 200 | Average loss: 0.2363\n",
      "Epoch: 63 | Training Batch: 210 | Average loss: 0.2290\n",
      "Epoch: 63 | Training Batch: 220 | Average loss: 0.2339\n",
      "Average training batch loss at epoch 63: 0.2410\n",
      "Average validation fold accuracy at epoch 63: 0.2567\n",
      "Epoch: 64 | Training Batch: 10 | Average loss: 0.2294\n",
      "Epoch: 64 | Training Batch: 20 | Average loss: 0.2329\n",
      "Epoch: 64 | Training Batch: 30 | Average loss: 0.2299\n",
      "Epoch: 64 | Training Batch: 40 | Average loss: 0.2281\n",
      "Epoch: 64 | Training Batch: 50 | Average loss: 0.2318\n",
      "Epoch: 64 | Training Batch: 60 | Average loss: 0.2313\n",
      "Epoch: 64 | Training Batch: 70 | Average loss: 0.2287\n",
      "Epoch: 64 | Training Batch: 80 | Average loss: 0.2319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64 | Training Batch: 90 | Average loss: 0.2310\n",
      "Epoch: 64 | Training Batch: 100 | Average loss: 0.2309\n",
      "Epoch: 64 | Training Batch: 110 | Average loss: 0.2320\n",
      "Epoch: 64 | Training Batch: 120 | Average loss: 0.2311\n",
      "Epoch: 64 | Training Batch: 130 | Average loss: 0.2284\n",
      "Epoch: 64 | Training Batch: 140 | Average loss: 0.2336\n",
      "Epoch: 64 | Training Batch: 150 | Average loss: 0.2320\n",
      "Epoch: 64 | Training Batch: 160 | Average loss: 0.2313\n",
      "Epoch: 64 | Training Batch: 170 | Average loss: 0.2281\n",
      "Epoch: 64 | Training Batch: 180 | Average loss: 0.2323\n",
      "Epoch: 64 | Training Batch: 190 | Average loss: 0.2341\n",
      "Epoch: 64 | Training Batch: 200 | Average loss: 0.2359\n",
      "Epoch: 64 | Training Batch: 210 | Average loss: 0.2362\n",
      "Epoch: 64 | Training Batch: 220 | Average loss: 0.2336\n",
      "Average training batch loss at epoch 64: 0.2409\n",
      "Average validation fold accuracy at epoch 64: 0.2566\n",
      "Epoch: 65 | Training Batch: 10 | Average loss: 0.2271\n",
      "Epoch: 65 | Training Batch: 20 | Average loss: 0.2301\n",
      "Epoch: 65 | Training Batch: 30 | Average loss: 0.2297\n",
      "Epoch: 65 | Training Batch: 40 | Average loss: 0.2316\n",
      "Epoch: 65 | Training Batch: 50 | Average loss: 0.2335\n",
      "Epoch: 65 | Training Batch: 60 | Average loss: 0.2331\n",
      "Epoch: 65 | Training Batch: 70 | Average loss: 0.2310\n",
      "Epoch: 65 | Training Batch: 80 | Average loss: 0.2367\n",
      "Epoch: 65 | Training Batch: 90 | Average loss: 0.2318\n",
      "Epoch: 65 | Training Batch: 100 | Average loss: 0.2270\n",
      "Epoch: 65 | Training Batch: 110 | Average loss: 0.2331\n",
      "Epoch: 65 | Training Batch: 120 | Average loss: 0.2318\n",
      "Epoch: 65 | Training Batch: 130 | Average loss: 0.2304\n",
      "Epoch: 65 | Training Batch: 140 | Average loss: 0.2324\n",
      "Epoch: 65 | Training Batch: 150 | Average loss: 0.2318\n",
      "Epoch: 65 | Training Batch: 160 | Average loss: 0.2306\n",
      "Epoch: 65 | Training Batch: 170 | Average loss: 0.2304\n",
      "Epoch: 65 | Training Batch: 180 | Average loss: 0.2298\n",
      "Epoch: 65 | Training Batch: 190 | Average loss: 0.2335\n",
      "Epoch: 65 | Training Batch: 200 | Average loss: 0.2296\n",
      "Epoch: 65 | Training Batch: 210 | Average loss: 0.2311\n",
      "Epoch: 65 | Training Batch: 220 | Average loss: 0.2327\n",
      "Average training batch loss at epoch 65: 0.2407\n",
      "Average validation fold accuracy at epoch 65: 0.2566\n",
      "Epoch: 66 | Training Batch: 10 | Average loss: 0.2325\n",
      "Epoch: 66 | Training Batch: 20 | Average loss: 0.2338\n",
      "Epoch: 66 | Training Batch: 30 | Average loss: 0.2253\n",
      "Epoch: 66 | Training Batch: 40 | Average loss: 0.2383\n",
      "Epoch: 66 | Training Batch: 50 | Average loss: 0.2292\n",
      "Epoch: 66 | Training Batch: 60 | Average loss: 0.2308\n",
      "Epoch: 66 | Training Batch: 70 | Average loss: 0.2294\n",
      "Epoch: 66 | Training Batch: 80 | Average loss: 0.2349\n",
      "Epoch: 66 | Training Batch: 90 | Average loss: 0.2275\n",
      "Epoch: 66 | Training Batch: 100 | Average loss: 0.2332\n",
      "Epoch: 66 | Training Batch: 110 | Average loss: 0.2349\n",
      "Epoch: 66 | Training Batch: 120 | Average loss: 0.2311\n",
      "Epoch: 66 | Training Batch: 130 | Average loss: 0.2292\n",
      "Epoch: 66 | Training Batch: 140 | Average loss: 0.2368\n",
      "Epoch: 66 | Training Batch: 150 | Average loss: 0.2338\n",
      "Epoch: 66 | Training Batch: 160 | Average loss: 0.2311\n",
      "Epoch: 66 | Training Batch: 170 | Average loss: 0.2296\n",
      "Epoch: 66 | Training Batch: 180 | Average loss: 0.2312\n",
      "Epoch: 66 | Training Batch: 190 | Average loss: 0.2288\n",
      "Epoch: 66 | Training Batch: 200 | Average loss: 0.2289\n",
      "Epoch: 66 | Training Batch: 210 | Average loss: 0.2337\n",
      "Epoch: 66 | Training Batch: 220 | Average loss: 0.2309\n",
      "Average training batch loss at epoch 66: 0.2406\n",
      "Average validation fold accuracy at epoch 66: 0.2565\n",
      "Epoch: 67 | Training Batch: 10 | Average loss: 0.2297\n",
      "Epoch: 67 | Training Batch: 20 | Average loss: 0.2324\n",
      "Epoch: 67 | Training Batch: 30 | Average loss: 0.2322\n",
      "Epoch: 67 | Training Batch: 40 | Average loss: 0.2324\n",
      "Epoch: 67 | Training Batch: 50 | Average loss: 0.2316\n",
      "Epoch: 67 | Training Batch: 60 | Average loss: 0.2297\n",
      "Epoch: 67 | Training Batch: 70 | Average loss: 0.2302\n",
      "Epoch: 67 | Training Batch: 80 | Average loss: 0.2298\n",
      "Epoch: 67 | Training Batch: 90 | Average loss: 0.2298\n",
      "Epoch: 67 | Training Batch: 100 | Average loss: 0.2337\n",
      "Epoch: 67 | Training Batch: 110 | Average loss: 0.2323\n",
      "Epoch: 67 | Training Batch: 120 | Average loss: 0.2294\n",
      "Epoch: 67 | Training Batch: 130 | Average loss: 0.2286\n",
      "Epoch: 67 | Training Batch: 140 | Average loss: 0.2317\n",
      "Epoch: 67 | Training Batch: 150 | Average loss: 0.2314\n",
      "Epoch: 67 | Training Batch: 160 | Average loss: 0.2317\n",
      "Epoch: 67 | Training Batch: 170 | Average loss: 0.2284\n",
      "Epoch: 67 | Training Batch: 180 | Average loss: 0.2355\n",
      "Epoch: 67 | Training Batch: 190 | Average loss: 0.2286\n",
      "Epoch: 67 | Training Batch: 200 | Average loss: 0.2339\n",
      "Epoch: 67 | Training Batch: 210 | Average loss: 0.2331\n",
      "Epoch: 67 | Training Batch: 220 | Average loss: 0.2333\n",
      "Average training batch loss at epoch 67: 0.2404\n",
      "Average validation fold accuracy at epoch 67: 0.2565\n",
      "Epoch: 68 | Training Batch: 10 | Average loss: 0.2281\n",
      "Epoch: 68 | Training Batch: 20 | Average loss: 0.2304\n",
      "Epoch: 68 | Training Batch: 30 | Average loss: 0.2298\n",
      "Epoch: 68 | Training Batch: 40 | Average loss: 0.2326\n",
      "Epoch: 68 | Training Batch: 50 | Average loss: 0.2315\n",
      "Epoch: 68 | Training Batch: 60 | Average loss: 0.2348\n",
      "Epoch: 68 | Training Batch: 70 | Average loss: 0.2292\n",
      "Epoch: 68 | Training Batch: 80 | Average loss: 0.2291\n",
      "Epoch: 68 | Training Batch: 90 | Average loss: 0.2338\n",
      "Epoch: 68 | Training Batch: 100 | Average loss: 0.2308\n",
      "Epoch: 68 | Training Batch: 110 | Average loss: 0.2280\n",
      "Epoch: 68 | Training Batch: 120 | Average loss: 0.2265\n",
      "Epoch: 68 | Training Batch: 130 | Average loss: 0.2322\n",
      "Epoch: 68 | Training Batch: 140 | Average loss: 0.2330\n",
      "Epoch: 68 | Training Batch: 150 | Average loss: 0.2310\n",
      "Epoch: 68 | Training Batch: 160 | Average loss: 0.2268\n",
      "Epoch: 68 | Training Batch: 170 | Average loss: 0.2342\n",
      "Epoch: 68 | Training Batch: 180 | Average loss: 0.2340\n",
      "Epoch: 68 | Training Batch: 190 | Average loss: 0.2350\n",
      "Epoch: 68 | Training Batch: 200 | Average loss: 0.2321\n",
      "Epoch: 68 | Training Batch: 210 | Average loss: 0.2318\n",
      "Epoch: 68 | Training Batch: 220 | Average loss: 0.2325\n",
      "Average training batch loss at epoch 68: 0.2403\n",
      "Average validation fold accuracy at epoch 68: 0.2565\n",
      "Epoch: 69 | Training Batch: 10 | Average loss: 0.2275\n",
      "Epoch: 69 | Training Batch: 20 | Average loss: 0.2281\n",
      "Epoch: 69 | Training Batch: 30 | Average loss: 0.2304\n",
      "Epoch: 69 | Training Batch: 40 | Average loss: 0.2316\n",
      "Epoch: 69 | Training Batch: 50 | Average loss: 0.2336\n",
      "Epoch: 69 | Training Batch: 60 | Average loss: 0.2358\n",
      "Epoch: 69 | Training Batch: 70 | Average loss: 0.2327\n",
      "Epoch: 69 | Training Batch: 80 | Average loss: 0.2307\n",
      "Epoch: 69 | Training Batch: 90 | Average loss: 0.2269\n",
      "Epoch: 69 | Training Batch: 100 | Average loss: 0.2277\n",
      "Epoch: 69 | Training Batch: 110 | Average loss: 0.2323\n",
      "Epoch: 69 | Training Batch: 120 | Average loss: 0.2311\n",
      "Epoch: 69 | Training Batch: 130 | Average loss: 0.2297\n",
      "Epoch: 69 | Training Batch: 140 | Average loss: 0.2297\n",
      "Epoch: 69 | Training Batch: 150 | Average loss: 0.2338\n",
      "Epoch: 69 | Training Batch: 160 | Average loss: 0.2362\n",
      "Epoch: 69 | Training Batch: 170 | Average loss: 0.2342\n",
      "Epoch: 69 | Training Batch: 180 | Average loss: 0.2330\n",
      "Epoch: 69 | Training Batch: 190 | Average loss: 0.2308\n",
      "Epoch: 69 | Training Batch: 200 | Average loss: 0.2319\n",
      "Epoch: 69 | Training Batch: 210 | Average loss: 0.2327\n",
      "Epoch: 69 | Training Batch: 220 | Average loss: 0.2307\n",
      "Average training batch loss at epoch 69: 0.2402\n",
      "Average validation fold accuracy at epoch 69: 0.2564\n",
      "Epoch: 70 | Training Batch: 10 | Average loss: 0.2322\n",
      "Epoch: 70 | Training Batch: 20 | Average loss: 0.2320\n",
      "Epoch: 70 | Training Batch: 30 | Average loss: 0.2311\n",
      "Epoch: 70 | Training Batch: 40 | Average loss: 0.2322\n",
      "Epoch: 70 | Training Batch: 50 | Average loss: 0.2323\n",
      "Epoch: 70 | Training Batch: 60 | Average loss: 0.2324\n",
      "Epoch: 70 | Training Batch: 70 | Average loss: 0.2316\n",
      "Epoch: 70 | Training Batch: 80 | Average loss: 0.2321\n",
      "Epoch: 70 | Training Batch: 90 | Average loss: 0.2281\n",
      "Epoch: 70 | Training Batch: 100 | Average loss: 0.2326\n",
      "Epoch: 70 | Training Batch: 110 | Average loss: 0.2326\n",
      "Epoch: 70 | Training Batch: 120 | Average loss: 0.2336\n",
      "Epoch: 70 | Training Batch: 130 | Average loss: 0.2289\n",
      "Epoch: 70 | Training Batch: 140 | Average loss: 0.2308\n",
      "Epoch: 70 | Training Batch: 150 | Average loss: 0.2317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70 | Training Batch: 160 | Average loss: 0.2319\n",
      "Epoch: 70 | Training Batch: 170 | Average loss: 0.2333\n",
      "Epoch: 70 | Training Batch: 180 | Average loss: 0.2278\n",
      "Epoch: 70 | Training Batch: 190 | Average loss: 0.2295\n",
      "Epoch: 70 | Training Batch: 200 | Average loss: 0.2296\n",
      "Epoch: 70 | Training Batch: 210 | Average loss: 0.2332\n",
      "Epoch: 70 | Training Batch: 220 | Average loss: 0.2319\n",
      "Average training batch loss at epoch 70: 0.2401\n",
      "Average validation fold accuracy at epoch 70: 0.2564\n",
      "Epoch: 71 | Training Batch: 10 | Average loss: 0.2305\n",
      "Epoch: 71 | Training Batch: 20 | Average loss: 0.2319\n",
      "Epoch: 71 | Training Batch: 30 | Average loss: 0.2330\n",
      "Epoch: 71 | Training Batch: 40 | Average loss: 0.2331\n",
      "Epoch: 71 | Training Batch: 50 | Average loss: 0.2309\n",
      "Epoch: 71 | Training Batch: 60 | Average loss: 0.2339\n",
      "Epoch: 71 | Training Batch: 70 | Average loss: 0.2369\n",
      "Epoch: 71 | Training Batch: 80 | Average loss: 0.2317\n",
      "Epoch: 71 | Training Batch: 90 | Average loss: 0.2290\n",
      "Epoch: 71 | Training Batch: 100 | Average loss: 0.2310\n",
      "Epoch: 71 | Training Batch: 110 | Average loss: 0.2302\n",
      "Epoch: 71 | Training Batch: 120 | Average loss: 0.2320\n",
      "Epoch: 71 | Training Batch: 130 | Average loss: 0.2315\n",
      "Epoch: 71 | Training Batch: 140 | Average loss: 0.2311\n",
      "Epoch: 71 | Training Batch: 150 | Average loss: 0.2339\n",
      "Epoch: 71 | Training Batch: 160 | Average loss: 0.2309\n",
      "Epoch: 71 | Training Batch: 170 | Average loss: 0.2302\n",
      "Epoch: 71 | Training Batch: 180 | Average loss: 0.2286\n",
      "Epoch: 71 | Training Batch: 190 | Average loss: 0.2304\n",
      "Epoch: 71 | Training Batch: 200 | Average loss: 0.2286\n",
      "Epoch: 71 | Training Batch: 210 | Average loss: 0.2317\n",
      "Epoch: 71 | Training Batch: 220 | Average loss: 0.2298\n",
      "Average training batch loss at epoch 71: 0.2399\n",
      "Average validation fold accuracy at epoch 71: 0.2564\n",
      "Epoch: 72 | Training Batch: 10 | Average loss: 0.2297\n",
      "Epoch: 72 | Training Batch: 20 | Average loss: 0.2322\n",
      "Epoch: 72 | Training Batch: 30 | Average loss: 0.2281\n",
      "Epoch: 72 | Training Batch: 40 | Average loss: 0.2310\n",
      "Epoch: 72 | Training Batch: 50 | Average loss: 0.2367\n",
      "Epoch: 72 | Training Batch: 60 | Average loss: 0.2376\n",
      "Epoch: 72 | Training Batch: 70 | Average loss: 0.2356\n",
      "Epoch: 72 | Training Batch: 80 | Average loss: 0.2302\n",
      "Epoch: 72 | Training Batch: 90 | Average loss: 0.2293\n",
      "Epoch: 72 | Training Batch: 100 | Average loss: 0.2322\n",
      "Epoch: 72 | Training Batch: 110 | Average loss: 0.2289\n",
      "Epoch: 72 | Training Batch: 120 | Average loss: 0.2286\n",
      "Epoch: 72 | Training Batch: 130 | Average loss: 0.2325\n",
      "Epoch: 72 | Training Batch: 140 | Average loss: 0.2303\n",
      "Epoch: 72 | Training Batch: 150 | Average loss: 0.2276\n",
      "Epoch: 72 | Training Batch: 160 | Average loss: 0.2338\n",
      "Epoch: 72 | Training Batch: 170 | Average loss: 0.2249\n",
      "Epoch: 72 | Training Batch: 180 | Average loss: 0.2309\n",
      "Epoch: 72 | Training Batch: 190 | Average loss: 0.2314\n",
      "Epoch: 72 | Training Batch: 200 | Average loss: 0.2312\n",
      "Epoch: 72 | Training Batch: 210 | Average loss: 0.2354\n",
      "Epoch: 72 | Training Batch: 220 | Average loss: 0.2322\n",
      "Average training batch loss at epoch 72: 0.2398\n",
      "Average validation fold accuracy at epoch 72: 0.2563\n",
      "Epoch: 73 | Training Batch: 10 | Average loss: 0.2284\n",
      "Epoch: 73 | Training Batch: 20 | Average loss: 0.2332\n",
      "Epoch: 73 | Training Batch: 30 | Average loss: 0.2311\n",
      "Epoch: 73 | Training Batch: 40 | Average loss: 0.2333\n",
      "Epoch: 73 | Training Batch: 50 | Average loss: 0.2318\n",
      "Epoch: 73 | Training Batch: 60 | Average loss: 0.2316\n",
      "Epoch: 73 | Training Batch: 70 | Average loss: 0.2292\n",
      "Epoch: 73 | Training Batch: 80 | Average loss: 0.2357\n",
      "Epoch: 73 | Training Batch: 90 | Average loss: 0.2303\n",
      "Epoch: 73 | Training Batch: 100 | Average loss: 0.2280\n",
      "Epoch: 73 | Training Batch: 110 | Average loss: 0.2342\n",
      "Epoch: 73 | Training Batch: 120 | Average loss: 0.2313\n",
      "Epoch: 73 | Training Batch: 130 | Average loss: 0.2303\n",
      "Epoch: 73 | Training Batch: 140 | Average loss: 0.2287\n",
      "Epoch: 73 | Training Batch: 150 | Average loss: 0.2262\n",
      "Epoch: 73 | Training Batch: 160 | Average loss: 0.2322\n",
      "Epoch: 73 | Training Batch: 170 | Average loss: 0.2323\n",
      "Epoch: 73 | Training Batch: 180 | Average loss: 0.2335\n",
      "Epoch: 73 | Training Batch: 190 | Average loss: 0.2302\n",
      "Epoch: 73 | Training Batch: 200 | Average loss: 0.2333\n",
      "Epoch: 73 | Training Batch: 210 | Average loss: 0.2326\n",
      "Epoch: 73 | Training Batch: 220 | Average loss: 0.2305\n",
      "Average training batch loss at epoch 73: 0.2397\n",
      "Average validation fold accuracy at epoch 73: 0.2563\n",
      "Epoch: 74 | Training Batch: 10 | Average loss: 0.2275\n",
      "Epoch: 74 | Training Batch: 20 | Average loss: 0.2295\n",
      "Epoch: 74 | Training Batch: 30 | Average loss: 0.2290\n",
      "Epoch: 74 | Training Batch: 40 | Average loss: 0.2321\n",
      "Epoch: 74 | Training Batch: 50 | Average loss: 0.2282\n",
      "Epoch: 74 | Training Batch: 60 | Average loss: 0.2356\n",
      "Epoch: 74 | Training Batch: 70 | Average loss: 0.2281\n",
      "Epoch: 74 | Training Batch: 80 | Average loss: 0.2330\n",
      "Epoch: 74 | Training Batch: 90 | Average loss: 0.2276\n",
      "Epoch: 74 | Training Batch: 100 | Average loss: 0.2334\n",
      "Epoch: 74 | Training Batch: 110 | Average loss: 0.2357\n",
      "Epoch: 74 | Training Batch: 120 | Average loss: 0.2329\n",
      "Epoch: 74 | Training Batch: 130 | Average loss: 0.2317\n",
      "Epoch: 74 | Training Batch: 140 | Average loss: 0.2292\n",
      "Epoch: 74 | Training Batch: 150 | Average loss: 0.2287\n",
      "Epoch: 74 | Training Batch: 160 | Average loss: 0.2319\n",
      "Epoch: 74 | Training Batch: 170 | Average loss: 0.2358\n",
      "Epoch: 74 | Training Batch: 180 | Average loss: 0.2289\n",
      "Epoch: 74 | Training Batch: 190 | Average loss: 0.2310\n",
      "Epoch: 74 | Training Batch: 200 | Average loss: 0.2355\n",
      "Epoch: 74 | Training Batch: 210 | Average loss: 0.2338\n",
      "Epoch: 74 | Training Batch: 220 | Average loss: 0.2308\n",
      "Average training batch loss at epoch 74: 0.2396\n",
      "Average validation fold accuracy at epoch 74: 0.2563\n",
      "Epoch: 75 | Training Batch: 10 | Average loss: 0.2307\n",
      "Epoch: 75 | Training Batch: 20 | Average loss: 0.2348\n",
      "Epoch: 75 | Training Batch: 30 | Average loss: 0.2298\n",
      "Epoch: 75 | Training Batch: 40 | Average loss: 0.2301\n",
      "Epoch: 75 | Training Batch: 50 | Average loss: 0.2353\n",
      "Epoch: 75 | Training Batch: 60 | Average loss: 0.2283\n",
      "Epoch: 75 | Training Batch: 70 | Average loss: 0.2312\n",
      "Epoch: 75 | Training Batch: 80 | Average loss: 0.2295\n",
      "Epoch: 75 | Training Batch: 90 | Average loss: 0.2283\n",
      "Epoch: 75 | Training Batch: 100 | Average loss: 0.2354\n",
      "Epoch: 75 | Training Batch: 110 | Average loss: 0.2337\n",
      "Epoch: 75 | Training Batch: 120 | Average loss: 0.2325\n",
      "Epoch: 75 | Training Batch: 130 | Average loss: 0.2332\n",
      "Epoch: 75 | Training Batch: 140 | Average loss: 0.2289\n",
      "Epoch: 75 | Training Batch: 150 | Average loss: 0.2276\n",
      "Epoch: 75 | Training Batch: 160 | Average loss: 0.2319\n",
      "Epoch: 75 | Training Batch: 170 | Average loss: 0.2355\n",
      "Epoch: 75 | Training Batch: 180 | Average loss: 0.2318\n",
      "Epoch: 75 | Training Batch: 190 | Average loss: 0.2283\n",
      "Epoch: 75 | Training Batch: 200 | Average loss: 0.2300\n",
      "Epoch: 75 | Training Batch: 210 | Average loss: 0.2303\n",
      "Epoch: 75 | Training Batch: 220 | Average loss: 0.2286\n",
      "Average training batch loss at epoch 75: 0.2395\n",
      "Average validation fold accuracy at epoch 75: 0.2562\n",
      "Epoch: 76 | Training Batch: 10 | Average loss: 0.2339\n",
      "Epoch: 76 | Training Batch: 20 | Average loss: 0.2312\n",
      "Epoch: 76 | Training Batch: 30 | Average loss: 0.2322\n",
      "Epoch: 76 | Training Batch: 40 | Average loss: 0.2336\n",
      "Epoch: 76 | Training Batch: 50 | Average loss: 0.2295\n",
      "Epoch: 76 | Training Batch: 60 | Average loss: 0.2312\n",
      "Epoch: 76 | Training Batch: 70 | Average loss: 0.2327\n",
      "Epoch: 76 | Training Batch: 80 | Average loss: 0.2350\n",
      "Epoch: 76 | Training Batch: 90 | Average loss: 0.2310\n",
      "Epoch: 76 | Training Batch: 100 | Average loss: 0.2321\n",
      "Epoch: 76 | Training Batch: 110 | Average loss: 0.2279\n",
      "Epoch: 76 | Training Batch: 120 | Average loss: 0.2289\n",
      "Epoch: 76 | Training Batch: 130 | Average loss: 0.2327\n",
      "Epoch: 76 | Training Batch: 140 | Average loss: 0.2295\n",
      "Epoch: 76 | Training Batch: 150 | Average loss: 0.2335\n",
      "Epoch: 76 | Training Batch: 160 | Average loss: 0.2331\n",
      "Epoch: 76 | Training Batch: 170 | Average loss: 0.2291\n",
      "Epoch: 76 | Training Batch: 180 | Average loss: 0.2302\n",
      "Epoch: 76 | Training Batch: 190 | Average loss: 0.2292\n",
      "Epoch: 76 | Training Batch: 200 | Average loss: 0.2276\n",
      "Epoch: 76 | Training Batch: 210 | Average loss: 0.2317\n",
      "Epoch: 76 | Training Batch: 220 | Average loss: 0.2294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training batch loss at epoch 76: 0.2394\n",
      "Average validation fold accuracy at epoch 76: 0.2562\n",
      "Epoch: 77 | Training Batch: 10 | Average loss: 0.2334\n",
      "Epoch: 77 | Training Batch: 20 | Average loss: 0.2299\n",
      "Epoch: 77 | Training Batch: 30 | Average loss: 0.2314\n",
      "Epoch: 77 | Training Batch: 40 | Average loss: 0.2335\n",
      "Epoch: 77 | Training Batch: 50 | Average loss: 0.2305\n",
      "Epoch: 77 | Training Batch: 60 | Average loss: 0.2294\n",
      "Epoch: 77 | Training Batch: 70 | Average loss: 0.2313\n",
      "Epoch: 77 | Training Batch: 80 | Average loss: 0.2288\n",
      "Epoch: 77 | Training Batch: 90 | Average loss: 0.2299\n",
      "Epoch: 77 | Training Batch: 100 | Average loss: 0.2312\n",
      "Epoch: 77 | Training Batch: 110 | Average loss: 0.2280\n",
      "Epoch: 77 | Training Batch: 120 | Average loss: 0.2341\n",
      "Epoch: 77 | Training Batch: 130 | Average loss: 0.2288\n",
      "Epoch: 77 | Training Batch: 140 | Average loss: 0.2352\n",
      "Epoch: 77 | Training Batch: 150 | Average loss: 0.2349\n",
      "Epoch: 77 | Training Batch: 160 | Average loss: 0.2364\n",
      "Epoch: 77 | Training Batch: 170 | Average loss: 0.2275\n",
      "Epoch: 77 | Training Batch: 180 | Average loss: 0.2268\n",
      "Epoch: 77 | Training Batch: 190 | Average loss: 0.2329\n",
      "Epoch: 77 | Training Batch: 200 | Average loss: 0.2310\n",
      "Epoch: 77 | Training Batch: 210 | Average loss: 0.2290\n",
      "Epoch: 77 | Training Batch: 220 | Average loss: 0.2315\n",
      "Average training batch loss at epoch 77: 0.2393\n",
      "Average validation fold accuracy at epoch 77: 0.2562\n",
      "Epoch: 78 | Training Batch: 10 | Average loss: 0.2331\n",
      "Epoch: 78 | Training Batch: 20 | Average loss: 0.2325\n",
      "Epoch: 78 | Training Batch: 30 | Average loss: 0.2315\n",
      "Epoch: 78 | Training Batch: 40 | Average loss: 0.2310\n",
      "Epoch: 78 | Training Batch: 50 | Average loss: 0.2334\n",
      "Epoch: 78 | Training Batch: 60 | Average loss: 0.2291\n",
      "Epoch: 78 | Training Batch: 70 | Average loss: 0.2255\n",
      "Epoch: 78 | Training Batch: 80 | Average loss: 0.2330\n",
      "Epoch: 78 | Training Batch: 90 | Average loss: 0.2272\n",
      "Epoch: 78 | Training Batch: 100 | Average loss: 0.2325\n",
      "Epoch: 78 | Training Batch: 110 | Average loss: 0.2284\n",
      "Epoch: 78 | Training Batch: 120 | Average loss: 0.2330\n",
      "Epoch: 78 | Training Batch: 130 | Average loss: 0.2270\n",
      "Epoch: 78 | Training Batch: 140 | Average loss: 0.2278\n",
      "Epoch: 78 | Training Batch: 150 | Average loss: 0.2379\n",
      "Epoch: 78 | Training Batch: 160 | Average loss: 0.2348\n",
      "Epoch: 78 | Training Batch: 170 | Average loss: 0.2315\n",
      "Epoch: 78 | Training Batch: 180 | Average loss: 0.2345\n",
      "Epoch: 78 | Training Batch: 190 | Average loss: 0.2279\n",
      "Epoch: 78 | Training Batch: 200 | Average loss: 0.2346\n",
      "Epoch: 78 | Training Batch: 210 | Average loss: 0.2290\n",
      "Epoch: 78 | Training Batch: 220 | Average loss: 0.2296\n",
      "Average training batch loss at epoch 78: 0.2392\n",
      "Average validation fold accuracy at epoch 78: 0.2561\n",
      "Epoch: 79 | Training Batch: 10 | Average loss: 0.2289\n",
      "Epoch: 79 | Training Batch: 20 | Average loss: 0.2356\n",
      "Epoch: 79 | Training Batch: 30 | Average loss: 0.2263\n",
      "Epoch: 79 | Training Batch: 40 | Average loss: 0.2361\n",
      "Epoch: 79 | Training Batch: 50 | Average loss: 0.2307\n",
      "Epoch: 79 | Training Batch: 60 | Average loss: 0.2311\n",
      "Epoch: 79 | Training Batch: 70 | Average loss: 0.2322\n",
      "Epoch: 79 | Training Batch: 80 | Average loss: 0.2380\n",
      "Epoch: 79 | Training Batch: 90 | Average loss: 0.2302\n",
      "Epoch: 79 | Training Batch: 100 | Average loss: 0.2323\n",
      "Epoch: 79 | Training Batch: 110 | Average loss: 0.2332\n",
      "Epoch: 79 | Training Batch: 120 | Average loss: 0.2280\n",
      "Epoch: 79 | Training Batch: 130 | Average loss: 0.2272\n",
      "Epoch: 79 | Training Batch: 140 | Average loss: 0.2329\n",
      "Epoch: 79 | Training Batch: 150 | Average loss: 0.2287\n",
      "Epoch: 79 | Training Batch: 160 | Average loss: 0.2347\n",
      "Epoch: 79 | Training Batch: 170 | Average loss: 0.2284\n",
      "Epoch: 79 | Training Batch: 180 | Average loss: 0.2299\n",
      "Epoch: 79 | Training Batch: 190 | Average loss: 0.2288\n",
      "Epoch: 79 | Training Batch: 200 | Average loss: 0.2308\n",
      "Epoch: 79 | Training Batch: 210 | Average loss: 0.2307\n",
      "Epoch: 79 | Training Batch: 220 | Average loss: 0.2329\n",
      "Average training batch loss at epoch 79: 0.2391\n",
      "Average validation fold accuracy at epoch 79: 0.2561\n",
      "Epoch: 80 | Training Batch: 10 | Average loss: 0.2282\n",
      "Epoch: 80 | Training Batch: 20 | Average loss: 0.2323\n",
      "Epoch: 80 | Training Batch: 30 | Average loss: 0.2339\n",
      "Epoch: 80 | Training Batch: 40 | Average loss: 0.2345\n",
      "Epoch: 80 | Training Batch: 50 | Average loss: 0.2296\n",
      "Epoch: 80 | Training Batch: 60 | Average loss: 0.2292\n",
      "Epoch: 80 | Training Batch: 70 | Average loss: 0.2317\n",
      "Epoch: 80 | Training Batch: 80 | Average loss: 0.2280\n",
      "Epoch: 80 | Training Batch: 90 | Average loss: 0.2265\n",
      "Epoch: 80 | Training Batch: 100 | Average loss: 0.2311\n",
      "Epoch: 80 | Training Batch: 110 | Average loss: 0.2335\n",
      "Epoch: 80 | Training Batch: 120 | Average loss: 0.2310\n",
      "Epoch: 80 | Training Batch: 130 | Average loss: 0.2353\n",
      "Epoch: 80 | Training Batch: 140 | Average loss: 0.2299\n",
      "Epoch: 80 | Training Batch: 150 | Average loss: 0.2283\n",
      "Epoch: 80 | Training Batch: 160 | Average loss: 0.2305\n",
      "Epoch: 80 | Training Batch: 170 | Average loss: 0.2326\n",
      "Epoch: 80 | Training Batch: 180 | Average loss: 0.2298\n",
      "Epoch: 80 | Training Batch: 190 | Average loss: 0.2345\n",
      "Epoch: 80 | Training Batch: 200 | Average loss: 0.2302\n",
      "Epoch: 80 | Training Batch: 210 | Average loss: 0.2274\n",
      "Epoch: 80 | Training Batch: 220 | Average loss: 0.2331\n",
      "Average training batch loss at epoch 80: 0.2390\n",
      "Average validation fold accuracy at epoch 80: 0.2561\n",
      "Epoch: 81 | Training Batch: 10 | Average loss: 0.2327\n",
      "Epoch: 81 | Training Batch: 20 | Average loss: 0.2284\n",
      "Epoch: 81 | Training Batch: 30 | Average loss: 0.2255\n",
      "Epoch: 81 | Training Batch: 40 | Average loss: 0.2297\n",
      "Epoch: 81 | Training Batch: 50 | Average loss: 0.2295\n",
      "Epoch: 81 | Training Batch: 60 | Average loss: 0.2298\n",
      "Epoch: 81 | Training Batch: 70 | Average loss: 0.2337\n",
      "Epoch: 81 | Training Batch: 80 | Average loss: 0.2312\n",
      "Epoch: 81 | Training Batch: 90 | Average loss: 0.2295\n",
      "Epoch: 81 | Training Batch: 100 | Average loss: 0.2304\n",
      "Epoch: 81 | Training Batch: 110 | Average loss: 0.2315\n",
      "Epoch: 81 | Training Batch: 120 | Average loss: 0.2317\n",
      "Epoch: 81 | Training Batch: 130 | Average loss: 0.2307\n",
      "Epoch: 81 | Training Batch: 140 | Average loss: 0.2310\n",
      "Epoch: 81 | Training Batch: 150 | Average loss: 0.2313\n",
      "Epoch: 81 | Training Batch: 160 | Average loss: 0.2351\n",
      "Epoch: 81 | Training Batch: 170 | Average loss: 0.2329\n",
      "Epoch: 81 | Training Batch: 180 | Average loss: 0.2332\n",
      "Epoch: 81 | Training Batch: 190 | Average loss: 0.2333\n",
      "Epoch: 81 | Training Batch: 200 | Average loss: 0.2268\n",
      "Epoch: 81 | Training Batch: 210 | Average loss: 0.2314\n",
      "Epoch: 81 | Training Batch: 220 | Average loss: 0.2312\n",
      "Average training batch loss at epoch 81: 0.2389\n",
      "Average validation fold accuracy at epoch 81: 0.2561\n",
      "Epoch: 82 | Training Batch: 10 | Average loss: 0.2255\n",
      "Epoch: 82 | Training Batch: 20 | Average loss: 0.2304\n",
      "Epoch: 82 | Training Batch: 30 | Average loss: 0.2330\n",
      "Epoch: 82 | Training Batch: 40 | Average loss: 0.2291\n",
      "Epoch: 82 | Training Batch: 50 | Average loss: 0.2339\n",
      "Epoch: 82 | Training Batch: 60 | Average loss: 0.2305\n",
      "Epoch: 82 | Training Batch: 70 | Average loss: 0.2314\n",
      "Epoch: 82 | Training Batch: 80 | Average loss: 0.2312\n",
      "Epoch: 82 | Training Batch: 90 | Average loss: 0.2265\n",
      "Epoch: 82 | Training Batch: 100 | Average loss: 0.2323\n",
      "Epoch: 82 | Training Batch: 110 | Average loss: 0.2316\n",
      "Epoch: 82 | Training Batch: 120 | Average loss: 0.2327\n",
      "Epoch: 82 | Training Batch: 130 | Average loss: 0.2349\n",
      "Epoch: 82 | Training Batch: 140 | Average loss: 0.2331\n",
      "Epoch: 82 | Training Batch: 150 | Average loss: 0.2314\n",
      "Epoch: 82 | Training Batch: 160 | Average loss: 0.2335\n",
      "Epoch: 82 | Training Batch: 170 | Average loss: 0.2316\n",
      "Epoch: 82 | Training Batch: 180 | Average loss: 0.2272\n",
      "Epoch: 82 | Training Batch: 190 | Average loss: 0.2295\n",
      "Epoch: 82 | Training Batch: 200 | Average loss: 0.2311\n",
      "Epoch: 82 | Training Batch: 210 | Average loss: 0.2293\n",
      "Epoch: 82 | Training Batch: 220 | Average loss: 0.2329\n",
      "Average training batch loss at epoch 82: 0.2388\n",
      "Average validation fold accuracy at epoch 82: 0.2560\n",
      "Epoch: 83 | Training Batch: 10 | Average loss: 0.2302\n",
      "Epoch: 83 | Training Batch: 20 | Average loss: 0.2318\n",
      "Epoch: 83 | Training Batch: 30 | Average loss: 0.2314\n",
      "Epoch: 83 | Training Batch: 40 | Average loss: 0.2279\n",
      "Epoch: 83 | Training Batch: 50 | Average loss: 0.2288\n",
      "Epoch: 83 | Training Batch: 60 | Average loss: 0.2342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83 | Training Batch: 70 | Average loss: 0.2321\n",
      "Epoch: 83 | Training Batch: 80 | Average loss: 0.2305\n",
      "Epoch: 83 | Training Batch: 90 | Average loss: 0.2290\n",
      "Epoch: 83 | Training Batch: 100 | Average loss: 0.2332\n",
      "Epoch: 83 | Training Batch: 110 | Average loss: 0.2305\n",
      "Epoch: 83 | Training Batch: 120 | Average loss: 0.2324\n",
      "Epoch: 83 | Training Batch: 130 | Average loss: 0.2272\n",
      "Epoch: 83 | Training Batch: 140 | Average loss: 0.2341\n",
      "Epoch: 83 | Training Batch: 150 | Average loss: 0.2287\n",
      "Epoch: 83 | Training Batch: 160 | Average loss: 0.2280\n",
      "Epoch: 83 | Training Batch: 170 | Average loss: 0.2292\n",
      "Epoch: 83 | Training Batch: 180 | Average loss: 0.2344\n",
      "Epoch: 83 | Training Batch: 190 | Average loss: 0.2315\n",
      "Epoch: 83 | Training Batch: 200 | Average loss: 0.2288\n",
      "Epoch: 83 | Training Batch: 210 | Average loss: 0.2347\n",
      "Epoch: 83 | Training Batch: 220 | Average loss: 0.2325\n",
      "Average training batch loss at epoch 83: 0.2387\n",
      "Average validation fold accuracy at epoch 83: 0.2560\n",
      "Epoch: 84 | Training Batch: 10 | Average loss: 0.2299\n",
      "Epoch: 84 | Training Batch: 20 | Average loss: 0.2298\n",
      "Epoch: 84 | Training Batch: 30 | Average loss: 0.2287\n",
      "Epoch: 84 | Training Batch: 40 | Average loss: 0.2310\n",
      "Epoch: 84 | Training Batch: 50 | Average loss: 0.2287\n",
      "Epoch: 84 | Training Batch: 60 | Average loss: 0.2310\n",
      "Epoch: 84 | Training Batch: 70 | Average loss: 0.2354\n",
      "Epoch: 84 | Training Batch: 80 | Average loss: 0.2324\n",
      "Epoch: 84 | Training Batch: 90 | Average loss: 0.2313\n",
      "Epoch: 84 | Training Batch: 100 | Average loss: 0.2332\n",
      "Epoch: 84 | Training Batch: 110 | Average loss: 0.2325\n",
      "Epoch: 84 | Training Batch: 120 | Average loss: 0.2311\n",
      "Epoch: 84 | Training Batch: 130 | Average loss: 0.2303\n",
      "Epoch: 84 | Training Batch: 140 | Average loss: 0.2306\n",
      "Epoch: 84 | Training Batch: 150 | Average loss: 0.2336\n",
      "Epoch: 84 | Training Batch: 160 | Average loss: 0.2275\n",
      "Epoch: 84 | Training Batch: 170 | Average loss: 0.2334\n",
      "Epoch: 84 | Training Batch: 180 | Average loss: 0.2311\n",
      "Epoch: 84 | Training Batch: 190 | Average loss: 0.2296\n",
      "Epoch: 84 | Training Batch: 200 | Average loss: 0.2313\n",
      "Epoch: 84 | Training Batch: 210 | Average loss: 0.2277\n",
      "Epoch: 84 | Training Batch: 220 | Average loss: 0.2301\n",
      "Average training batch loss at epoch 84: 0.2386\n",
      "Average validation fold accuracy at epoch 84: 0.2560\n",
      "Epoch: 85 | Training Batch: 10 | Average loss: 0.2331\n",
      "Epoch: 85 | Training Batch: 20 | Average loss: 0.2281\n",
      "Epoch: 85 | Training Batch: 30 | Average loss: 0.2314\n",
      "Epoch: 85 | Training Batch: 40 | Average loss: 0.2272\n",
      "Epoch: 85 | Training Batch: 50 | Average loss: 0.2304\n",
      "Epoch: 85 | Training Batch: 60 | Average loss: 0.2318\n",
      "Epoch: 85 | Training Batch: 70 | Average loss: 0.2332\n",
      "Epoch: 85 | Training Batch: 80 | Average loss: 0.2354\n",
      "Epoch: 85 | Training Batch: 90 | Average loss: 0.2266\n",
      "Epoch: 85 | Training Batch: 100 | Average loss: 0.2307\n",
      "Epoch: 85 | Training Batch: 110 | Average loss: 0.2288\n",
      "Epoch: 85 | Training Batch: 120 | Average loss: 0.2300\n",
      "Epoch: 85 | Training Batch: 130 | Average loss: 0.2346\n",
      "Epoch: 85 | Training Batch: 140 | Average loss: 0.2284\n",
      "Epoch: 85 | Training Batch: 150 | Average loss: 0.2311\n",
      "Epoch: 85 | Training Batch: 160 | Average loss: 0.2295\n",
      "Epoch: 85 | Training Batch: 170 | Average loss: 0.2283\n",
      "Epoch: 85 | Training Batch: 180 | Average loss: 0.2321\n",
      "Epoch: 85 | Training Batch: 190 | Average loss: 0.2348\n",
      "Epoch: 85 | Training Batch: 200 | Average loss: 0.2319\n",
      "Epoch: 85 | Training Batch: 210 | Average loss: 0.2322\n",
      "Epoch: 85 | Training Batch: 220 | Average loss: 0.2323\n",
      "Average training batch loss at epoch 85: 0.2385\n",
      "Average validation fold accuracy at epoch 85: 0.2560\n",
      "Epoch: 86 | Training Batch: 10 | Average loss: 0.2302\n",
      "Epoch: 86 | Training Batch: 20 | Average loss: 0.2302\n",
      "Epoch: 86 | Training Batch: 30 | Average loss: 0.2312\n",
      "Epoch: 86 | Training Batch: 40 | Average loss: 0.2330\n",
      "Epoch: 86 | Training Batch: 50 | Average loss: 0.2293\n",
      "Epoch: 86 | Training Batch: 60 | Average loss: 0.2307\n",
      "Epoch: 86 | Training Batch: 70 | Average loss: 0.2272\n",
      "Epoch: 86 | Training Batch: 80 | Average loss: 0.2282\n",
      "Epoch: 86 | Training Batch: 90 | Average loss: 0.2272\n",
      "Epoch: 86 | Training Batch: 100 | Average loss: 0.2305\n",
      "Epoch: 86 | Training Batch: 110 | Average loss: 0.2287\n",
      "Epoch: 86 | Training Batch: 120 | Average loss: 0.2319\n",
      "Epoch: 86 | Training Batch: 130 | Average loss: 0.2353\n",
      "Epoch: 86 | Training Batch: 140 | Average loss: 0.2306\n",
      "Epoch: 86 | Training Batch: 150 | Average loss: 0.2323\n",
      "Epoch: 86 | Training Batch: 160 | Average loss: 0.2330\n",
      "Epoch: 86 | Training Batch: 170 | Average loss: 0.2307\n",
      "Epoch: 86 | Training Batch: 180 | Average loss: 0.2328\n",
      "Epoch: 86 | Training Batch: 190 | Average loss: 0.2306\n",
      "Epoch: 86 | Training Batch: 200 | Average loss: 0.2287\n",
      "Epoch: 86 | Training Batch: 210 | Average loss: 0.2334\n",
      "Epoch: 86 | Training Batch: 220 | Average loss: 0.2336\n",
      "Average training batch loss at epoch 86: 0.2384\n",
      "Average validation fold accuracy at epoch 86: 0.2559\n",
      "Epoch: 87 | Training Batch: 10 | Average loss: 0.2301\n",
      "Epoch: 87 | Training Batch: 20 | Average loss: 0.2322\n",
      "Epoch: 87 | Training Batch: 30 | Average loss: 0.2298\n",
      "Epoch: 87 | Training Batch: 40 | Average loss: 0.2323\n",
      "Epoch: 87 | Training Batch: 50 | Average loss: 0.2345\n",
      "Epoch: 87 | Training Batch: 60 | Average loss: 0.2317\n",
      "Epoch: 87 | Training Batch: 70 | Average loss: 0.2340\n",
      "Epoch: 87 | Training Batch: 80 | Average loss: 0.2310\n",
      "Epoch: 87 | Training Batch: 90 | Average loss: 0.2300\n",
      "Epoch: 87 | Training Batch: 100 | Average loss: 0.2303\n",
      "Epoch: 87 | Training Batch: 110 | Average loss: 0.2312\n",
      "Epoch: 87 | Training Batch: 120 | Average loss: 0.2333\n",
      "Epoch: 87 | Training Batch: 130 | Average loss: 0.2335\n",
      "Epoch: 87 | Training Batch: 140 | Average loss: 0.2298\n",
      "Epoch: 87 | Training Batch: 150 | Average loss: 0.2312\n",
      "Epoch: 87 | Training Batch: 160 | Average loss: 0.2301\n",
      "Epoch: 87 | Training Batch: 170 | Average loss: 0.2327\n",
      "Epoch: 87 | Training Batch: 180 | Average loss: 0.2325\n",
      "Epoch: 87 | Training Batch: 190 | Average loss: 0.2259\n",
      "Epoch: 87 | Training Batch: 200 | Average loss: 0.2272\n",
      "Epoch: 87 | Training Batch: 210 | Average loss: 0.2299\n",
      "Epoch: 87 | Training Batch: 220 | Average loss: 0.2282\n",
      "Average training batch loss at epoch 87: 0.2383\n",
      "Average validation fold accuracy at epoch 87: 0.2559\n",
      "Epoch: 88 | Training Batch: 10 | Average loss: 0.2322\n",
      "Epoch: 88 | Training Batch: 20 | Average loss: 0.2288\n",
      "Epoch: 88 | Training Batch: 30 | Average loss: 0.2280\n",
      "Epoch: 88 | Training Batch: 40 | Average loss: 0.2312\n",
      "Epoch: 88 | Training Batch: 50 | Average loss: 0.2337\n",
      "Epoch: 88 | Training Batch: 60 | Average loss: 0.2287\n",
      "Epoch: 88 | Training Batch: 70 | Average loss: 0.2290\n",
      "Epoch: 88 | Training Batch: 80 | Average loss: 0.2304\n",
      "Epoch: 88 | Training Batch: 90 | Average loss: 0.2277\n",
      "Epoch: 88 | Training Batch: 100 | Average loss: 0.2288\n",
      "Epoch: 88 | Training Batch: 110 | Average loss: 0.2352\n",
      "Epoch: 88 | Training Batch: 120 | Average loss: 0.2270\n",
      "Epoch: 88 | Training Batch: 130 | Average loss: 0.2319\n",
      "Epoch: 88 | Training Batch: 140 | Average loss: 0.2299\n",
      "Epoch: 88 | Training Batch: 150 | Average loss: 0.2300\n",
      "Epoch: 88 | Training Batch: 160 | Average loss: 0.2359\n",
      "Epoch: 88 | Training Batch: 170 | Average loss: 0.2304\n",
      "Epoch: 88 | Training Batch: 180 | Average loss: 0.2355\n",
      "Epoch: 88 | Training Batch: 190 | Average loss: 0.2367\n",
      "Epoch: 88 | Training Batch: 200 | Average loss: 0.2302\n",
      "Epoch: 88 | Training Batch: 210 | Average loss: 0.2302\n",
      "Epoch: 88 | Training Batch: 220 | Average loss: 0.2319\n",
      "Average training batch loss at epoch 88: 0.2382\n",
      "Average validation fold accuracy at epoch 88: 0.2559\n",
      "Epoch: 89 | Training Batch: 10 | Average loss: 0.2320\n",
      "Epoch: 89 | Training Batch: 20 | Average loss: 0.2287\n",
      "Epoch: 89 | Training Batch: 30 | Average loss: 0.2311\n",
      "Epoch: 89 | Training Batch: 40 | Average loss: 0.2263\n",
      "Epoch: 89 | Training Batch: 50 | Average loss: 0.2316\n",
      "Epoch: 89 | Training Batch: 60 | Average loss: 0.2324\n",
      "Epoch: 89 | Training Batch: 70 | Average loss: 0.2273\n",
      "Epoch: 89 | Training Batch: 80 | Average loss: 0.2303\n",
      "Epoch: 89 | Training Batch: 90 | Average loss: 0.2330\n",
      "Epoch: 89 | Training Batch: 100 | Average loss: 0.2319\n",
      "Epoch: 89 | Training Batch: 110 | Average loss: 0.2274\n",
      "Epoch: 89 | Training Batch: 120 | Average loss: 0.2362\n",
      "Epoch: 89 | Training Batch: 130 | Average loss: 0.2319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89 | Training Batch: 140 | Average loss: 0.2324\n",
      "Epoch: 89 | Training Batch: 150 | Average loss: 0.2309\n",
      "Epoch: 89 | Training Batch: 160 | Average loss: 0.2315\n",
      "Epoch: 89 | Training Batch: 170 | Average loss: 0.2323\n",
      "Epoch: 89 | Training Batch: 180 | Average loss: 0.2296\n",
      "Epoch: 89 | Training Batch: 190 | Average loss: 0.2248\n",
      "Epoch: 89 | Training Batch: 200 | Average loss: 0.2333\n",
      "Epoch: 89 | Training Batch: 210 | Average loss: 0.2328\n",
      "Epoch: 89 | Training Batch: 220 | Average loss: 0.2338\n",
      "Average training batch loss at epoch 89: 0.2382\n",
      "Average validation fold accuracy at epoch 89: 0.2559\n",
      "Epoch: 90 | Training Batch: 10 | Average loss: 0.2362\n",
      "Epoch: 90 | Training Batch: 20 | Average loss: 0.2317\n",
      "Epoch: 90 | Training Batch: 30 | Average loss: 0.2307\n",
      "Epoch: 90 | Training Batch: 40 | Average loss: 0.2294\n",
      "Epoch: 90 | Training Batch: 50 | Average loss: 0.2273\n",
      "Epoch: 90 | Training Batch: 60 | Average loss: 0.2318\n",
      "Epoch: 90 | Training Batch: 70 | Average loss: 0.2282\n",
      "Epoch: 90 | Training Batch: 80 | Average loss: 0.2325\n",
      "Epoch: 90 | Training Batch: 90 | Average loss: 0.2306\n",
      "Epoch: 90 | Training Batch: 100 | Average loss: 0.2317\n",
      "Epoch: 90 | Training Batch: 110 | Average loss: 0.2308\n",
      "Epoch: 90 | Training Batch: 120 | Average loss: 0.2378\n",
      "Epoch: 90 | Training Batch: 130 | Average loss: 0.2309\n",
      "Epoch: 90 | Training Batch: 140 | Average loss: 0.2276\n",
      "Epoch: 90 | Training Batch: 150 | Average loss: 0.2325\n",
      "Epoch: 90 | Training Batch: 160 | Average loss: 0.2304\n",
      "Epoch: 90 | Training Batch: 170 | Average loss: 0.2262\n",
      "Epoch: 90 | Training Batch: 180 | Average loss: 0.2299\n",
      "Epoch: 90 | Training Batch: 190 | Average loss: 0.2347\n",
      "Epoch: 90 | Training Batch: 200 | Average loss: 0.2310\n",
      "Epoch: 90 | Training Batch: 210 | Average loss: 0.2274\n",
      "Epoch: 90 | Training Batch: 220 | Average loss: 0.2294\n",
      "Average training batch loss at epoch 90: 0.2381\n",
      "Average validation fold accuracy at epoch 90: 0.2559\n",
      "Epoch: 91 | Training Batch: 10 | Average loss: 0.2307\n",
      "Epoch: 91 | Training Batch: 20 | Average loss: 0.2326\n",
      "Epoch: 91 | Training Batch: 30 | Average loss: 0.2320\n",
      "Epoch: 91 | Training Batch: 40 | Average loss: 0.2332\n",
      "Epoch: 91 | Training Batch: 50 | Average loss: 0.2273\n",
      "Epoch: 91 | Training Batch: 60 | Average loss: 0.2303\n",
      "Epoch: 91 | Training Batch: 70 | Average loss: 0.2319\n",
      "Epoch: 91 | Training Batch: 80 | Average loss: 0.2337\n",
      "Epoch: 91 | Training Batch: 90 | Average loss: 0.2295\n",
      "Epoch: 91 | Training Batch: 100 | Average loss: 0.2265\n",
      "Epoch: 91 | Training Batch: 110 | Average loss: 0.2279\n",
      "Epoch: 91 | Training Batch: 120 | Average loss: 0.2305\n",
      "Epoch: 91 | Training Batch: 130 | Average loss: 0.2292\n",
      "Epoch: 91 | Training Batch: 140 | Average loss: 0.2295\n",
      "Epoch: 91 | Training Batch: 150 | Average loss: 0.2283\n",
      "Epoch: 91 | Training Batch: 160 | Average loss: 0.2322\n",
      "Epoch: 91 | Training Batch: 170 | Average loss: 0.2312\n",
      "Epoch: 91 | Training Batch: 180 | Average loss: 0.2341\n",
      "Epoch: 91 | Training Batch: 190 | Average loss: 0.2328\n",
      "Epoch: 91 | Training Batch: 200 | Average loss: 0.2301\n",
      "Epoch: 91 | Training Batch: 210 | Average loss: 0.2313\n",
      "Epoch: 91 | Training Batch: 220 | Average loss: 0.2352\n",
      "Average training batch loss at epoch 91: 0.2380\n",
      "Average validation fold accuracy at epoch 91: 0.2558\n",
      "Epoch: 92 | Training Batch: 10 | Average loss: 0.2352\n",
      "Epoch: 92 | Training Batch: 20 | Average loss: 0.2291\n",
      "Epoch: 92 | Training Batch: 30 | Average loss: 0.2288\n",
      "Epoch: 92 | Training Batch: 40 | Average loss: 0.2283\n",
      "Epoch: 92 | Training Batch: 50 | Average loss: 0.2317\n",
      "Epoch: 92 | Training Batch: 60 | Average loss: 0.2317\n",
      "Epoch: 92 | Training Batch: 70 | Average loss: 0.2327\n",
      "Epoch: 92 | Training Batch: 80 | Average loss: 0.2300\n",
      "Epoch: 92 | Training Batch: 90 | Average loss: 0.2302\n",
      "Epoch: 92 | Training Batch: 100 | Average loss: 0.2341\n",
      "Epoch: 92 | Training Batch: 110 | Average loss: 0.2325\n",
      "Epoch: 92 | Training Batch: 120 | Average loss: 0.2318\n",
      "Epoch: 92 | Training Batch: 130 | Average loss: 0.2289\n",
      "Epoch: 92 | Training Batch: 140 | Average loss: 0.2296\n",
      "Epoch: 92 | Training Batch: 150 | Average loss: 0.2329\n",
      "Epoch: 92 | Training Batch: 160 | Average loss: 0.2298\n",
      "Epoch: 92 | Training Batch: 170 | Average loss: 0.2274\n",
      "Epoch: 92 | Training Batch: 180 | Average loss: 0.2343\n",
      "Epoch: 92 | Training Batch: 190 | Average loss: 0.2274\n",
      "Epoch: 92 | Training Batch: 200 | Average loss: 0.2290\n",
      "Epoch: 92 | Training Batch: 210 | Average loss: 0.2351\n",
      "Epoch: 92 | Training Batch: 220 | Average loss: 0.2302\n",
      "Average training batch loss at epoch 92: 0.2379\n",
      "Average validation fold accuracy at epoch 92: 0.2558\n",
      "Epoch: 93 | Training Batch: 10 | Average loss: 0.2294\n",
      "Epoch: 93 | Training Batch: 20 | Average loss: 0.2312\n",
      "Epoch: 93 | Training Batch: 30 | Average loss: 0.2265\n",
      "Epoch: 93 | Training Batch: 40 | Average loss: 0.2336\n",
      "Epoch: 93 | Training Batch: 50 | Average loss: 0.2314\n",
      "Epoch: 93 | Training Batch: 60 | Average loss: 0.2324\n",
      "Epoch: 93 | Training Batch: 70 | Average loss: 0.2330\n",
      "Epoch: 93 | Training Batch: 80 | Average loss: 0.2274\n",
      "Epoch: 93 | Training Batch: 90 | Average loss: 0.2326\n",
      "Epoch: 93 | Training Batch: 100 | Average loss: 0.2285\n",
      "Epoch: 93 | Training Batch: 110 | Average loss: 0.2317\n",
      "Epoch: 93 | Training Batch: 120 | Average loss: 0.2311\n",
      "Epoch: 93 | Training Batch: 130 | Average loss: 0.2295\n",
      "Epoch: 93 | Training Batch: 140 | Average loss: 0.2294\n",
      "Epoch: 93 | Training Batch: 150 | Average loss: 0.2342\n",
      "Epoch: 93 | Training Batch: 160 | Average loss: 0.2307\n",
      "Epoch: 93 | Training Batch: 170 | Average loss: 0.2301\n",
      "Epoch: 93 | Training Batch: 180 | Average loss: 0.2309\n",
      "Epoch: 93 | Training Batch: 190 | Average loss: 0.2292\n",
      "Epoch: 93 | Training Batch: 200 | Average loss: 0.2325\n",
      "Epoch: 93 | Training Batch: 210 | Average loss: 0.2343\n",
      "Epoch: 93 | Training Batch: 220 | Average loss: 0.2311\n",
      "Average training batch loss at epoch 93: 0.2379\n",
      "Average validation fold accuracy at epoch 93: 0.2558\n",
      "Epoch: 94 | Training Batch: 10 | Average loss: 0.2277\n",
      "Epoch: 94 | Training Batch: 20 | Average loss: 0.2285\n",
      "Epoch: 94 | Training Batch: 30 | Average loss: 0.2333\n",
      "Epoch: 94 | Training Batch: 40 | Average loss: 0.2304\n",
      "Epoch: 94 | Training Batch: 50 | Average loss: 0.2334\n",
      "Epoch: 94 | Training Batch: 60 | Average loss: 0.2278\n",
      "Epoch: 94 | Training Batch: 70 | Average loss: 0.2327\n",
      "Epoch: 94 | Training Batch: 80 | Average loss: 0.2298\n",
      "Epoch: 94 | Training Batch: 90 | Average loss: 0.2295\n",
      "Epoch: 94 | Training Batch: 100 | Average loss: 0.2296\n",
      "Epoch: 94 | Training Batch: 110 | Average loss: 0.2319\n",
      "Epoch: 94 | Training Batch: 120 | Average loss: 0.2291\n",
      "Epoch: 94 | Training Batch: 130 | Average loss: 0.2322\n",
      "Epoch: 94 | Training Batch: 140 | Average loss: 0.2307\n",
      "Epoch: 94 | Training Batch: 150 | Average loss: 0.2335\n",
      "Epoch: 94 | Training Batch: 160 | Average loss: 0.2313\n",
      "Epoch: 94 | Training Batch: 170 | Average loss: 0.2321\n",
      "Epoch: 94 | Training Batch: 180 | Average loss: 0.2280\n",
      "Epoch: 94 | Training Batch: 190 | Average loss: 0.2342\n",
      "Epoch: 94 | Training Batch: 200 | Average loss: 0.2310\n",
      "Epoch: 94 | Training Batch: 210 | Average loss: 0.2324\n",
      "Epoch: 94 | Training Batch: 220 | Average loss: 0.2313\n",
      "Average training batch loss at epoch 94: 0.2378\n",
      "Average validation fold accuracy at epoch 94: 0.2558\n",
      "Epoch: 95 | Training Batch: 10 | Average loss: 0.2349\n",
      "Epoch: 95 | Training Batch: 20 | Average loss: 0.2307\n",
      "Epoch: 95 | Training Batch: 30 | Average loss: 0.2276\n",
      "Epoch: 95 | Training Batch: 40 | Average loss: 0.2298\n",
      "Epoch: 95 | Training Batch: 50 | Average loss: 0.2293\n",
      "Epoch: 95 | Training Batch: 60 | Average loss: 0.2292\n",
      "Epoch: 95 | Training Batch: 70 | Average loss: 0.2327\n",
      "Epoch: 95 | Training Batch: 80 | Average loss: 0.2328\n",
      "Epoch: 95 | Training Batch: 90 | Average loss: 0.2300\n",
      "Epoch: 95 | Training Batch: 100 | Average loss: 0.2259\n",
      "Epoch: 95 | Training Batch: 110 | Average loss: 0.2306\n",
      "Epoch: 95 | Training Batch: 120 | Average loss: 0.2316\n",
      "Epoch: 95 | Training Batch: 130 | Average loss: 0.2293\n",
      "Epoch: 95 | Training Batch: 140 | Average loss: 0.2352\n",
      "Epoch: 95 | Training Batch: 150 | Average loss: 0.2307\n",
      "Epoch: 95 | Training Batch: 160 | Average loss: 0.2288\n",
      "Epoch: 95 | Training Batch: 170 | Average loss: 0.2286\n",
      "Epoch: 95 | Training Batch: 180 | Average loss: 0.2339\n",
      "Epoch: 95 | Training Batch: 190 | Average loss: 0.2353\n",
      "Epoch: 95 | Training Batch: 200 | Average loss: 0.2310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95 | Training Batch: 210 | Average loss: 0.2266\n",
      "Epoch: 95 | Training Batch: 220 | Average loss: 0.2334\n",
      "Average training batch loss at epoch 95: 0.2377\n",
      "Average validation fold accuracy at epoch 95: 0.2558\n",
      "Epoch: 96 | Training Batch: 10 | Average loss: 0.2329\n",
      "Epoch: 96 | Training Batch: 20 | Average loss: 0.2300\n",
      "Epoch: 96 | Training Batch: 30 | Average loss: 0.2323\n",
      "Epoch: 96 | Training Batch: 40 | Average loss: 0.2289\n",
      "Epoch: 96 | Training Batch: 50 | Average loss: 0.2336\n",
      "Epoch: 96 | Training Batch: 60 | Average loss: 0.2294\n",
      "Epoch: 96 | Training Batch: 70 | Average loss: 0.2268\n",
      "Epoch: 96 | Training Batch: 80 | Average loss: 0.2304\n",
      "Epoch: 96 | Training Batch: 90 | Average loss: 0.2329\n",
      "Epoch: 96 | Training Batch: 100 | Average loss: 0.2312\n",
      "Epoch: 96 | Training Batch: 110 | Average loss: 0.2306\n",
      "Epoch: 96 | Training Batch: 120 | Average loss: 0.2339\n",
      "Epoch: 96 | Training Batch: 130 | Average loss: 0.2291\n",
      "Epoch: 96 | Training Batch: 140 | Average loss: 0.2278\n",
      "Epoch: 96 | Training Batch: 150 | Average loss: 0.2296\n",
      "Epoch: 96 | Training Batch: 160 | Average loss: 0.2301\n",
      "Epoch: 96 | Training Batch: 170 | Average loss: 0.2327\n",
      "Epoch: 96 | Training Batch: 180 | Average loss: 0.2292\n",
      "Epoch: 96 | Training Batch: 190 | Average loss: 0.2281\n",
      "Epoch: 96 | Training Batch: 200 | Average loss: 0.2332\n",
      "Epoch: 96 | Training Batch: 210 | Average loss: 0.2328\n",
      "Epoch: 96 | Training Batch: 220 | Average loss: 0.2326\n",
      "Average training batch loss at epoch 96: 0.2376\n",
      "Average validation fold accuracy at epoch 96: 0.2557\n",
      "Epoch: 97 | Training Batch: 10 | Average loss: 0.2307\n",
      "Epoch: 97 | Training Batch: 20 | Average loss: 0.2340\n",
      "Epoch: 97 | Training Batch: 30 | Average loss: 0.2240\n",
      "Epoch: 97 | Training Batch: 40 | Average loss: 0.2329\n",
      "Epoch: 97 | Training Batch: 50 | Average loss: 0.2350\n",
      "Epoch: 97 | Training Batch: 60 | Average loss: 0.2300\n",
      "Epoch: 97 | Training Batch: 70 | Average loss: 0.2322\n",
      "Epoch: 97 | Training Batch: 80 | Average loss: 0.2272\n",
      "Epoch: 97 | Training Batch: 90 | Average loss: 0.2328\n",
      "Epoch: 97 | Training Batch: 100 | Average loss: 0.2297\n",
      "Epoch: 97 | Training Batch: 110 | Average loss: 0.2288\n",
      "Epoch: 97 | Training Batch: 120 | Average loss: 0.2317\n",
      "Epoch: 97 | Training Batch: 130 | Average loss: 0.2295\n",
      "Epoch: 97 | Training Batch: 140 | Average loss: 0.2313\n",
      "Epoch: 97 | Training Batch: 150 | Average loss: 0.2324\n",
      "Epoch: 97 | Training Batch: 160 | Average loss: 0.2325\n",
      "Epoch: 97 | Training Batch: 170 | Average loss: 0.2320\n",
      "Epoch: 97 | Training Batch: 180 | Average loss: 0.2297\n",
      "Epoch: 97 | Training Batch: 190 | Average loss: 0.2274\n",
      "Epoch: 97 | Training Batch: 200 | Average loss: 0.2314\n",
      "Epoch: 97 | Training Batch: 210 | Average loss: 0.2299\n",
      "Epoch: 97 | Training Batch: 220 | Average loss: 0.2307\n",
      "Average training batch loss at epoch 97: 0.2376\n",
      "Average validation fold accuracy at epoch 97: 0.2557\n",
      "Epoch: 98 | Training Batch: 10 | Average loss: 0.2253\n",
      "Epoch: 98 | Training Batch: 20 | Average loss: 0.2231\n",
      "Epoch: 98 | Training Batch: 30 | Average loss: 0.2296\n",
      "Epoch: 98 | Training Batch: 40 | Average loss: 0.2309\n",
      "Epoch: 98 | Training Batch: 50 | Average loss: 0.2316\n",
      "Epoch: 98 | Training Batch: 60 | Average loss: 0.2356\n",
      "Epoch: 98 | Training Batch: 70 | Average loss: 0.2270\n",
      "Epoch: 98 | Training Batch: 80 | Average loss: 0.2283\n",
      "Epoch: 98 | Training Batch: 90 | Average loss: 0.2380\n",
      "Epoch: 98 | Training Batch: 100 | Average loss: 0.2270\n",
      "Epoch: 98 | Training Batch: 110 | Average loss: 0.2353\n",
      "Epoch: 98 | Training Batch: 120 | Average loss: 0.2332\n",
      "Epoch: 98 | Training Batch: 130 | Average loss: 0.2372\n",
      "Epoch: 98 | Training Batch: 140 | Average loss: 0.2329\n",
      "Epoch: 98 | Training Batch: 150 | Average loss: 0.2288\n",
      "Epoch: 98 | Training Batch: 160 | Average loss: 0.2311\n",
      "Epoch: 98 | Training Batch: 170 | Average loss: 0.2309\n",
      "Epoch: 98 | Training Batch: 180 | Average loss: 0.2319\n",
      "Epoch: 98 | Training Batch: 190 | Average loss: 0.2277\n",
      "Epoch: 98 | Training Batch: 200 | Average loss: 0.2306\n",
      "Epoch: 98 | Training Batch: 210 | Average loss: 0.2328\n",
      "Epoch: 98 | Training Batch: 220 | Average loss: 0.2296\n",
      "Average training batch loss at epoch 98: 0.2375\n",
      "Average validation fold accuracy at epoch 98: 0.2557\n",
      "Epoch: 99 | Training Batch: 10 | Average loss: 0.2291\n",
      "Epoch: 99 | Training Batch: 20 | Average loss: 0.2288\n",
      "Epoch: 99 | Training Batch: 30 | Average loss: 0.2326\n",
      "Epoch: 99 | Training Batch: 40 | Average loss: 0.2344\n",
      "Epoch: 99 | Training Batch: 50 | Average loss: 0.2329\n",
      "Epoch: 99 | Training Batch: 60 | Average loss: 0.2339\n",
      "Epoch: 99 | Training Batch: 70 | Average loss: 0.2297\n",
      "Epoch: 99 | Training Batch: 80 | Average loss: 0.2339\n",
      "Epoch: 99 | Training Batch: 90 | Average loss: 0.2299\n",
      "Epoch: 99 | Training Batch: 100 | Average loss: 0.2328\n",
      "Epoch: 99 | Training Batch: 110 | Average loss: 0.2298\n",
      "Epoch: 99 | Training Batch: 120 | Average loss: 0.2322\n",
      "Epoch: 99 | Training Batch: 130 | Average loss: 0.2309\n",
      "Epoch: 99 | Training Batch: 140 | Average loss: 0.2323\n",
      "Epoch: 99 | Training Batch: 150 | Average loss: 0.2257\n",
      "Epoch: 99 | Training Batch: 160 | Average loss: 0.2347\n",
      "Epoch: 99 | Training Batch: 170 | Average loss: 0.2318\n",
      "Epoch: 99 | Training Batch: 180 | Average loss: 0.2266\n",
      "Epoch: 99 | Training Batch: 190 | Average loss: 0.2289\n",
      "Epoch: 99 | Training Batch: 200 | Average loss: 0.2326\n",
      "Epoch: 99 | Training Batch: 210 | Average loss: 0.2279\n",
      "Epoch: 99 | Training Batch: 220 | Average loss: 0.2267\n",
      "Average training batch loss at epoch 99: 0.2374\n",
      "Average validation fold accuracy at epoch 99: 0.2557\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "pretrain=False\n",
    "\n",
    "opt = {\n",
    "    'node_size':201,\n",
    "    'hidden_size':30,\n",
    "    'num_layers':1,\n",
    "    'embedding_dim':50,\n",
    "    'learning_rate':0.0001,#10**np.random.uniform(-5, -3)\n",
    "#     'learning_rate':10**np.random.uniform(-5, -3),\n",
    "    'bidirection_lstm':False,\n",
    "    'seqlen':50,\n",
    "    'batch_size':64\n",
    "}\n",
    "print('learning_rate: ',opt['learning_rate'])\n",
    "\n",
    "batch_size = opt['batch_size']\n",
    "# Initialize global tracking variables\n",
    "best_validation_accuracy = 0\n",
    "epochs_without_improvement = 0\n",
    "total_train_loss = list()\n",
    "total_valid_loss = []\n",
    "avg_trainings = []\n",
    "avg_valids = []\n",
    "\n",
    "# Loading model\n",
    "if pretrain:\n",
    "    classifier = torch.load('SiameseNN1.pt')\n",
    "else:\n",
    "    classifier = SiameseClassifier(opt, is_train=True)\n",
    "    # Initialize parameters\n",
    "    classifier.initialize_parameters()\n",
    "\n",
    "#Learning rate decay\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(classifier.optimizer_a, milestones=[20,40,60,80], gamma=0.5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    scheduler.step()\n",
    "    # Initiate the training data loader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
    "    running_loss = list()\n",
    "    # Training loop\n",
    "    for i, (batch_x,label_var) in enumerate(train_loader):\n",
    "        s1_var = batch_x[:,0,:]\n",
    "        s2_var  = batch_x[:,1,:]\n",
    "        #s1_var = one_hot(s1_var)\n",
    "        #s2_var = one_hot(s2_var)\n",
    "        classifier.train_step(s1_var, s2_var, label_var)\n",
    "        train_batch_loss = classifier.loss.data[0]\n",
    "        running_loss.append(train_batch_loss)\n",
    "        total_train_loss.append(train_batch_loss)\n",
    "\n",
    "        if i % 10 == 0 and i != 0:\n",
    "            running_avg_loss = sum(running_loss) / len(running_loss)\n",
    "            print('Epoch: %d | Training Batch: %d | Average loss: %.4f' %\n",
    "                  (epoch, i , running_avg_loss))\n",
    "            running_loss = []\n",
    "            \n",
    "\n",
    "    # Report epoch statistics\n",
    "    avg_training_accuracy = sum(total_train_loss) / len(total_train_loss)\n",
    "    print('Average training batch loss at epoch %d: %.4f' % (epoch, avg_training_accuracy))\n",
    "    avg_trainings.append(avg_training_accuracy) \n",
    "    \n",
    "\n",
    "    # Validate after each epoch; set tracking variables\n",
    "    if epoch >= 0:\n",
    "        # Initiate the training data loader\n",
    "        valid_loader = DataLoader(val_dataset, batch_size=32,shuffle=True)\n",
    "        \n",
    "        # Validation loop (i.e. perform inference on the validation set)\n",
    "        for i, (batch_x,label_var) in enumerate(valid_loader):\n",
    "            s1_var = batch_x[:,0,:]\n",
    "            s2_var  = batch_x[:,1,:]\n",
    "            #s1_var = one_hot(s1_var)\n",
    "            #s2_var = one_hot(s2_var)\n",
    "            # Get predictions and update tracking values\n",
    "            classifier.test_step(s1_var, s2_var, label_var)\n",
    "            valid_batch_loss = classifier.loss.data[0]\n",
    "            total_valid_loss.append(valid_batch_loss)\n",
    "\n",
    "        # Report fold statistics\n",
    "        avg_valid_accuracy = sum(total_valid_loss) / len(total_valid_loss)\n",
    "        print('Average validation fold accuracy at epoch %d: %.4f' % (epoch, avg_valid_accuracy))\n",
    "        avg_valids.append(avg_valid_accuracy)\n",
    "        # Save network parameters if performance has improved\n",
    "        if avg_valid_accuracy <= best_validation_accuracy:\n",
    "            epochs_without_improvement += 1\n",
    "        else:\n",
    "            best_validation_accuracy = avg_valid_accuracy\n",
    "            epochs_without_improvement = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize= (20,5))\n",
    "ax.plot(avg_valids,marker='o',label='Val')\n",
    "ax.plot(avg_trainings,marker='o',label='Train')\n",
    "ax.set_xlabel('Loss',fontsize=15)\n",
    "ax.set_ylabel('Number of epochs',fontsize=15)\n",
    "ax.set_title('Loss',fontsize=20,fontweight =\"bold\")\n",
    "ax.legend()\n",
    "graph_path = \"./Pytorch_Results/FullDataOnlyLSTM1lr\" + str(opt['learning_rate']) +'Decay.png'\n",
    "fig.savefig(graph_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richard/anaconda3/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/richard/anaconda3/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type LSTMEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./Pytorch_Model/FullDataOnlyLSTM1lr\" + str(opt['learning_rate']) +'Decay.pt'\n",
    "torch.save(classifier, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['embedding_table.weight', 'lstm_rnn.weight_ih_l0', 'lstm_rnn.weight_hh_l0', 'lstm_rnn.bias_ih_l0', 'lstm_rnn.bias_hh_l0', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.encoder_a.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters in this model:  20420\n"
     ]
    }
   ],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "print(\"number of parameters in this model: \",get_n_params(classifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
