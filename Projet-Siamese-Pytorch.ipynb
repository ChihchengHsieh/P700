{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# Load data\n",
    "with open(\"data.txt\", \"rb\") as fp:   # Unpickling\n",
    "    df = pickle.load(fp)\n",
    "X = df[['left','right']]     \n",
    "Y = df['target']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperate to training, validation, and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 101)\n",
    "validation_size = int(len(X_train) * 0.1)\n",
    "training_size = len(X_train) - validation_size\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=validation_size,random_state= 101)\n",
    "Y_test = Y_test.values\n",
    "Y_train = Y_train.values\n",
    "Y_validation = Y_validation.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 1440\n",
      "Validation size: 160\n",
      "test size: 400\n"
     ]
    }
   ],
   "source": [
    "print('Training size:',X_train.shape[0])\n",
    "print('Validation size:',X_validation.shape[0])\n",
    "print('test size:',X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check shape\n",
    "assert X_train['left'].shape == X_train['right'].shape\n",
    "assert len(X_train['left']) == len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two help function\n",
    "def one_hot(s):\n",
    "    nb_digits=201\n",
    "    batch_size = s.shape[0]\n",
    "    seqlen = s.shape[1]\n",
    "    s_onehot = torch.FloatTensor(batch_size,seqlen,nb_digits)\n",
    "    s_onehot.zero_()\n",
    "    s_onehot.scatter_(2, s.unsqueeze(2), 1)\n",
    "    return s_onehot\n",
    "def padding(data):\n",
    "    left = [] \n",
    "    maxlen= 50\n",
    "    for i in range(data.shape[0]):\n",
    "        diff = maxlen - len((data.iloc[i]['left']))\n",
    "        if diff>=1:\n",
    "            data.iloc[i]['left']+= [0]*diff\n",
    "        left.append((data.iloc[i]['left']))\n",
    "    right = [] \n",
    "    maxlen= 50\n",
    "    for i in range(data.shape[0]):\n",
    "        diff = maxlen - len((data.iloc[i]['right']))\n",
    "        if diff>=1:\n",
    "            data.iloc[i]['right']+= [0]*diff\n",
    "        right.append((data.iloc[i]['right']))\n",
    "    return torch.tensor(np.array([right,left])).transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding and creat the loaders\n",
    "X_train = padding(X_train)\n",
    "Y_train = torch.FloatTensor(np.array(Y_train))\n",
    "train_dataset  = Data.TensorDataset(X_train,Y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32,shuffle=True)\n",
    "\n",
    "X_validation = padding(X_validation)\n",
    "Y_validation = torch.FloatTensor(np.array(Y_validation))\n",
    "val_dataset  = Data.TensorDataset(X_validation,Y_validation)\n",
    "\n",
    "X_test = padding(X_test)\n",
    "Y_test = torch.FloatTensor(np.array(Y_test))\n",
    "test_dataset  = Data.TensorDataset(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    \"\"\" Implements the network type integrated within the Siamese RNN architecture. \"\"\"\n",
    "    def __init__(self, opt, is_train=False):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.node_size = opt['node_size']\n",
    "        self.name = 'sim_encoder'\n",
    "        self.hidden_size= opt['hidden_size']\n",
    "        self.num_layers= opt['num_layers']\n",
    "        self.embedding_dim = opt['embedding_dim']\n",
    "        self.embedding_table = nn.Embedding(num_embeddings=self.node_size, embedding_dim=self.embedding_dim,\n",
    "                                          padding_idx=0, max_norm=None, scale_grad_by_freq=False, sparse=False)\n",
    "        self.lstm_rnn = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.hidden_size, num_layers=self.num_layers)\n",
    "        self.fc1= nn.Linear(self.hidden_size,10)\n",
    "        self.fc2= nn.Linear(10,2)\n",
    "    def initialize_hidden_plus_cell(self, batch_size):\n",
    "        \"\"\" Re-initializes the hidden state, cell state, and the forget gate bias of the network. \"\"\"\n",
    "        zero_hidden = torch.randn(self.num_layers, batch_size, self.hidden_size)\n",
    "        zero_cell = torch.randn(self.num_layers, batch_size,self.hidden_size)\n",
    "        return zero_hidden, zero_cell\n",
    "\n",
    "    def forward(self, input_data, hidden, cell):\n",
    "        \"\"\" Performs a forward pass through the network. \"\"\"\n",
    "        output = self.embedding_table(input_data)\n",
    "        output, (hidden, cell) = self.lstm_rnn(output, (hidden, cell))\n",
    "        output = nn.functional.relu(self.fc1(output[-1]))\n",
    "        output = self.fc2(output) \n",
    "        return output, hidden[-1], cell[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseClassifier(nn.Module):\n",
    "    \"\"\" Sentence similarity estimator implementing a siamese arcitecture. Uses pretrained word2vec embeddings. \n",
    "    Different to the paper, the weights are untied, to avoid exploding/ vanishing gradients. \"\"\"\n",
    "    def __init__(self, opt, is_train=False):\n",
    "        super(SiameseClassifier, self).__init__()\n",
    "        self.learning_rate= opt['learning_rate']\n",
    "        # Initialize network\n",
    "        self.encoder_a =  LSTMEncoder(opt, is_train)\n",
    "        # Initialize network parameters\n",
    "        self.initialize_parameters()\n",
    "        # Declare loss function\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        # Initialize network optimizers\n",
    "        self.optimizer_a = optim.Adam(self.encoder_a.parameters(), lr=self.learning_rate,\n",
    "                                      betas=(0.9, 0.999),weight_decay=0)\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\" Performs a single forward pass through the siamese architecture. \"\"\"\n",
    "        \n",
    "        # Obtain the input length (each batch consists of padded sentences)\n",
    "        input_length = self.batch_a.size(0)\n",
    "        \n",
    "        # Obtain sentence encodings from each encoder\n",
    "        hidden_a, cell_a = self.encoder_a.initialize_hidden_plus_cell(self.batch_size)\n",
    "        output_a, hidden_a, cell_a = self.encoder_a(self.batch_a, hidden_a, cell_a)\n",
    "\n",
    "        hidden_b, cell_b = self.encoder_a.initialize_hidden_plus_cell(self.batch_size)\n",
    "        output_b, hidden_b, cell_b = self.encoder_a(self.batch_b, hidden_b, cell_b)\n",
    "\n",
    "        # Format sentence encodings as 2D tensors\n",
    "        self.encoding_a = output_a.squeeze()\n",
    "        self.encoding_b = output_b.squeeze()\n",
    "\n",
    "        # Obtain similarity score predictions by calculating the Manhattan distance between sentence encodings\n",
    "        if self.batch_size == 1:\n",
    "            self.prediction = torch.exp(-torch.norm((self.encoding_a - self.encoding_b), 1))\n",
    "        else:\n",
    "            self.prediction = torch.exp(-torch.norm((self.encoding_a - self.encoding_b), 1, 1))\n",
    "            \n",
    "\n",
    "    def get_loss(self):\n",
    "        \"\"\" Calculates the MSE loss between the network predictions and the ground truth. \"\"\"\n",
    "        # Loss is the L1 norm of the difference between the obtained sentence encodings\n",
    "        self.loss = self.loss_function(self.prediction, self.labels)\n",
    "\n",
    "    def load_pretrained_parameters(self,pretrained_state_dict_path):\n",
    "        \"\"\" Loads the parameters learned during the pre-training on the SemEval data. \"\"\"\n",
    "        self.encoder_a.load_state_dict(torch.load(pretrained_state_dict_path))\n",
    "        print('Pretrained parameters have been successfully loaded into the encoder networks.')\n",
    "    \n",
    "    def save_lstm(self,path):\n",
    "        torch.save(self.encoder_a.state_dict(), path)\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\" Initializes network parameters. \"\"\"\n",
    "        state_dict = self.encoder_a.state_dict()\n",
    "        for key in state_dict.keys():\n",
    "            if '.weight' in key:\n",
    "                state_dict[key] = torch.nn.init.xavier_uniform_((state_dict[key]),gain=1)\n",
    "            if '.bias' in key:\n",
    "                bias_length = state_dict[key].size()[0]\n",
    "                start, end = bias_length // 4, bias_length // 2\n",
    "                state_dict[key][start:end].fill_(2.5)\n",
    "        self.encoder_a.load_state_dict(state_dict)\n",
    "\n",
    "    def train_step(self, train_batch_a, train_batch_b, train_labels):\n",
    "        \"\"\" Optimizes the parameters of the active networks, i.e. performs a single training step. \"\"\"\n",
    "        # Get batches\n",
    "        self.batch_a = train_batch_a.transpose(0,1)\n",
    "        self.batch_b = train_batch_b.transpose(0,1)\n",
    "        self.labels = train_labels\n",
    "        self.batch_size = self.batch_a.size(1)\n",
    "        self.encoder_a.zero_grad() \n",
    "        self.forward()\n",
    "        self.get_loss()\n",
    "        self.loss.backward()\n",
    "        clip_grad_norm(self.encoder_a.parameters(), 0.25)\n",
    "        self.optimizer_a.step()\n",
    "\n",
    "    def test_step(self, test_batch_a, test_batch_b, test_labels):\n",
    "        \"\"\" Performs a single test step. \"\"\"\n",
    "        self.batch_a = test_batch_a.transpose(0,1)\n",
    "        self.batch_b = test_batch_b.transpose(0,1)\n",
    "        self.labels = test_labels\n",
    "        self.batch_size = self.batch_a.size(1)\n",
    "        self.forward()\n",
    "        self.get_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richard/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:78: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/Users/richard/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training Batch: 10 | Average loss: 0.2949\n",
      "Epoch: 0 | Training Batch: 20 | Average loss: 0.2878\n",
      "Average training batch loss at epoch 0: 0.2945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richard/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:72: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation fold accuracy at epoch 0: 0.2747\n",
      "Epoch: 1 | Training Batch: 10 | Average loss: 0.2845\n",
      "Epoch: 1 | Training Batch: 20 | Average loss: 0.2793\n",
      "Average training batch loss at epoch 1: 0.2887\n",
      "Average validation fold accuracy at epoch 1: 0.2820\n",
      "Epoch: 2 | Training Batch: 10 | Average loss: 0.2945\n",
      "Epoch: 2 | Training Batch: 20 | Average loss: 0.2838\n",
      "Average training batch loss at epoch 2: 0.2900\n",
      "Average validation fold accuracy at epoch 2: 0.2762\n",
      "Epoch: 3 | Training Batch: 10 | Average loss: 0.2848\n",
      "Epoch: 3 | Training Batch: 20 | Average loss: 0.2871\n",
      "Average training batch loss at epoch 3: 0.2891\n",
      "Average validation fold accuracy at epoch 3: 0.2872\n",
      "Epoch: 4 | Training Batch: 10 | Average loss: 0.2840\n",
      "Epoch: 4 | Training Batch: 20 | Average loss: 0.2947\n",
      "Average training batch loss at epoch 4: 0.2886\n",
      "Average validation fold accuracy at epoch 4: 0.2850\n",
      "Epoch: 5 | Training Batch: 10 | Average loss: 0.2925\n",
      "Epoch: 5 | Training Batch: 20 | Average loss: 0.2926\n",
      "Average training batch loss at epoch 5: 0.2888\n",
      "Average validation fold accuracy at epoch 5: 0.2841\n",
      "Epoch: 6 | Training Batch: 10 | Average loss: 0.2819\n",
      "Epoch: 6 | Training Batch: 20 | Average loss: 0.2870\n",
      "Average training batch loss at epoch 6: 0.2878\n",
      "Average validation fold accuracy at epoch 6: 0.2818\n",
      "Epoch: 7 | Training Batch: 10 | Average loss: 0.2872\n",
      "Epoch: 7 | Training Batch: 20 | Average loss: 0.2751\n",
      "Average training batch loss at epoch 7: 0.2869\n",
      "Average validation fold accuracy at epoch 7: 0.2804\n",
      "Epoch: 8 | Training Batch: 10 | Average loss: 0.2866\n",
      "Epoch: 8 | Training Batch: 20 | Average loss: 0.2907\n",
      "Average training batch loss at epoch 8: 0.2871\n",
      "Average validation fold accuracy at epoch 8: 0.2797\n",
      "Epoch: 9 | Training Batch: 10 | Average loss: 0.2833\n",
      "Epoch: 9 | Training Batch: 20 | Average loss: 0.2964\n",
      "Average training batch loss at epoch 9: 0.2873\n",
      "Average validation fold accuracy at epoch 9: 0.2773\n",
      "Epoch: 10 | Training Batch: 10 | Average loss: 0.2921\n",
      "Epoch: 10 | Training Batch: 20 | Average loss: 0.2956\n",
      "Average training batch loss at epoch 10: 0.2880\n",
      "Average validation fold accuracy at epoch 10: 0.2803\n",
      "Epoch: 11 | Training Batch: 10 | Average loss: 0.2825\n",
      "Epoch: 11 | Training Batch: 20 | Average loss: 0.3088\n",
      "Average training batch loss at epoch 11: 0.2886\n",
      "Average validation fold accuracy at epoch 11: 0.2815\n",
      "Epoch: 12 | Training Batch: 10 | Average loss: 0.2709\n",
      "Epoch: 12 | Training Batch: 20 | Average loss: 0.2836\n",
      "Average training batch loss at epoch 12: 0.2877\n",
      "Average validation fold accuracy at epoch 12: 0.2807\n",
      "Epoch: 13 | Training Batch: 10 | Average loss: 0.2853\n",
      "Epoch: 13 | Training Batch: 20 | Average loss: 0.2683\n",
      "Average training batch loss at epoch 13: 0.2869\n",
      "Average validation fold accuracy at epoch 13: 0.2807\n",
      "Epoch: 14 | Training Batch: 10 | Average loss: 0.2761\n",
      "Epoch: 14 | Training Batch: 20 | Average loss: 0.2796\n",
      "Average training batch loss at epoch 14: 0.2864\n",
      "Average validation fold accuracy at epoch 14: 0.2784\n",
      "Epoch: 15 | Training Batch: 10 | Average loss: 0.2858\n",
      "Epoch: 15 | Training Batch: 20 | Average loss: 0.2787\n",
      "Average training batch loss at epoch 15: 0.2862\n",
      "Average validation fold accuracy at epoch 15: 0.2789\n",
      "Epoch: 16 | Training Batch: 10 | Average loss: 0.2918\n",
      "Epoch: 16 | Training Batch: 20 | Average loss: 0.2590\n",
      "Average training batch loss at epoch 16: 0.2857\n",
      "Average validation fold accuracy at epoch 16: 0.2783\n",
      "Epoch: 17 | Training Batch: 10 | Average loss: 0.2676\n",
      "Epoch: 17 | Training Batch: 20 | Average loss: 0.2785\n",
      "Average training batch loss at epoch 17: 0.2850\n",
      "Average validation fold accuracy at epoch 17: 0.2784\n",
      "Epoch: 18 | Training Batch: 10 | Average loss: 0.2644\n",
      "Epoch: 18 | Training Batch: 20 | Average loss: 0.2722\n",
      "Average training batch loss at epoch 18: 0.2841\n",
      "Average validation fold accuracy at epoch 18: 0.2781\n",
      "Epoch: 19 | Training Batch: 10 | Average loss: 0.2756\n",
      "Epoch: 19 | Training Batch: 20 | Average loss: 0.2669\n",
      "Average training batch loss at epoch 19: 0.2834\n",
      "Average validation fold accuracy at epoch 19: 0.2780\n",
      "Epoch: 20 | Training Batch: 10 | Average loss: 0.2679\n",
      "Epoch: 20 | Training Batch: 20 | Average loss: 0.2634\n",
      "Average training batch loss at epoch 20: 0.2826\n",
      "Average validation fold accuracy at epoch 20: 0.2775\n",
      "Epoch: 21 | Training Batch: 10 | Average loss: 0.2643\n",
      "Epoch: 21 | Training Batch: 20 | Average loss: 0.2794\n",
      "Average training batch loss at epoch 21: 0.2821\n",
      "Average validation fold accuracy at epoch 21: 0.2770\n",
      "Epoch: 22 | Training Batch: 10 | Average loss: 0.2617\n",
      "Epoch: 22 | Training Batch: 20 | Average loss: 0.2691\n",
      "Average training batch loss at epoch 22: 0.2815\n",
      "Average validation fold accuracy at epoch 22: 0.2769\n",
      "Epoch: 23 | Training Batch: 10 | Average loss: 0.2818\n",
      "Epoch: 23 | Training Batch: 20 | Average loss: 0.2690\n",
      "Average training batch loss at epoch 23: 0.2811\n",
      "Average validation fold accuracy at epoch 23: 0.2761\n",
      "Epoch: 24 | Training Batch: 10 | Average loss: 0.2571\n",
      "Epoch: 24 | Training Batch: 20 | Average loss: 0.2625\n",
      "Average training batch loss at epoch 24: 0.2803\n",
      "Average validation fold accuracy at epoch 24: 0.2762\n",
      "Epoch: 25 | Training Batch: 10 | Average loss: 0.2531\n",
      "Epoch: 25 | Training Batch: 20 | Average loss: 0.2931\n",
      "Average training batch loss at epoch 25: 0.2800\n",
      "Average validation fold accuracy at epoch 25: 0.2756\n",
      "Epoch: 26 | Training Batch: 10 | Average loss: 0.2903\n",
      "Epoch: 26 | Training Batch: 20 | Average loss: 0.2600\n",
      "Average training batch loss at epoch 26: 0.2798\n",
      "Average validation fold accuracy at epoch 26: 0.2748\n",
      "Epoch: 27 | Training Batch: 10 | Average loss: 0.2597\n",
      "Epoch: 27 | Training Batch: 20 | Average loss: 0.2757\n",
      "Average training batch loss at epoch 27: 0.2794\n",
      "Average validation fold accuracy at epoch 27: 0.2739\n",
      "Epoch: 28 | Training Batch: 10 | Average loss: 0.2644\n",
      "Epoch: 28 | Training Batch: 20 | Average loss: 0.2584\n",
      "Average training batch loss at epoch 28: 0.2788\n",
      "Average validation fold accuracy at epoch 28: 0.2730\n",
      "Epoch: 29 | Training Batch: 10 | Average loss: 0.2546\n",
      "Epoch: 29 | Training Batch: 20 | Average loss: 0.2563\n",
      "Average training batch loss at epoch 29: 0.2779\n",
      "Average validation fold accuracy at epoch 29: 0.2721\n",
      "Epoch: 30 | Training Batch: 10 | Average loss: 0.2583\n",
      "Epoch: 30 | Training Batch: 20 | Average loss: 0.2649\n",
      "Average training batch loss at epoch 30: 0.2773\n",
      "Average validation fold accuracy at epoch 30: 0.2719\n",
      "Epoch: 31 | Training Batch: 10 | Average loss: 0.2581\n",
      "Epoch: 31 | Training Batch: 20 | Average loss: 0.2527\n",
      "Average training batch loss at epoch 31: 0.2766\n",
      "Average validation fold accuracy at epoch 31: 0.2718\n",
      "Epoch: 32 | Training Batch: 10 | Average loss: 0.2568\n",
      "Epoch: 32 | Training Batch: 20 | Average loss: 0.2477\n",
      "Average training batch loss at epoch 32: 0.2759\n",
      "Average validation fold accuracy at epoch 32: 0.2714\n",
      "Epoch: 33 | Training Batch: 10 | Average loss: 0.2463\n",
      "Epoch: 33 | Training Batch: 20 | Average loss: 0.2442\n",
      "Average training batch loss at epoch 33: 0.2750\n",
      "Average validation fold accuracy at epoch 33: 0.2713\n",
      "Epoch: 34 | Training Batch: 10 | Average loss: 0.2539\n",
      "Epoch: 34 | Training Batch: 20 | Average loss: 0.2472\n",
      "Average training batch loss at epoch 34: 0.2744\n",
      "Average validation fold accuracy at epoch 34: 0.2712\n",
      "Epoch: 35 | Training Batch: 10 | Average loss: 0.2460\n",
      "Epoch: 35 | Training Batch: 20 | Average loss: 0.2428\n",
      "Average training batch loss at epoch 35: 0.2735\n",
      "Average validation fold accuracy at epoch 35: 0.2711\n",
      "Epoch: 36 | Training Batch: 10 | Average loss: 0.2429\n",
      "Epoch: 36 | Training Batch: 20 | Average loss: 0.2571\n",
      "Average training batch loss at epoch 36: 0.2729\n",
      "Average validation fold accuracy at epoch 36: 0.2709\n",
      "Epoch: 37 | Training Batch: 10 | Average loss: 0.2514\n",
      "Epoch: 37 | Training Batch: 20 | Average loss: 0.2453\n",
      "Average training batch loss at epoch 37: 0.2722\n",
      "Average validation fold accuracy at epoch 37: 0.2704\n",
      "Epoch: 38 | Training Batch: 10 | Average loss: 0.2445\n",
      "Epoch: 38 | Training Batch: 20 | Average loss: 0.2483\n",
      "Average training batch loss at epoch 38: 0.2715\n",
      "Average validation fold accuracy at epoch 38: 0.2707\n",
      "Epoch: 39 | Training Batch: 10 | Average loss: 0.2360\n",
      "Epoch: 39 | Training Batch: 20 | Average loss: 0.2509\n",
      "Average training batch loss at epoch 39: 0.2708\n",
      "Average validation fold accuracy at epoch 39: 0.2704\n",
      "Epoch: 40 | Training Batch: 10 | Average loss: 0.2373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 | Training Batch: 20 | Average loss: 0.2406\n",
      "Average training batch loss at epoch 40: 0.2700\n",
      "Average validation fold accuracy at epoch 40: 0.2706\n",
      "Epoch: 41 | Training Batch: 10 | Average loss: 0.2348\n",
      "Epoch: 41 | Training Batch: 20 | Average loss: 0.2430\n",
      "Average training batch loss at epoch 41: 0.2693\n",
      "Average validation fold accuracy at epoch 41: 0.2701\n",
      "Epoch: 42 | Training Batch: 10 | Average loss: 0.2459\n",
      "Epoch: 42 | Training Batch: 20 | Average loss: 0.2416\n",
      "Average training batch loss at epoch 42: 0.2687\n",
      "Average validation fold accuracy at epoch 42: 0.2695\n",
      "Epoch: 43 | Training Batch: 10 | Average loss: 0.2478\n",
      "Epoch: 43 | Training Batch: 20 | Average loss: 0.2391\n",
      "Average training batch loss at epoch 43: 0.2681\n",
      "Average validation fold accuracy at epoch 43: 0.2694\n",
      "Epoch: 44 | Training Batch: 10 | Average loss: 0.2374\n",
      "Epoch: 44 | Training Batch: 20 | Average loss: 0.2307\n",
      "Average training batch loss at epoch 44: 0.2673\n",
      "Average validation fold accuracy at epoch 44: 0.2694\n",
      "Epoch: 45 | Training Batch: 10 | Average loss: 0.2166\n",
      "Epoch: 45 | Training Batch: 20 | Average loss: 0.2412\n",
      "Average training batch loss at epoch 45: 0.2665\n",
      "Average validation fold accuracy at epoch 45: 0.2693\n",
      "Epoch: 46 | Training Batch: 10 | Average loss: 0.2297\n",
      "Epoch: 46 | Training Batch: 20 | Average loss: 0.2301\n",
      "Average training batch loss at epoch 46: 0.2657\n",
      "Average validation fold accuracy at epoch 46: 0.2693\n",
      "Epoch: 47 | Training Batch: 10 | Average loss: 0.2377\n",
      "Epoch: 47 | Training Batch: 20 | Average loss: 0.2255\n",
      "Average training batch loss at epoch 47: 0.2650\n",
      "Average validation fold accuracy at epoch 47: 0.2693\n",
      "Epoch: 48 | Training Batch: 10 | Average loss: 0.2182\n",
      "Epoch: 48 | Training Batch: 20 | Average loss: 0.2241\n",
      "Average training batch loss at epoch 48: 0.2642\n",
      "Average validation fold accuracy at epoch 48: 0.2692\n",
      "Epoch: 49 | Training Batch: 10 | Average loss: 0.2296\n",
      "Epoch: 49 | Training Batch: 20 | Average loss: 0.2310\n",
      "Average training batch loss at epoch 49: 0.2634\n",
      "Average validation fold accuracy at epoch 49: 0.2694\n",
      "Epoch: 50 | Training Batch: 10 | Average loss: 0.2135\n",
      "Epoch: 50 | Training Batch: 20 | Average loss: 0.2252\n",
      "Average training batch loss at epoch 50: 0.2626\n",
      "Average validation fold accuracy at epoch 50: 0.2698\n",
      "Epoch: 51 | Training Batch: 10 | Average loss: 0.2176\n",
      "Epoch: 51 | Training Batch: 20 | Average loss: 0.2174\n",
      "Average training batch loss at epoch 51: 0.2617\n",
      "Average validation fold accuracy at epoch 51: 0.2697\n",
      "Epoch: 52 | Training Batch: 10 | Average loss: 0.2134\n",
      "Epoch: 52 | Training Batch: 20 | Average loss: 0.2351\n",
      "Average training batch loss at epoch 52: 0.2610\n",
      "Average validation fold accuracy at epoch 52: 0.2697\n",
      "Epoch: 53 | Training Batch: 10 | Average loss: 0.2048\n",
      "Epoch: 53 | Training Batch: 20 | Average loss: 0.2198\n",
      "Average training batch loss at epoch 53: 0.2601\n",
      "Average validation fold accuracy at epoch 53: 0.2698\n",
      "Epoch: 54 | Training Batch: 10 | Average loss: 0.2003\n",
      "Epoch: 54 | Training Batch: 20 | Average loss: 0.2095\n",
      "Average training batch loss at epoch 54: 0.2591\n",
      "Average validation fold accuracy at epoch 54: 0.2699\n",
      "Epoch: 55 | Training Batch: 10 | Average loss: 0.2117\n",
      "Epoch: 55 | Training Batch: 20 | Average loss: 0.2072\n",
      "Average training batch loss at epoch 55: 0.2582\n",
      "Average validation fold accuracy at epoch 55: 0.2700\n",
      "Epoch: 56 | Training Batch: 10 | Average loss: 0.2156\n",
      "Epoch: 56 | Training Batch: 20 | Average loss: 0.2104\n",
      "Average training batch loss at epoch 56: 0.2574\n",
      "Average validation fold accuracy at epoch 56: 0.2705\n",
      "Epoch: 57 | Training Batch: 10 | Average loss: 0.2010\n",
      "Epoch: 57 | Training Batch: 20 | Average loss: 0.2125\n",
      "Average training batch loss at epoch 57: 0.2565\n",
      "Average validation fold accuracy at epoch 57: 0.2707\n",
      "Epoch: 58 | Training Batch: 10 | Average loss: 0.2068\n",
      "Epoch: 58 | Training Batch: 20 | Average loss: 0.2137\n",
      "Average training batch loss at epoch 58: 0.2558\n",
      "Average validation fold accuracy at epoch 58: 0.2708\n",
      "Epoch: 59 | Training Batch: 10 | Average loss: 0.1975\n",
      "Epoch: 59 | Training Batch: 20 | Average loss: 0.2037\n",
      "Average training batch loss at epoch 59: 0.2548\n",
      "Average validation fold accuracy at epoch 59: 0.2712\n",
      "Epoch: 60 | Training Batch: 10 | Average loss: 0.1966\n",
      "Epoch: 60 | Training Batch: 20 | Average loss: 0.2005\n",
      "Average training batch loss at epoch 60: 0.2539\n",
      "Average validation fold accuracy at epoch 60: 0.2715\n",
      "Epoch: 61 | Training Batch: 10 | Average loss: 0.1964\n",
      "Epoch: 61 | Training Batch: 20 | Average loss: 0.1930\n",
      "Average training batch loss at epoch 61: 0.2529\n",
      "Average validation fold accuracy at epoch 61: 0.2719\n",
      "Epoch: 62 | Training Batch: 10 | Average loss: 0.2012\n",
      "Epoch: 62 | Training Batch: 20 | Average loss: 0.2028\n",
      "Average training batch loss at epoch 62: 0.2521\n",
      "Average validation fold accuracy at epoch 62: 0.2722\n",
      "Epoch: 63 | Training Batch: 10 | Average loss: 0.1989\n",
      "Epoch: 63 | Training Batch: 20 | Average loss: 0.2004\n",
      "Average training batch loss at epoch 63: 0.2512\n",
      "Average validation fold accuracy at epoch 63: 0.2724\n",
      "Epoch: 64 | Training Batch: 10 | Average loss: 0.1930\n",
      "Epoch: 64 | Training Batch: 20 | Average loss: 0.1889\n",
      "Average training batch loss at epoch 64: 0.2503\n",
      "Average validation fold accuracy at epoch 64: 0.2729\n",
      "Epoch: 65 | Training Batch: 10 | Average loss: 0.1924\n",
      "Epoch: 65 | Training Batch: 20 | Average loss: 0.1773\n",
      "Average training batch loss at epoch 65: 0.2493\n",
      "Average validation fold accuracy at epoch 65: 0.2734\n",
      "Epoch: 66 | Training Batch: 10 | Average loss: 0.1730\n",
      "Epoch: 66 | Training Batch: 20 | Average loss: 0.1878\n",
      "Average training batch loss at epoch 66: 0.2483\n",
      "Average validation fold accuracy at epoch 66: 0.2737\n",
      "Epoch: 67 | Training Batch: 10 | Average loss: 0.1805\n",
      "Epoch: 67 | Training Batch: 20 | Average loss: 0.1750\n",
      "Average training batch loss at epoch 67: 0.2473\n",
      "Average validation fold accuracy at epoch 67: 0.2742\n",
      "Epoch: 68 | Training Batch: 10 | Average loss: 0.1727\n",
      "Epoch: 68 | Training Batch: 20 | Average loss: 0.1633\n",
      "Average training batch loss at epoch 68: 0.2461\n",
      "Average validation fold accuracy at epoch 68: 0.2745\n",
      "Epoch: 69 | Training Batch: 10 | Average loss: 0.1701\n",
      "Epoch: 69 | Training Batch: 20 | Average loss: 0.1683\n",
      "Average training batch loss at epoch 69: 0.2450\n",
      "Average validation fold accuracy at epoch 69: 0.2750\n",
      "Epoch: 70 | Training Batch: 10 | Average loss: 0.1726\n",
      "Epoch: 70 | Training Batch: 20 | Average loss: 0.1605\n",
      "Average training batch loss at epoch 70: 0.2440\n",
      "Average validation fold accuracy at epoch 70: 0.2754\n",
      "Epoch: 71 | Training Batch: 10 | Average loss: 0.1691\n",
      "Epoch: 71 | Training Batch: 20 | Average loss: 0.1780\n",
      "Average training batch loss at epoch 71: 0.2430\n",
      "Average validation fold accuracy at epoch 71: 0.2755\n",
      "Epoch: 72 | Training Batch: 10 | Average loss: 0.1668\n",
      "Epoch: 72 | Training Batch: 20 | Average loss: 0.1540\n",
      "Average training batch loss at epoch 72: 0.2419\n",
      "Average validation fold accuracy at epoch 72: 0.2761\n",
      "Epoch: 73 | Training Batch: 10 | Average loss: 0.1576\n",
      "Epoch: 73 | Training Batch: 20 | Average loss: 0.1600\n",
      "Average training batch loss at epoch 73: 0.2408\n",
      "Average validation fold accuracy at epoch 73: 0.2766\n",
      "Epoch: 74 | Training Batch: 10 | Average loss: 0.1567\n",
      "Epoch: 74 | Training Batch: 20 | Average loss: 0.1483\n",
      "Average training batch loss at epoch 74: 0.2396\n",
      "Average validation fold accuracy at epoch 74: 0.2768\n",
      "Epoch: 75 | Training Batch: 10 | Average loss: 0.1432\n",
      "Epoch: 75 | Training Batch: 20 | Average loss: 0.1533\n",
      "Average training batch loss at epoch 75: 0.2384\n",
      "Average validation fold accuracy at epoch 75: 0.2774\n",
      "Epoch: 76 | Training Batch: 10 | Average loss: 0.1370\n",
      "Epoch: 76 | Training Batch: 20 | Average loss: 0.1470\n",
      "Average training batch loss at epoch 76: 0.2371\n",
      "Average validation fold accuracy at epoch 76: 0.2778\n",
      "Epoch: 77 | Training Batch: 10 | Average loss: 0.1421\n",
      "Epoch: 77 | Training Batch: 20 | Average loss: 0.1437\n",
      "Average training batch loss at epoch 77: 0.2359\n",
      "Average validation fold accuracy at epoch 77: 0.2782\n",
      "Epoch: 78 | Training Batch: 10 | Average loss: 0.1254\n",
      "Epoch: 78 | Training Batch: 20 | Average loss: 0.1359\n",
      "Average training batch loss at epoch 78: 0.2346\n",
      "Average validation fold accuracy at epoch 78: 0.2788\n",
      "Epoch: 79 | Training Batch: 10 | Average loss: 0.1307\n",
      "Epoch: 79 | Training Batch: 20 | Average loss: 0.1350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training batch loss at epoch 79: 0.2333\n",
      "Average validation fold accuracy at epoch 79: 0.2795\n",
      "Epoch: 80 | Training Batch: 10 | Average loss: 0.1228\n",
      "Epoch: 80 | Training Batch: 20 | Average loss: 0.1215\n",
      "Average training batch loss at epoch 80: 0.2320\n",
      "Average validation fold accuracy at epoch 80: 0.2802\n",
      "Epoch: 81 | Training Batch: 10 | Average loss: 0.1065\n",
      "Epoch: 81 | Training Batch: 20 | Average loss: 0.1377\n",
      "Average training batch loss at epoch 81: 0.2307\n",
      "Average validation fold accuracy at epoch 81: 0.2808\n",
      "Epoch: 82 | Training Batch: 10 | Average loss: 0.1216\n",
      "Epoch: 82 | Training Batch: 20 | Average loss: 0.1125\n",
      "Average training batch loss at epoch 82: 0.2293\n",
      "Average validation fold accuracy at epoch 82: 0.2813\n",
      "Epoch: 83 | Training Batch: 10 | Average loss: 0.1165\n",
      "Epoch: 83 | Training Batch: 20 | Average loss: 0.1156\n",
      "Average training batch loss at epoch 83: 0.2279\n",
      "Average validation fold accuracy at epoch 83: 0.2819\n",
      "Epoch: 84 | Training Batch: 10 | Average loss: 0.1173\n",
      "Epoch: 84 | Training Batch: 20 | Average loss: 0.1146\n",
      "Average training batch loss at epoch 84: 0.2266\n",
      "Average validation fold accuracy at epoch 84: 0.2827\n",
      "Epoch: 85 | Training Batch: 10 | Average loss: 0.1027\n",
      "Epoch: 85 | Training Batch: 20 | Average loss: 0.1020\n",
      "Average training batch loss at epoch 85: 0.2252\n",
      "Average validation fold accuracy at epoch 85: 0.2833\n",
      "Epoch: 86 | Training Batch: 10 | Average loss: 0.0970\n",
      "Epoch: 86 | Training Batch: 20 | Average loss: 0.0996\n",
      "Average training batch loss at epoch 86: 0.2238\n",
      "Average validation fold accuracy at epoch 86: 0.2839\n",
      "Epoch: 87 | Training Batch: 10 | Average loss: 0.1049\n",
      "Epoch: 87 | Training Batch: 20 | Average loss: 0.1022\n",
      "Average training batch loss at epoch 87: 0.2224\n",
      "Average validation fold accuracy at epoch 87: 0.2846\n",
      "Epoch: 88 | Training Batch: 10 | Average loss: 0.0967\n",
      "Epoch: 88 | Training Batch: 20 | Average loss: 0.0936\n",
      "Average training batch loss at epoch 88: 0.2210\n",
      "Average validation fold accuracy at epoch 88: 0.2852\n",
      "Epoch: 89 | Training Batch: 10 | Average loss: 0.1002\n",
      "Epoch: 89 | Training Batch: 20 | Average loss: 0.0976\n",
      "Average training batch loss at epoch 89: 0.2196\n",
      "Average validation fold accuracy at epoch 89: 0.2857\n",
      "Epoch: 90 | Training Batch: 10 | Average loss: 0.0905\n",
      "Epoch: 90 | Training Batch: 20 | Average loss: 0.0916\n",
      "Average training batch loss at epoch 90: 0.2182\n",
      "Average validation fold accuracy at epoch 90: 0.2861\n",
      "Epoch: 91 | Training Batch: 10 | Average loss: 0.0852\n",
      "Epoch: 91 | Training Batch: 20 | Average loss: 0.0896\n",
      "Average training batch loss at epoch 91: 0.2168\n",
      "Average validation fold accuracy at epoch 91: 0.2867\n",
      "Epoch: 92 | Training Batch: 10 | Average loss: 0.0831\n",
      "Epoch: 92 | Training Batch: 20 | Average loss: 0.0928\n",
      "Average training batch loss at epoch 92: 0.2154\n",
      "Average validation fold accuracy at epoch 92: 0.2873\n",
      "Epoch: 93 | Training Batch: 10 | Average loss: 0.0781\n",
      "Epoch: 93 | Training Batch: 20 | Average loss: 0.0790\n",
      "Average training batch loss at epoch 93: 0.2139\n",
      "Average validation fold accuracy at epoch 93: 0.2880\n",
      "Epoch: 94 | Training Batch: 10 | Average loss: 0.0682\n",
      "Epoch: 94 | Training Batch: 20 | Average loss: 0.0746\n",
      "Average training batch loss at epoch 94: 0.2124\n",
      "Average validation fold accuracy at epoch 94: 0.2885\n",
      "Epoch: 95 | Training Batch: 10 | Average loss: 0.0748\n",
      "Epoch: 95 | Training Batch: 20 | Average loss: 0.0745\n",
      "Average training batch loss at epoch 95: 0.2110\n",
      "Average validation fold accuracy at epoch 95: 0.2889\n",
      "Epoch: 96 | Training Batch: 10 | Average loss: 0.0691\n",
      "Epoch: 96 | Training Batch: 20 | Average loss: 0.0749\n",
      "Average training batch loss at epoch 96: 0.2096\n",
      "Average validation fold accuracy at epoch 96: 0.2894\n",
      "Epoch: 97 | Training Batch: 10 | Average loss: 0.0721\n",
      "Epoch: 97 | Training Batch: 20 | Average loss: 0.0754\n",
      "Average training batch loss at epoch 97: 0.2081\n",
      "Average validation fold accuracy at epoch 97: 0.2902\n",
      "Epoch: 98 | Training Batch: 10 | Average loss: 0.0661\n",
      "Epoch: 98 | Training Batch: 20 | Average loss: 0.0677\n",
      "Average training batch loss at epoch 98: 0.2067\n",
      "Average validation fold accuracy at epoch 98: 0.2908\n",
      "Epoch: 99 | Training Batch: 10 | Average loss: 0.0631\n",
      "Epoch: 99 | Training Batch: 20 | Average loss: 0.0639\n",
      "Average training batch loss at epoch 99: 0.2053\n",
      "Average validation fold accuracy at epoch 99: 0.2910\n",
      "Epoch: 100 | Training Batch: 10 | Average loss: 0.0677\n",
      "Epoch: 100 | Training Batch: 20 | Average loss: 0.0750\n",
      "Average training batch loss at epoch 100: 0.2040\n",
      "Average validation fold accuracy at epoch 100: 0.2912\n",
      "Epoch: 101 | Training Batch: 10 | Average loss: 0.0665\n",
      "Epoch: 101 | Training Batch: 20 | Average loss: 0.0678\n",
      "Average training batch loss at epoch 101: 0.2026\n",
      "Average validation fold accuracy at epoch 101: 0.2915\n",
      "Epoch: 102 | Training Batch: 10 | Average loss: 0.0573\n",
      "Epoch: 102 | Training Batch: 20 | Average loss: 0.0722\n",
      "Average training batch loss at epoch 102: 0.2013\n",
      "Average validation fold accuracy at epoch 102: 0.2919\n",
      "Epoch: 103 | Training Batch: 10 | Average loss: 0.0618\n",
      "Epoch: 103 | Training Batch: 20 | Average loss: 0.0611\n",
      "Average training batch loss at epoch 103: 0.1999\n",
      "Average validation fold accuracy at epoch 103: 0.2924\n",
      "Epoch: 104 | Training Batch: 10 | Average loss: 0.0560\n",
      "Epoch: 104 | Training Batch: 20 | Average loss: 0.0604\n",
      "Average training batch loss at epoch 104: 0.1986\n",
      "Average validation fold accuracy at epoch 104: 0.2928\n",
      "Epoch: 105 | Training Batch: 10 | Average loss: 0.0586\n",
      "Epoch: 105 | Training Batch: 20 | Average loss: 0.0571\n",
      "Average training batch loss at epoch 105: 0.1972\n",
      "Average validation fold accuracy at epoch 105: 0.2932\n",
      "Epoch: 106 | Training Batch: 10 | Average loss: 0.0549\n",
      "Epoch: 106 | Training Batch: 20 | Average loss: 0.0576\n",
      "Average training batch loss at epoch 106: 0.1959\n",
      "Average validation fold accuracy at epoch 106: 0.2938\n",
      "Epoch: 107 | Training Batch: 10 | Average loss: 0.0587\n",
      "Epoch: 107 | Training Batch: 20 | Average loss: 0.0483\n",
      "Average training batch loss at epoch 107: 0.1946\n",
      "Average validation fold accuracy at epoch 107: 0.2943\n",
      "Epoch: 108 | Training Batch: 10 | Average loss: 0.0518\n",
      "Epoch: 108 | Training Batch: 20 | Average loss: 0.0521\n",
      "Average training batch loss at epoch 108: 0.1933\n",
      "Average validation fold accuracy at epoch 108: 0.2945\n",
      "Epoch: 109 | Training Batch: 10 | Average loss: 0.0479\n",
      "Epoch: 109 | Training Batch: 20 | Average loss: 0.0510\n",
      "Average training batch loss at epoch 109: 0.1920\n",
      "Average validation fold accuracy at epoch 109: 0.2946\n",
      "Epoch: 110 | Training Batch: 10 | Average loss: 0.0476\n",
      "Epoch: 110 | Training Batch: 20 | Average loss: 0.0524\n",
      "Average training batch loss at epoch 110: 0.1907\n",
      "Average validation fold accuracy at epoch 110: 0.2948\n",
      "Epoch: 111 | Training Batch: 10 | Average loss: 0.0517\n",
      "Epoch: 111 | Training Batch: 20 | Average loss: 0.0534\n",
      "Average training batch loss at epoch 111: 0.1895\n",
      "Average validation fold accuracy at epoch 111: 0.2951\n",
      "Epoch: 112 | Training Batch: 10 | Average loss: 0.0449\n",
      "Epoch: 112 | Training Batch: 20 | Average loss: 0.0500\n",
      "Average training batch loss at epoch 112: 0.1882\n",
      "Average validation fold accuracy at epoch 112: 0.2952\n",
      "Epoch: 113 | Training Batch: 10 | Average loss: 0.0418\n",
      "Epoch: 113 | Training Batch: 20 | Average loss: 0.0423\n",
      "Average training batch loss at epoch 113: 0.1870\n",
      "Average validation fold accuracy at epoch 113: 0.2958\n",
      "Epoch: 114 | Training Batch: 10 | Average loss: 0.0490\n",
      "Epoch: 114 | Training Batch: 20 | Average loss: 0.0416\n",
      "Average training batch loss at epoch 114: 0.1857\n",
      "Average validation fold accuracy at epoch 114: 0.2961\n",
      "Epoch: 115 | Training Batch: 10 | Average loss: 0.0422\n",
      "Epoch: 115 | Training Batch: 20 | Average loss: 0.0438\n",
      "Average training batch loss at epoch 115: 0.1845\n",
      "Average validation fold accuracy at epoch 115: 0.2966\n",
      "Epoch: 116 | Training Batch: 10 | Average loss: 0.0424\n",
      "Epoch: 116 | Training Batch: 20 | Average loss: 0.0418\n",
      "Average training batch loss at epoch 116: 0.1833\n",
      "Average validation fold accuracy at epoch 116: 0.2971\n",
      "Epoch: 117 | Training Batch: 10 | Average loss: 0.0327\n",
      "Epoch: 117 | Training Batch: 20 | Average loss: 0.0423\n",
      "Average training batch loss at epoch 117: 0.1821\n",
      "Average validation fold accuracy at epoch 117: 0.2975\n",
      "Epoch: 118 | Training Batch: 10 | Average loss: 0.0398\n",
      "Epoch: 118 | Training Batch: 20 | Average loss: 0.0374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training batch loss at epoch 118: 0.1809\n",
      "Average validation fold accuracy at epoch 118: 0.2978\n",
      "Epoch: 119 | Training Batch: 10 | Average loss: 0.0387\n",
      "Epoch: 119 | Training Batch: 20 | Average loss: 0.0345\n",
      "Average training batch loss at epoch 119: 0.1797\n",
      "Average validation fold accuracy at epoch 119: 0.2982\n",
      "Epoch: 120 | Training Batch: 10 | Average loss: 0.0335\n",
      "Epoch: 120 | Training Batch: 20 | Average loss: 0.0397\n",
      "Average training batch loss at epoch 120: 0.1785\n",
      "Average validation fold accuracy at epoch 120: 0.2986\n",
      "Epoch: 121 | Training Batch: 10 | Average loss: 0.0375\n",
      "Epoch: 121 | Training Batch: 20 | Average loss: 0.0447\n",
      "Average training batch loss at epoch 121: 0.1774\n",
      "Average validation fold accuracy at epoch 121: 0.2988\n",
      "Epoch: 122 | Training Batch: 10 | Average loss: 0.0353\n",
      "Epoch: 122 | Training Batch: 20 | Average loss: 0.0405\n",
      "Average training batch loss at epoch 122: 0.1762\n",
      "Average validation fold accuracy at epoch 122: 0.2991\n",
      "Epoch: 123 | Training Batch: 10 | Average loss: 0.0353\n",
      "Epoch: 123 | Training Batch: 20 | Average loss: 0.0301\n",
      "Average training batch loss at epoch 123: 0.1751\n",
      "Average validation fold accuracy at epoch 123: 0.2994\n",
      "Epoch: 124 | Training Batch: 10 | Average loss: 0.0334\n",
      "Epoch: 124 | Training Batch: 20 | Average loss: 0.0330\n",
      "Average training batch loss at epoch 124: 0.1739\n",
      "Average validation fold accuracy at epoch 124: 0.2999\n",
      "Epoch: 125 | Training Batch: 10 | Average loss: 0.0363\n",
      "Epoch: 125 | Training Batch: 20 | Average loss: 0.0333\n",
      "Average training batch loss at epoch 125: 0.1728\n",
      "Average validation fold accuracy at epoch 125: 0.3003\n",
      "Epoch: 126 | Training Batch: 10 | Average loss: 0.0299\n",
      "Epoch: 126 | Training Batch: 20 | Average loss: 0.0382\n",
      "Average training batch loss at epoch 126: 0.1717\n",
      "Average validation fold accuracy at epoch 126: 0.3007\n",
      "Epoch: 127 | Training Batch: 10 | Average loss: 0.0315\n",
      "Epoch: 127 | Training Batch: 20 | Average loss: 0.0404\n",
      "Average training batch loss at epoch 127: 0.1707\n",
      "Average validation fold accuracy at epoch 127: 0.3010\n",
      "Epoch: 128 | Training Batch: 10 | Average loss: 0.0334\n",
      "Epoch: 128 | Training Batch: 20 | Average loss: 0.0371\n",
      "Average training batch loss at epoch 128: 0.1696\n",
      "Average validation fold accuracy at epoch 128: 0.3014\n",
      "Epoch: 129 | Training Batch: 10 | Average loss: 0.0283\n",
      "Epoch: 129 | Training Batch: 20 | Average loss: 0.0352\n",
      "Average training batch loss at epoch 129: 0.1686\n",
      "Average validation fold accuracy at epoch 129: 0.3017\n",
      "Epoch: 130 | Training Batch: 10 | Average loss: 0.0353\n",
      "Epoch: 130 | Training Batch: 20 | Average loss: 0.0281\n",
      "Average training batch loss at epoch 130: 0.1675\n",
      "Average validation fold accuracy at epoch 130: 0.3023\n",
      "Epoch: 131 | Training Batch: 10 | Average loss: 0.0363\n",
      "Epoch: 131 | Training Batch: 20 | Average loss: 0.0436\n",
      "Average training batch loss at epoch 131: 0.1665\n",
      "Average validation fold accuracy at epoch 131: 0.3027\n",
      "Epoch: 132 | Training Batch: 10 | Average loss: 0.0351\n",
      "Epoch: 132 | Training Batch: 20 | Average loss: 0.0357\n",
      "Average training batch loss at epoch 132: 0.1655\n",
      "Average validation fold accuracy at epoch 132: 0.3031\n",
      "Epoch: 133 | Training Batch: 10 | Average loss: 0.0319\n",
      "Epoch: 133 | Training Batch: 20 | Average loss: 0.0301\n",
      "Average training batch loss at epoch 133: 0.1645\n",
      "Average validation fold accuracy at epoch 133: 0.3037\n",
      "Epoch: 134 | Training Batch: 10 | Average loss: 0.0305\n",
      "Epoch: 134 | Training Batch: 20 | Average loss: 0.0276\n",
      "Average training batch loss at epoch 134: 0.1635\n",
      "Average validation fold accuracy at epoch 134: 0.3041\n",
      "Epoch: 135 | Training Batch: 10 | Average loss: 0.0302\n",
      "Epoch: 135 | Training Batch: 20 | Average loss: 0.0271\n",
      "Average training batch loss at epoch 135: 0.1625\n",
      "Average validation fold accuracy at epoch 135: 0.3045\n",
      "Epoch: 136 | Training Batch: 10 | Average loss: 0.0269\n",
      "Epoch: 136 | Training Batch: 20 | Average loss: 0.0276\n",
      "Average training batch loss at epoch 136: 0.1616\n",
      "Average validation fold accuracy at epoch 136: 0.3049\n",
      "Epoch: 137 | Training Batch: 10 | Average loss: 0.0274\n",
      "Epoch: 137 | Training Batch: 20 | Average loss: 0.0252\n",
      "Average training batch loss at epoch 137: 0.1606\n",
      "Average validation fold accuracy at epoch 137: 0.3052\n",
      "Epoch: 138 | Training Batch: 10 | Average loss: 0.0257\n",
      "Epoch: 138 | Training Batch: 20 | Average loss: 0.0291\n",
      "Average training batch loss at epoch 138: 0.1596\n",
      "Average validation fold accuracy at epoch 138: 0.3055\n",
      "Epoch: 139 | Training Batch: 10 | Average loss: 0.0225\n",
      "Epoch: 139 | Training Batch: 20 | Average loss: 0.0269\n",
      "Average training batch loss at epoch 139: 0.1587\n",
      "Average validation fold accuracy at epoch 139: 0.3058\n",
      "Epoch: 140 | Training Batch: 10 | Average loss: 0.0236\n",
      "Epoch: 140 | Training Batch: 20 | Average loss: 0.0254\n",
      "Average training batch loss at epoch 140: 0.1577\n",
      "Average validation fold accuracy at epoch 140: 0.3061\n",
      "Epoch: 141 | Training Batch: 10 | Average loss: 0.0212\n",
      "Epoch: 141 | Training Batch: 20 | Average loss: 0.0265\n",
      "Average training batch loss at epoch 141: 0.1568\n",
      "Average validation fold accuracy at epoch 141: 0.3065\n",
      "Epoch: 142 | Training Batch: 10 | Average loss: 0.0211\n",
      "Epoch: 142 | Training Batch: 20 | Average loss: 0.0230\n",
      "Average training batch loss at epoch 142: 0.1558\n",
      "Average validation fold accuracy at epoch 142: 0.3069\n",
      "Epoch: 143 | Training Batch: 10 | Average loss: 0.0226\n",
      "Epoch: 143 | Training Batch: 20 | Average loss: 0.0231\n",
      "Average training batch loss at epoch 143: 0.1549\n",
      "Average validation fold accuracy at epoch 143: 0.3073\n",
      "Epoch: 144 | Training Batch: 10 | Average loss: 0.0281\n",
      "Epoch: 144 | Training Batch: 20 | Average loss: 0.0242\n",
      "Average training batch loss at epoch 144: 0.1540\n",
      "Average validation fold accuracy at epoch 144: 0.3076\n",
      "Epoch: 145 | Training Batch: 10 | Average loss: 0.0231\n",
      "Epoch: 145 | Training Batch: 20 | Average loss: 0.0257\n",
      "Average training batch loss at epoch 145: 0.1531\n",
      "Average validation fold accuracy at epoch 145: 0.3081\n",
      "Epoch: 146 | Training Batch: 10 | Average loss: 0.0294\n",
      "Epoch: 146 | Training Batch: 20 | Average loss: 0.0258\n",
      "Average training batch loss at epoch 146: 0.1523\n",
      "Average validation fold accuracy at epoch 146: 0.3085\n",
      "Epoch: 147 | Training Batch: 10 | Average loss: 0.0216\n",
      "Epoch: 147 | Training Batch: 20 | Average loss: 0.0268\n",
      "Average training batch loss at epoch 147: 0.1514\n",
      "Average validation fold accuracy at epoch 147: 0.3089\n",
      "Epoch: 148 | Training Batch: 10 | Average loss: 0.0292\n",
      "Epoch: 148 | Training Batch: 20 | Average loss: 0.0162\n",
      "Average training batch loss at epoch 148: 0.1506\n",
      "Average validation fold accuracy at epoch 148: 0.3092\n",
      "Epoch: 149 | Training Batch: 10 | Average loss: 0.0219\n",
      "Epoch: 149 | Training Batch: 20 | Average loss: 0.0209\n",
      "Average training batch loss at epoch 149: 0.1497\n",
      "Average validation fold accuracy at epoch 149: 0.3093\n",
      "Epoch: 150 | Training Batch: 10 | Average loss: 0.0191\n",
      "Epoch: 150 | Training Batch: 20 | Average loss: 0.0220\n",
      "Average training batch loss at epoch 150: 0.1488\n",
      "Average validation fold accuracy at epoch 150: 0.3098\n",
      "Epoch: 151 | Training Batch: 10 | Average loss: 0.0253\n",
      "Epoch: 151 | Training Batch: 20 | Average loss: 0.0166\n",
      "Average training batch loss at epoch 151: 0.1480\n",
      "Average validation fold accuracy at epoch 151: 0.3101\n",
      "Epoch: 152 | Training Batch: 10 | Average loss: 0.0206\n",
      "Epoch: 152 | Training Batch: 20 | Average loss: 0.0209\n",
      "Average training batch loss at epoch 152: 0.1472\n",
      "Average validation fold accuracy at epoch 152: 0.3102\n",
      "Epoch: 153 | Training Batch: 10 | Average loss: 0.0200\n",
      "Epoch: 153 | Training Batch: 20 | Average loss: 0.0199\n",
      "Average training batch loss at epoch 153: 0.1464\n",
      "Average validation fold accuracy at epoch 153: 0.3105\n",
      "Epoch: 154 | Training Batch: 10 | Average loss: 0.0181\n",
      "Epoch: 154 | Training Batch: 20 | Average loss: 0.0179\n",
      "Average training batch loss at epoch 154: 0.1455\n",
      "Average validation fold accuracy at epoch 154: 0.3109\n",
      "Epoch: 155 | Training Batch: 10 | Average loss: 0.0233\n",
      "Epoch: 155 | Training Batch: 20 | Average loss: 0.0172\n",
      "Average training batch loss at epoch 155: 0.1447\n",
      "Average validation fold accuracy at epoch 155: 0.3113\n",
      "Epoch: 156 | Training Batch: 10 | Average loss: 0.0230\n",
      "Epoch: 156 | Training Batch: 20 | Average loss: 0.0196\n",
      "Average training batch loss at epoch 156: 0.1440\n",
      "Average validation fold accuracy at epoch 156: 0.3117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 157 | Training Batch: 10 | Average loss: 0.0223\n",
      "Epoch: 157 | Training Batch: 20 | Average loss: 0.0157\n",
      "Average training batch loss at epoch 157: 0.1432\n",
      "Average validation fold accuracy at epoch 157: 0.3120\n",
      "Epoch: 158 | Training Batch: 10 | Average loss: 0.0173\n",
      "Epoch: 158 | Training Batch: 20 | Average loss: 0.0263\n",
      "Average training batch loss at epoch 158: 0.1424\n",
      "Average validation fold accuracy at epoch 158: 0.3124\n",
      "Epoch: 159 | Training Batch: 10 | Average loss: 0.0201\n",
      "Epoch: 159 | Training Batch: 20 | Average loss: 0.0215\n",
      "Average training batch loss at epoch 159: 0.1416\n",
      "Average validation fold accuracy at epoch 159: 0.3126\n",
      "Epoch: 160 | Training Batch: 10 | Average loss: 0.0240\n",
      "Epoch: 160 | Training Batch: 20 | Average loss: 0.0240\n",
      "Average training batch loss at epoch 160: 0.1409\n",
      "Average validation fold accuracy at epoch 160: 0.3130\n",
      "Epoch: 161 | Training Batch: 10 | Average loss: 0.0234\n",
      "Epoch: 161 | Training Batch: 20 | Average loss: 0.0239\n",
      "Average training batch loss at epoch 161: 0.1402\n",
      "Average validation fold accuracy at epoch 161: 0.3134\n",
      "Epoch: 162 | Training Batch: 10 | Average loss: 0.0328\n",
      "Epoch: 162 | Training Batch: 20 | Average loss: 0.0238\n",
      "Average training batch loss at epoch 162: 0.1395\n",
      "Average validation fold accuracy at epoch 162: 0.3136\n",
      "Epoch: 163 | Training Batch: 10 | Average loss: 0.0251\n",
      "Epoch: 163 | Training Batch: 20 | Average loss: 0.0291\n",
      "Average training batch loss at epoch 163: 0.1388\n",
      "Average validation fold accuracy at epoch 163: 0.3138\n",
      "Epoch: 164 | Training Batch: 10 | Average loss: 0.0253\n",
      "Epoch: 164 | Training Batch: 20 | Average loss: 0.0257\n",
      "Average training batch loss at epoch 164: 0.1381\n",
      "Average validation fold accuracy at epoch 164: 0.3141\n",
      "Epoch: 165 | Training Batch: 10 | Average loss: 0.0218\n",
      "Epoch: 165 | Training Batch: 20 | Average loss: 0.0280\n",
      "Average training batch loss at epoch 165: 0.1374\n",
      "Average validation fold accuracy at epoch 165: 0.3144\n",
      "Epoch: 166 | Training Batch: 10 | Average loss: 0.0158\n",
      "Epoch: 166 | Training Batch: 20 | Average loss: 0.0213\n",
      "Average training batch loss at epoch 166: 0.1367\n",
      "Average validation fold accuracy at epoch 166: 0.3147\n",
      "Epoch: 167 | Training Batch: 10 | Average loss: 0.0273\n",
      "Epoch: 167 | Training Batch: 20 | Average loss: 0.0249\n",
      "Average training batch loss at epoch 167: 0.1361\n",
      "Average validation fold accuracy at epoch 167: 0.3149\n",
      "Epoch: 168 | Training Batch: 10 | Average loss: 0.0266\n",
      "Epoch: 168 | Training Batch: 20 | Average loss: 0.0323\n",
      "Average training batch loss at epoch 168: 0.1354\n",
      "Average validation fold accuracy at epoch 168: 0.3150\n",
      "Epoch: 169 | Training Batch: 10 | Average loss: 0.0244\n",
      "Epoch: 169 | Training Batch: 20 | Average loss: 0.0154\n",
      "Average training batch loss at epoch 169: 0.1347\n",
      "Average validation fold accuracy at epoch 169: 0.3152\n",
      "Epoch: 170 | Training Batch: 10 | Average loss: 0.0141\n",
      "Epoch: 170 | Training Batch: 20 | Average loss: 0.0255\n",
      "Average training batch loss at epoch 170: 0.1341\n",
      "Average validation fold accuracy at epoch 170: 0.3155\n",
      "Epoch: 171 | Training Batch: 10 | Average loss: 0.0212\n",
      "Epoch: 171 | Training Batch: 20 | Average loss: 0.0203\n",
      "Average training batch loss at epoch 171: 0.1334\n",
      "Average validation fold accuracy at epoch 171: 0.3158\n",
      "Epoch: 172 | Training Batch: 10 | Average loss: 0.0169\n",
      "Epoch: 172 | Training Batch: 20 | Average loss: 0.0269\n",
      "Average training batch loss at epoch 172: 0.1328\n",
      "Average validation fold accuracy at epoch 172: 0.3162\n",
      "Epoch: 173 | Training Batch: 10 | Average loss: 0.0183\n",
      "Epoch: 173 | Training Batch: 20 | Average loss: 0.0129\n",
      "Average training batch loss at epoch 173: 0.1321\n",
      "Average validation fold accuracy at epoch 173: 0.3165\n",
      "Epoch: 174 | Training Batch: 10 | Average loss: 0.0173\n",
      "Epoch: 174 | Training Batch: 20 | Average loss: 0.0215\n",
      "Average training batch loss at epoch 174: 0.1315\n",
      "Average validation fold accuracy at epoch 174: 0.3168\n",
      "Epoch: 175 | Training Batch: 10 | Average loss: 0.0135\n",
      "Epoch: 175 | Training Batch: 20 | Average loss: 0.0172\n",
      "Average training batch loss at epoch 175: 0.1308\n",
      "Average validation fold accuracy at epoch 175: 0.3172\n",
      "Epoch: 176 | Training Batch: 10 | Average loss: 0.0209\n",
      "Epoch: 176 | Training Batch: 20 | Average loss: 0.0166\n",
      "Average training batch loss at epoch 176: 0.1302\n",
      "Average validation fold accuracy at epoch 176: 0.3175\n",
      "Epoch: 177 | Training Batch: 10 | Average loss: 0.0186\n",
      "Epoch: 177 | Training Batch: 20 | Average loss: 0.0176\n",
      "Average training batch loss at epoch 177: 0.1295\n",
      "Average validation fold accuracy at epoch 177: 0.3177\n",
      "Epoch: 178 | Training Batch: 10 | Average loss: 0.0284\n",
      "Epoch: 178 | Training Batch: 20 | Average loss: 0.0188\n",
      "Average training batch loss at epoch 178: 0.1290\n",
      "Average validation fold accuracy at epoch 178: 0.3182\n",
      "Epoch: 179 | Training Batch: 10 | Average loss: 0.0159\n",
      "Epoch: 179 | Training Batch: 20 | Average loss: 0.0298\n",
      "Average training batch loss at epoch 179: 0.1284\n",
      "Average validation fold accuracy at epoch 179: 0.3185\n",
      "Epoch: 180 | Training Batch: 10 | Average loss: 0.0244\n",
      "Epoch: 180 | Training Batch: 20 | Average loss: 0.0164\n",
      "Average training batch loss at epoch 180: 0.1278\n",
      "Average validation fold accuracy at epoch 180: 0.3189\n",
      "Epoch: 181 | Training Batch: 10 | Average loss: 0.0249\n",
      "Epoch: 181 | Training Batch: 20 | Average loss: 0.0223\n",
      "Average training batch loss at epoch 181: 0.1272\n",
      "Average validation fold accuracy at epoch 181: 0.3191\n",
      "Epoch: 182 | Training Batch: 10 | Average loss: 0.0128\n",
      "Epoch: 182 | Training Batch: 20 | Average loss: 0.0204\n",
      "Average training batch loss at epoch 182: 0.1266\n",
      "Average validation fold accuracy at epoch 182: 0.3193\n",
      "Epoch: 183 | Training Batch: 10 | Average loss: 0.0185\n",
      "Epoch: 183 | Training Batch: 20 | Average loss: 0.0200\n",
      "Average training batch loss at epoch 183: 0.1260\n",
      "Average validation fold accuracy at epoch 183: 0.3196\n",
      "Epoch: 184 | Training Batch: 10 | Average loss: 0.0120\n",
      "Epoch: 184 | Training Batch: 20 | Average loss: 0.0172\n",
      "Average training batch loss at epoch 184: 0.1254\n",
      "Average validation fold accuracy at epoch 184: 0.3199\n",
      "Epoch: 185 | Training Batch: 10 | Average loss: 0.0076\n",
      "Epoch: 185 | Training Batch: 20 | Average loss: 0.0232\n",
      "Average training batch loss at epoch 185: 0.1248\n",
      "Average validation fold accuracy at epoch 185: 0.3204\n",
      "Epoch: 186 | Training Batch: 10 | Average loss: 0.0094\n",
      "Epoch: 186 | Training Batch: 20 | Average loss: 0.0279\n",
      "Average training batch loss at epoch 186: 0.1242\n",
      "Average validation fold accuracy at epoch 186: 0.3208\n",
      "Epoch: 187 | Training Batch: 10 | Average loss: 0.0147\n",
      "Epoch: 187 | Training Batch: 20 | Average loss: 0.0179\n",
      "Average training batch loss at epoch 187: 0.1237\n",
      "Average validation fold accuracy at epoch 187: 0.3212\n",
      "Epoch: 188 | Training Batch: 10 | Average loss: 0.0224\n",
      "Epoch: 188 | Training Batch: 20 | Average loss: 0.0245\n",
      "Average training batch loss at epoch 188: 0.1231\n",
      "Average validation fold accuracy at epoch 188: 0.3217\n",
      "Epoch: 189 | Training Batch: 10 | Average loss: 0.0263\n",
      "Epoch: 189 | Training Batch: 20 | Average loss: 0.0166\n",
      "Average training batch loss at epoch 189: 0.1226\n",
      "Average validation fold accuracy at epoch 189: 0.3221\n",
      "Epoch: 190 | Training Batch: 10 | Average loss: 0.0194\n",
      "Epoch: 190 | Training Batch: 20 | Average loss: 0.0228\n",
      "Average training batch loss at epoch 190: 0.1221\n",
      "Average validation fold accuracy at epoch 190: 0.3225\n",
      "Epoch: 191 | Training Batch: 10 | Average loss: 0.0158\n",
      "Epoch: 191 | Training Batch: 20 | Average loss: 0.0157\n",
      "Average training batch loss at epoch 191: 0.1215\n",
      "Average validation fold accuracy at epoch 191: 0.3230\n",
      "Epoch: 192 | Training Batch: 10 | Average loss: 0.0114\n",
      "Epoch: 192 | Training Batch: 20 | Average loss: 0.0103\n",
      "Average training batch loss at epoch 192: 0.1209\n",
      "Average validation fold accuracy at epoch 192: 0.3236\n",
      "Epoch: 193 | Training Batch: 10 | Average loss: 0.0132\n",
      "Epoch: 193 | Training Batch: 20 | Average loss: 0.0102\n",
      "Average training batch loss at epoch 193: 0.1204\n",
      "Average validation fold accuracy at epoch 193: 0.3239\n",
      "Epoch: 194 | Training Batch: 10 | Average loss: 0.0193\n",
      "Epoch: 194 | Training Batch: 20 | Average loss: 0.0164\n",
      "Average training batch loss at epoch 194: 0.1198\n",
      "Average validation fold accuracy at epoch 194: 0.3243\n",
      "Epoch: 195 | Training Batch: 10 | Average loss: 0.0174\n",
      "Epoch: 195 | Training Batch: 20 | Average loss: 0.0195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training batch loss at epoch 195: 0.1193\n",
      "Average validation fold accuracy at epoch 195: 0.3246\n",
      "Epoch: 196 | Training Batch: 10 | Average loss: 0.0167\n",
      "Epoch: 196 | Training Batch: 20 | Average loss: 0.0169\n",
      "Average training batch loss at epoch 196: 0.1188\n",
      "Average validation fold accuracy at epoch 196: 0.3250\n",
      "Epoch: 197 | Training Batch: 10 | Average loss: 0.0175\n",
      "Epoch: 197 | Training Batch: 20 | Average loss: 0.0172\n",
      "Average training batch loss at epoch 197: 0.1183\n",
      "Average validation fold accuracy at epoch 197: 0.3254\n",
      "Epoch: 198 | Training Batch: 10 | Average loss: 0.0120\n",
      "Epoch: 198 | Training Batch: 20 | Average loss: 0.0131\n",
      "Average training batch loss at epoch 198: 0.1178\n",
      "Average validation fold accuracy at epoch 198: 0.3256\n",
      "Epoch: 199 | Training Batch: 10 | Average loss: 0.0107\n",
      "Epoch: 199 | Training Batch: 20 | Average loss: 0.0134\n",
      "Average training batch loss at epoch 199: 0.1173\n",
      "Average validation fold accuracy at epoch 199: 0.3260\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "pretrain=False\n",
    "batch_size = 64\n",
    "opt = {\n",
    "    'node_size':201,\n",
    "    'hidden_size':30,\n",
    "    'num_layers':2,\n",
    "    'embedding_dim':50,\n",
    "    'learning_rate':5e-3\n",
    "}\n",
    "\n",
    "# Initialize global tracking variables\n",
    "best_validation_accuracy = 0\n",
    "epochs_without_improvement = 0\n",
    "total_train_loss = list()\n",
    "total_valid_loss = []\n",
    "avg_trainings = []\n",
    "avg_valids = []\n",
    "\n",
    "\n",
    "# Loading model\n",
    "if pretrain:\n",
    "    classifier = torch.load('SiameseNN1.pt')\n",
    "else:\n",
    "    classifier = SiameseClassifier(opt, is_train=True)\n",
    "    # Initialize parameters\n",
    "    classifier.initialize_parameters()\n",
    "\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Initiate the training data loader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
    "    running_loss = list()\n",
    "    # Training loop\n",
    "    for i, (batch_x,label_var) in enumerate(train_loader):\n",
    "        s1_var = batch_x[:,0,:]\n",
    "        s2_var  = batch_x[:,1,:]\n",
    "        #s1_var = one_hot(s1_var)\n",
    "        #s2_var = one_hot(s2_var)\n",
    "        classifier.train_step(s1_var, s2_var, label_var)\n",
    "        train_batch_loss = classifier.loss.data[0]\n",
    "        running_loss.append(train_batch_loss)\n",
    "        total_train_loss.append(train_batch_loss)\n",
    "\n",
    "        if i % 10 == 0 and i != 0:\n",
    "            running_avg_loss = sum(running_loss) / len(running_loss)\n",
    "            print('Epoch: %d | Training Batch: %d | Average loss: %.4f' %\n",
    "                  (epoch, i , running_avg_loss))\n",
    "            running_loss = []\n",
    "            \n",
    "\n",
    "    # Report epoch statistics\n",
    "    avg_training_accuracy = sum(total_train_loss) / len(total_train_loss)\n",
    "    print('Average training batch loss at epoch %d: %.4f' % (epoch, avg_training_accuracy))\n",
    "    avg_trainings.append(avg_training_accuracy) \n",
    "    \n",
    "\n",
    "    # Validate after each epoch; set tracking variables\n",
    "    if epoch >= 0:\n",
    "        # Initiate the training data loader\n",
    "        valid_loader = DataLoader(val_dataset, batch_size=32,shuffle=True)\n",
    "        \n",
    "        # Validation loop (i.e. perform inference on the validation set)\n",
    "        for i, (batch_x,label_var) in enumerate(valid_loader):\n",
    "            s1_var = batch_x[:,0,:]\n",
    "            s2_var  = batch_x[:,1,:]\n",
    "            #s1_var = one_hot(s1_var)\n",
    "            #s2_var = one_hot(s2_var)\n",
    "            # Get predictions and update tracking values\n",
    "            classifier.test_step(s1_var, s2_var, label_var)\n",
    "            valid_batch_loss = classifier.loss.data[0]\n",
    "            total_valid_loss.append(valid_batch_loss)\n",
    "\n",
    "        # Report fold statistics\n",
    "        avg_valid_accuracy = sum(total_valid_loss) / len(total_valid_loss)\n",
    "        print('Average validation fold accuracy at epoch %d: %.4f' % (epoch, avg_valid_accuracy))\n",
    "        avg_valids.append(avg_valid_accuracy)\n",
    "        # Save network parameters if performance has improved\n",
    "        if avg_valid_accuracy <= best_validation_accuracy:\n",
    "            epochs_without_improvement += 1\n",
    "        else:\n",
    "            best_validation_accuracy = avg_valid_accuracy\n",
    "            epochs_without_improvement = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJ4AAAFXCAYAAADjzIQxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3Xm8nHV5///X5yyBw5YQFpUlLJZaEBEwIBZkaVQWZSlFNm1VbNGqtcpXBdQfIm2/ptK60MWVqP21AlERAUW0QMUNISAExSqoLAnIFsGFk3BOzvX9Y+Ykc+bMJPfM3Pesr+fjweOcueeemc8wmZwz71zX9UkRgSRJkiRJkpS3oU4vQJIkSZIkSf3J4EmSJEmSJEmFMHiSJEmSJElSIQyeJEmSJEmSVAiDJ0mSJEmSJBXC4EmSJEmSJEmFMHiSJEmSJElSIQyeJEmSMkop7ZpSior/PtvpNUmSJHUzgydJkiRJkiQVwuBJkiRJkiRJhTB4kiRJKlAqOS2ldE1K6eGU0tMppSdSSj9IKZ2bUtqqxm12TCl9NKV0V0rp9+XbPJxSuj2ltCSl9GetnC9JktQuKSI6vQZJkqSekFLaFfhlxaHPRcRrN3D+GPBl4MgN3O0vgSMj4u7ybbYFlgPP2sBtvhURhzdzviRJUjuNdHoBkiRJfexDzAydvg98E/hD4NTysd2Ar6SU9omISeAk1odIq4HPAA8A2wG7AIdVPUaj50uSJLWNwZMkSVIBUkrzgb+sOPRt4IiIWFu+/mfAeeXr9gSOpVQdtWnFbb4VEW+qut8hYNeKQ42eL0mS1DYGT5IkScV4ITN/1/rcdOhUtoT1wRPAwZSCp28DASTgyJTSXcCPgHuAO4HrI+IXFbdr9HxJkqS2MXiSJEkqxvyqy7/ayOX5ABFxa0rpLcA/APMoVUPtWXHe2pTShRFxbjPnS5IktZO72kmSJBVjVdXlZ27k8rrzI+Lfy9cfCrwRuBD4bvnqYeCclNKhzZ4vSZLULlY8SZIkFeMHwCTrf9/6i5TSZyJiqnz5jKrzvwuQUnoWQEQ8RKmN7tvl4wn4NTC3fP4BwI2Nnp/j85MkSdoogydJkqTmvSKltKzOdW+gNMfpzPLlQ4HvpJS+CezB+l3tAH4KXFX+/mBgaUrpJkqzmh4CJoAXsz5EgvUVUo2eL0mS1DYGT5IkSc3bpvxfLVsCbwN2B15SPvai8n+V7geOj4jJimOpzrnT7ga+2ML5kiRJbeGMJ0mSpIJExDhwJPDnwLXAo5Ta734DLAPeCzw/In5acbPvAecAXwF+BjwBrAWeBG4F/g54YUT8tsnzJUmS2iZFRKfXIEmSJEmSpD5kxZMkSZIkSZIKYfAkSZIkSZKkQhg8SZIkSZIkqRAGT5IkSZIkSSqEwZMkSZIkSZIKMdLpBRRp2223jV133bXTy5AkSZIkSeobt95662MRsV2Wc/s6eNp1111ZtmxZp5chSZIkSZLUN1JK92U911Y7SZIkSZIkFcLgSZIkSZIkSYUweJIkSZIkSVIh+nrGUy0TExOsWLGC1atXd3opbbPpppuy0047MTo62umlSJIkSZKkATJwwdOKFSvYcsst2XXXXUkpdXo5hYsIHn/8cVasWMFuu+3W6eVIkiRJkqQBMnCtdqtXr2abbbYZiNAJIKXENttsM1AVXpIkSZIkqTsMXPAEDEzoNG3Qnq8kSZIkSeoOAxk8ddrhhx/OtddeO+PYRz7yEd70pjfVvc0WW2xR9LIkSZIkSVIBrvjhSg5efD27nfNVDl58PVf8cGWnl9Q2AzfjqRucdtppXHrppRx55JHrjl166aVceOGFHVyVJEmSJElqxBU/XMmF1/6UB58YZ4d5Y7zzyOcAzDh2xB9tx5duXcn4xFoAVj4xzrmX3wnACfvt2LG1t4vB00bU+kPU6h+Mk046ife+972sWbOGTTbZhHvvvZcHH3yQfffdl0WLFvHrX/+aiYkJ/v7v/57jjz8+p2ciSZIkSZIqZQmONnTs3MvvnBEovfMLd0CCibWx7th/3nT/rMcdn1jLhdf+1OBp0F3xw5Wz/hDlkUpus802HHjggXz961/n+OOP59JLL+WUU05hbGyML3/5y2y11VY89thjHHTQQRx33HHOaJIkSZIkqQHNViLVCo7e8YXbSSlVHbuD4aHEmsmpGY87MRWZ1/jgE+N5PNWuN9DB0/uv+jF3Pfibutf/8P4neHrtzD9E4xNredcXl3PJzbMTS4C9dtiK9x373I0+9nS73XTwtGTJEiKCd7/73dx4440MDQ2xcuVKHn74YZ75zGc29sQkSZIkSRoQ1SFT1kCpViVSreColC1F1bFgsoGQqZYd5o21dPteMdDB08ZUh04bO96IE044gbPOOovbbruN8fFx9t9/fz772c/y6KOPcuuttzI6Osquu+7K6tWrW34sSZIkSZK6WbMtb7VCpqyBUjslZkZXY6PD655Pvxvo4GljlUkHL76elTVK33acN8Zlb3hRS4+9xRZbcPjhh3PGGWdw2mmnAfDkk0+y/fbbMzo6yg033MB9993X0mNIkiRJktRtmq1QqtXyVitkapd5Y6OsmZxat26A0aE0Y91QCpn+7AU7csP/Pprr/OheMdDB08a888jnzJjxBPmmkqeddhonnngil156KQCvetWrOPbYY1m4cCH77rsvf/RHf5TL40iSJEmSVKRGKpaqZym30vLWqupKpFrBUb0w6fzjSsUsWZ73oIRMtaSIzpabFWnhwoWxbNmyGcd+8pOfsOeee2a+jyJ2teuERp+3JEmSJGmw5NXuBjA6BFRUJwGMDieG0uyB3EXIEijVq0QCw6SNSSndGhELM51r8DQYBvV5S5IkSdIgyTc8qlX9w6xAqdNqzU/KGigNcnjUCoOnMoOn9Qb1eUuSJElSL6nXdZMlUMoaHg0nSCm1vCtbnlppeRvk+Umd0kjw5IwnSZIkSZIK1kxwtPKJcc69/E6W3bcq0/DtrLOS1gbQoSKUWgO5bXnrb1Y8DYhBfd6SJEmSlJei29jqqa4G6kZZZyp94MTnAYZHvc5WuzKDp/UG9XlLkiRJGlzNBkWtzUDKHih1m7zb3cCAqV8ZPJUZPK03qM9bkiRJUu9qNTg69/I7mwqKui08Gk7l9rgM2hEe1TpmoDRYDJ7KujF4evzxx1m0aBEAv/rVrxgeHma77bYD4Oabb2bOnDkbvY/Xve51nHPOOTznOc/J/Lidft6SJEmSBlPxu6zNPrbJyBCbjAzxm9WTbXqW+am3Q1uW/xeGR2oXg6eybgyeKp1//vlsscUWvOMd75hxPCKICIaGhnJ7rG563pIkSZJ6SyfCo36TdQZSvR3asr4GhkdqB3e1y9PypXDdBfDkCpi7Eyw6D/Y5OfeHueeeezjhhBM45JBD+MEPfsDVV1/N+9//fm677TbGx8c55ZRTOO+88wA45JBD+Nd//Vf23ntvtt12W974xjdyzTXXsNlmm/GVr3yF7bffPvf1SZIkSeovtYKMWgFHrZ3Wau2o1soua7WOdaNOzkA6Yb8da15n0KRuZ/C0IcuXwlVvhYnx0uUnHyhdhkLCp7vuuovPfOYzfPzjHwdg8eLFzJ8/n8nJSY444ghOOukk9tprrxm3efLJJznssMNYvHgxZ511FkuWLOGcc87JfW2SJEmSekOWyphaYdK5l9/JsvtWzTreq+HRvLFR1kxO5Tbjqag2NoMj9bvBDp6uOQd+dWf961fcAmvXzDw2MQ5feQvc+rnat3nm8+DoxU0t59nPfjYHHHDAusuXXHIJF198MZOTkzz44IPcdddds4KnsbExjj76aABe8IIX8O1vf7upx5YkSZLUPfJqbWukEml8Ym3N453USoXR+cc9F2huB7s8wiMDJalksIOnjakOnTZ2vEWbb775uu/vvvtuPvrRj3LzzTczb948Xv3qV7N69epZt6kcRj48PMzkZO8Nz5MkSZL6UdHhUSutbe3U6V3WsoZChkdSMQY7eNpYZdKH9y6111WbuzO87qvFrKnsN7/5DVtuuSVbbbUVDz30ENdeey1HHXVUoY8pSZIkaeOabWXrhblIwymxtsYGVJ0Oj6oZEkm9Y7CDp41ZdN7MGU8Ao2Ol4wXbf//92Wuvvdh7773ZfffdOfjggwt/TEmSJGlQNVKddO7ldzbVytZtc5Gqw6TpkKh6FzrDI0mtSFEjze4XCxcujGXLls049pOf/IQ999wz+520aVe7ojX8vCVJkqQek1drG9Su3pkznBgZHuKpp9fSzbJUJ9ULk2rtareh8EjSYEop3RoRCzOda/A0GAb1eUuSJKn3NdPaBtnbwbpRO1rbDJMkNauR4MlWO0mSJEmF68QubZ1ubeuFuUiSVDQrngbEoD5vSZIkFafZuUjQu9VI88ZGWTM5tdHnUlR4JEndwIqnjYgIUkqdXkbb9HO4KEmSpPzluWvbOV9azpyRoRlBDXS+Gqla1rlI5x/3XMCh2pKU1cBVPP3yl79kyy23ZJttthmI8CkiePzxx/ntb3/Lbrvt1unlSJIkqYPynJXUjZyLJEnt4XDxslrB08TEBCtWrGD16tUdWlX7bbrppuy0006Mjo52eimSJEkqgIGSrW2S1E5dHTyllI4CPgoMA5+OiMVV178ReDOwFvgdcGZE3FW+7lzg9eXr3hoR127osWoFT5IkSVKv6JdAKetcJKuRJKk3dG3wlFIaBn4GvBRYAdwCnDYdLJXP2SoiflP+/jjgTRFxVEppL+AS4EBgB+C/gT+MiLXUYfAkSZKkbpP3QO5OylqJ9IETnwdkqzyqdcxASZK6SzcHTy8Czo+II8uXzwWIiA/UOf804C8i4ujqc1NK15bv6/v1Hs/gSZIkSe2SZ3XSJiNDzBkZ4rerJ9v+PCD/1jaDI0nqL928q92OwAMVl1cAL6w+KaX0ZuAsYA7wJxW3vanqtv4EkyRJUm4aqUZqZoe3/7zp/lmPWWsntzWTU6yZnMr9+bUrUDJokiRNa3fwVGsbuVk/aSPi34B/SymdDrwXeE3W26aUzgTOBFiwYEFLi5UkSVJ/aKYaqV549I4v3E5KqalAqSgGSpKkbtXu4GkFsHPF5Z2ABzdw/qXAxxq5bUR8EvgklFrtWlmsJEmSuletMOmE/XacdTzvaqRSIVLxv2ZmHchtoCRJ6mbtnvE0Qmm4+CJgJaXh4qdHxI8rztkjIu4uf38s8L6IWJhSei7wedYPF78O2MPh4pIkSf2l2VlJY6PDnLj/DnzptpWsnsi/TS0vRQzkNjiSJLVT1w4XB0gpHQN8BBgGlkTEP6SULgCWRcSVKaWPAi8BJoBfA2+ZDqZSSu8BzgAmgbdFxDUbeiyDJ0mSpM7Ia1YSdN9ObvU4kFuSNCi6Onhqp74InpYvhesugCdXwNydYNF5sM/JnV6VJEnqM80GRa2ER90YKGUJjwyUJEmDzuCprOeDp+VL4aq3wsT4+mOjY3DsRYZPkiSpKfUCpnMvv7OpoGg4QUqJyTYO0t6Q4ZRYW+P327yrkWodM1CSJA0Kg6eyng+ePrw3PPnA7ONzd4a3/6j965EkSV2rlblIm44O8eunJjq19KZVh0nTwVGt52g1kiRJ+TF4Kuv54On8edTeMSXB+U+0ezWSJKlggzgXqZZWqpNq7WpnoCRJUr4Mnsp6Pniy4kmSpL7QbDVSr8xFqsVZSZIk9S+Dp7KeD55qzXgCOOxsOOLds891CLkkSR1XHTL1QjXSvLFR1kxONRV6OStJkqTBY/BU1vPBE8wMlLbaASZWw5zN4A03wmbz159Tawj580+Hu7+RLYwyuJIkaYOarVrqNrXmIn3gxOcBze1gZ3gkSdLgMXgq64vgqdrKW+FTL4GRTWBydSkkevr3ML5q47ed3hEPZoZMe7wM7vh88bvn1Qq3qtdi4CVJaqNGZipl2fWtk1qdiyRJkpSVwVNZXwZPy5fCFW+CqSZ3nhndHKYmYe2ajZ87Nh/mbL7xUChLoFQr3BoahZRg7dMV62uwUkuSpIyabYHbZGSIkeHE79cUX8XkXCRJktQLDJ7K+jJ4qjdwvB1qhUJZA6VZv0q3+LjTYZRVVJKkGrKETO3USjUS2NomSZK6i8FTWV8GT+fPI0uAMwUM1TgelH757XnTYZRVVJI0UHphzlKtGUpWI0mSpH5i8FTWl8FTvYqnqra4/3j8OZw0fCObpfUBzFMxh9XMYX76XY07bqEiqSdUPT/DKEnqKs0GSu2as5R11zdnKEmSpEFg8FTWl8FTvR3sqgaBH7z4el7wm2/yrpGl7JAe58HYhg9Olq5fPPrpGYHUeMzh9m1ezm6rvsP28RiPpG3ZenSCTSaeyHnxVeFP3i15ra7HMEqS2qLZWUtFyNoC18iub4ZMkiSp3xk8lfVl8AS15xpVhSNX/HAlZ39pOWsmp9Yd22RkiKkIjo5vzwqkrpw6ZMbtT5rzPRaPfpqRtasrjtYLhTIESvVCHdj4EPJ6j5uGIQpuozCMkqSWdNOsJVvgJEmS8mHwVNavwVOtdoRavxC/6lPf57s/X0WCdee9/bLbM9cTvXaLmzl/8y9tOBTKGig1EtZUB2sbetyiB5vXUsRzlqQekqUtbvrYuZff2dUhk4GSJElS4wyeyvoxeLrihytn/RI/3QJQ+cvzU09P8sJ/uI6X7vUMPnTKvuuOH7z4elY+MU5WO84bm/kL+vB3cw1XsoZodau8suxq10gVVbNaqfIyoJLUxZptixsdTgylNKPyNg+t7A5nyCRJkpQPg6eyfgye6gVHO84b47vn/Mm6y19Y9gDv/OJyLjvzIF64+zbrjtcKrrJGMK3+Ip/lw0thbQ9ZqqgKmS+VsQ3x2ItK3xtISeqgLH9PFzWJr5VACWyNkyRJaieDp7J+DJ52O+erdacs/XLxy9ddPulj32PVU09z3VmHkVKacW4r8zaabV2oFXhltaFBry19sOhYGFXD2HyYHC++jVHSQGp2x7iiOGtJkiSptxk8lfVj8JSl4unuh3/LSz98I+8+5o8489BnZ7rf6g8ljbTj1foAUR0KNdril0V1lde0zO17tXRTGFXrsWzpk7QR3bRj3LyxUdZMTmWqbDVQkiRJ6h0GT2X9GDx9+dYVnPXFO6h82YYS/PNJz+dPX7ATAH939V38x/fv5fvnLmLbLTZp6nFaDYqqQ6F6lVqtSMCHT9k31/a9Wsd2fOBqdr7tQraPx3gkbcvvd1nEsx/8SvuHmjeikYDKMErqCc1WLbWhiXiDlanVazRkkiRJ6n0GT2X9GDwtveUB3vWl5Wy92ShPPDXB3LFRnhif4J1HPoc3H/EHrJlcy0H/9zpe9Oxt+PdXvaDpx2llFtS06cHkz5y7KY//bg1P1/iX9er7bPQxhhJMNfFHeGQIUkozB+HWGo5b58PU+3f7MYfe/7F1YdQD+78TYFZAtcuKKxhZu3rdbdemEYaHhmYHQiNjML6q8SfSsKr/w4ZRUlfqpqol2+IkSZJULffgKaW0JzA3Im4qXx4D/j9gL+C6iPiXFtZbmH4Lnp4cn+BP/ul/2G3bzfnCG19ESomI4K2X3s5VdzzItlvM4bHflQKNNx62O+ccvWdLj5f3kNnqkKjWh5esH642HR0iAeMT+e6W1Kx6odUrhr7D/xm6jB3S4zwY2/ARTuXAXefPCq0O2HVrJr/yNzNCqqD0/7d4hlFS3rJUJ23oWLMz8VrV7Bw/SZIkDZYigqcbgO9FxHvKl/8NeC3wbeBQ4H0RcWHTKy5IvwVP51/5Yz73/Xu56i2HsPeOc9cdv/Tm+zj38h9VfVgY4gMn7pP7h4NWBpPPGxtl801GNvrhJesHtrdfdnunGtlaUu+D3erbLuVtXLoupPoW+/HK4RuZE2vWnVuzYqodLX2GURLQfLtb7XAaqK68HE4MpcSayXxD9VZ2jDNkkiRJUrUigqdHgddFxNUppVHgMeAdEfGplNLbgDdERGvlNQXoh+Cp8kNOAAc/ez7/9VcvmnFOloHj7VrjhgaTV++816p6z7vV9r1OSAlqvRWPH/oO7xxZusGKqY7NnNpQGFU9pN2QSl0uz0Cp03/n2BonSZKkohURPD0FHBURN6aUXgzcADwjIh5PKR0KfD0iNmtp1QXo9eCp1pylTUeHWFxVyVRvcHfeQU9W7QrCav3/aaV9L+uxTn+orFZv5tQBu27d/h36psOo6sexYkodkmeg1GlWLUmSJKlbFBE8/Qj4j4j4YErpQ8CLI+KA8nUnAh+LiGe0sugi9HrwlDXA6XTFU7V6gdAHTnxe4a1/rbbvZTnWSpBVL+Zpdkh6PXU/fA5/tw1hVL3bO0tK+aj3vu+mgdytmDc2yprJqaZ35zRkkiRJUtGKCJ5eD3wMuAPYj1Lb3f9fvu4iYM+IeGnzSy5GrwdPWSuZ2hn0ZJU1EOpVzQZZtT4IT3+gzHsL9KztNjs+cPWs3fhmte4VVudlGDWoingPffHWFazukg0HslQn1atY+sCJzwMMlCRJktS9cg+eynd6KHAAcHtEXFdx/Hzgloj4ahNrLVSvB0+NVDL1e9DTT1qp1sg7jKr3wbe6de/3uyxilxVXZNt1Lw1DtLIbl2FUL8t3+Hb3VSe10u4G2aos/btbkiRJ3a6Q4KkX9Xrw1I2VTGqvdoRRWYwMwbFD3+X/DF22btD5DbEffzb0LTZL6weYj8ccbt/m5ez7+FcZqzg+FaV2wmZVh1yTw5ty304nsPl917F9PMojaTse2P+drNz5FX6wz0Ge1UjdGB7Vkneg5J8xSZIk9bPCgqeU0h8COwGbVl8XEV/LfEdt0uvBE1jJpNm6JYwCOG7oO7yrYte9D06ezFVTh3Bs1fHrpvbllcM3zgipWg6jorRx37SnYg5fmjqMI9IP2SE9xoOxLf88dQpAOTArHfsIp7Lp/qe2VI3SysywomeQtfpcsoZHI0OQUuqaQGk4JdbW+HlmoCRJkiTlr4gZT3sBlwF7Ubu7JiJiuKFVtkE/BE9SFt0URtVTHVK1I4xaE8MkEnPS5LpjT8Ucvrj2UP5k6PYZAdXVcUim+Tu15nE1Mr+n6F0Xsx7bdHSIY573TL66/FesmeyOuUhZ1ZtfVm/2k4GSJEmSlK8igqdvA9sD7wLuAp6uPici7mtwnYUzeNIga7aSpdWAql7lSS3tCKNqqb7Pp2IOX1h7KIsqwqgPTp7MlVOHzLpt3jsQDqpWhm/X3LGxTiWagZIkSZKUvyKCp98Bp0bE1a0urp0MnqSZih783EhFUL2AqxfDKK3n8G1JkiSp/xURPN0B/F1EfLHVxbWTwZPUnFZnGOUdcL1i6DsbHWxeRBhVa47Uuyf/kqmgHI71diDVylwkZyVJkiRJg6uI4OklwAeBkyLiFy2ur20MnqTu1krAteMDV7PzbReyfTzGI2lbfr/LInZZcQUja1evu/+1aQRIDMfEumPVO+Q1anXaFKbWsmlaf5/1BptXz42qV+XVbNDTyrE85iLVOmagJEmSJPW/XIKnlNItzPwstAuwNXAv8ET1+RFxYMMrLZjBkzRgli+F6y6AJ1fA3J1g0Xml45XH9ngZkz/8rxkBVathVK37mBzelPt2OoHN77uO7eNRHknbceOCv+Z9v3xurkFPK8eciyRJkiSpGXkFT5+lgRnDEfG6rOe2i8GTpJqqA6o9XgZ3fB4mxitOKmAfwNExfr7D8TPCqAf2fycH7Lr17MBsn5PzfWxJkiRJyknurXa9yuBJUmZZwqjRMRgZg/FV+T3u0GhpkNTais1CR8fg+afD3d8wjJIkSZLUdYqY8bQlsEVEPFTjumcBv42I3zW80oIZPElqSb3WvaveWnx1VPV9GkZJkiRJ6hJFBE9LgScj4q9qXPcJYG5EnNrwSgtm8CSpEJ1q1TOMkiRJktQFigiefgW8MSKuqHHd8cDHImKHhldaMIMnSW1jGCVJkiRpQDQSPI1kvM+5wFN1rltNabc7SRpc+5w8O9xZcNDGw6haM54aCqiqzpsYh2VL1h9/8oFSa+D9NxlGSZIkSWq7rMHT3cDLgW/UuO4Y4Oe5rUiS+kWWMGp6blSu1VIZw6jpNUqSJElSQbIGT/8CfDyl9DTwWeAh4FnAa4A3A39dyOokqd/UCqOmj1fKUi3Vahh13QWl76uDMMMoSZIkSTnJNOMJIKX0XuBcYNOKw6uBv4uIxQWsrWXOeJLUV4qYIzU0ClMT6y87I0qSJEnSRuQ+XLzijucCfwzMBx4Hvh8RTza1yjYweJLU99ox1LxeGAVWS0mSJEkDqLDgqdcYPEkaSFnCqNGxqnCqQbWGoo+OwbEXGT5JkiRJfa6Q4CmltDvwTuAQShVPq4BvA/8UEb9ocq2FMniSpLLqMGrReeXLD+T7OGPzYc7mVkFJkiRJfSz34Cml9ALgBkozna4GHgaeQWmnu02BIyLitqZXXBCDJ0nagOVLS7vb5dmWV82ZUZIkSVLfKSJ4ugEYAo6OiKcqjm8GfA2Yiog/aXK9hTF4kqSNaMeMqGqGUZIkSVJPKyJ4+j1wckR8tcZ1rwAui4jNG15pwQyeJKkJWcKoWjOeWmEYJUmSJPWMRoKnkYz3OQ5sU+e6+ZRa8CRJ/WCfk2cHPgsOmj0jCmYee/r3ML6qucecGIdlF6+//OQDpTbA+28yjJIkSZJ6WNaKp88BL6VU9fSdiuOHAJcB34yI12Z6wJSOAj4KDAOfjojFVdefBfwlMAk8CpwREfeVr1sL3Fk+9f6IOG5Dj2XFkyS1kTOjJEmSpIFQRKvdNsBXgBdRCoMeBrYv//c94ISIeDzD/QwDP6MUYq0AbgFOi4i7Ks45AvhBRDyVUvpr4PCIOKV83e8iYossTwwMniSp7To1M+rYi0rfV1dlGUhJkiRJucs9eKq446OAA4BnAQ9RCoi+0cDtXwScHxFHli+fCxARH6hz/n7Av0bEweXLBk+S1GvaEUZtMhemnp55n1ZHSZIkSYUoYsYTABHxdeDrTa2qZEfggYrLK4AXbuD81wPXVFzeNKW0jFIb3uKIuKL6BimlM4EzARYsWNDCUiVJucgyM6rVMGrNk7OPTYzDsiXr72N6btT0miRJkiQVrqHgKaX0MuBAZlY8fbORu6hxrOanipR/fcX6AAAgAElEQVTSq4GFwGEVhxdExIMppd2B61NKd0bEz2fcWcQngU9CqeKpgbVJktqlHWEUzD53YhyuOduWPEmSJKlNMgVPKaUdgC9TarN7pPzf9sAF5QqkP42IlRnuagWwc8XlnYAHazzeS4D3AIdFxJrp4xHxYPnrL1JK/wPsB/y8+vaSpB7UbBg1OgYjY9l31Btftf5cd8+TJEmSCpV1uPjVwD7AqRHxvYrjBwOXAMsj4hUZ7meE0nDxRcBKSsPFT4+IH1ecsx/wReCoiLi74vjWwFMRsSaltC3wfeD4ysHk1ZzxJEl9qHpm1KLzSsfz3FHP+VCSJElSXUXsavcUcEZEXFrjutOBT0XE5hkXdwzwEWAYWBIR/5BSugBYFhFXppT+G3gepVY+gPsj4riU0h8DnwCmgCHgIxFx8YYey+BJkgZIpiHmjagKrgyjJEmSJKCY4OmXwNsi4is1rvtTSiHQLg2vtGAGT5I04KrDqKd/n70lr6YaYdSxFxk+SZIkaaAUETz9FfAW4OURsaLi+E7A1cC/l4d6dxWDJ0nSDMuX5tuSBzA2H+ZsbhWUJEmSBkYjwVPWXe1eBmwD/DyldBvrh4vvX/7+JeWB4AAREac0uGZJkoo3HQjluXuew8olSZKkurJWPN3QyJ1GxBFNryhHVjxJkjLJNB/KYeWSJEkSFNBq16sMniRJTWvHsHLnQ0mSJKkHFRo8pZQS8CzgkYiYbGJ9bWPwJEnKVd7Dyp0PJUmSpB5UxIwnUkrHAO8D9i3f7gDgtpTSp4BvRcR/NrNYSZJ6xj4nzwyGWh1WXms+1LTKgMtASpIkST1qKMtJKaW/AK4E/hc4k9Jv1dN+Brw+/6VJktTl9jm51C43d2cglb4uPKPURjdDqnXr2SbG4eqz4Mq3loIoYn0gtXxpzouXJEmSipd1uPhPgcsj4tyU0jAwASyMiNvKlVCfiYhnFLzWhtlqJ0nqiNznQ2FbniRJkrpGEa12uwDfrHPdamCrjPcjSVL/q27JA1hwUGvzoWq15d1/kzvlSZIkqatlDZ4eAPYDrq9x3ULgntxWJElSP8oyH2p0DEbGsgVSE+OwbAnr5klVzogyfJIkSVKXyDTjCbgYeF9K6dXA9OCKlFJaBLwL+FQRi5MkqW/Vmg917EVw9D/WmBFVT1W7/MQ4XHM2fHhvOH9e6auzoSRJktRBWWc8JeBfgTcCaylVSk0Aw8AnIuLNRS6yWc54kiT1pOoZUY225VUaHYPnn25LniRJknLTyIynTMFTxR0/G1gEbAusAq6PiJ81tco2MHiSJPWFWm15JGZVPGU1OlaqrjJ8kiRJUhMKC556jcGTJKlv5L1TnrvkSZIkqUkGT2UGT5KkvmZLniRJkjrA4KnM4EmSNFBabsmrOteWPEmSJNXQSPCUdVc7SZLU7WrtlLfwDHfJkyRJUsfUrXhKKS0AHoqIifYuKT9WPEmShC15kiRJylUurXYppbXAiyLi5pTS9cCbIuJ/c1xn4QyeJEmqwZY8SZIktSCvVrtxYLPy94cDW7W4LkmS1A1syZMkSVKbjGzguh8CH00pfbN8+W9SSg/VOTci4ux8lyZJkgqzz8mzK5QWHNR8S974qvXnPvlAqaJq+nEkSZI0sDYUPP0VcCFwPKV/2lwErKlzbgAGT5Ik9bLqMKqVlryJ8VKIBTPDLOdBSZIkDZS6M55mnJTSFHBQRNxc/JLy44wnSZJaVD2YfI+XwR2frwqjNmBoFKYq9ilxOLkkSVLPa2TG04YqnirtBtRrs5MkSf2q1Za8qarNcSfGYdkS1lVN2ZYnSZLU1zJVPAGklEaAPwMOAeYDq4BvA5dHxGRhK2yBFU+SJLVBrZa80bHsVVEAY/NhzuZWQUmSJPWAvHa1q7zD7YFlwCXAy4Hdy18vBW5JKW3X5FolSVKvq7VL3rrLGY2vKlU/EeuroNwZT5IkqedlnfH0n8BhwIkRcUvF8QOALwHfiog/L2yVTbLiSZKkDmplODlYBSVJktSlipjxdAzwlsrQCSAibkkpnQv8S4NrlCRJ/W46JGp2OPn4qvWzo6aroO6/ycHkkiRJPSRr8LQJ8Ns61/0WmJPPciRJUl9pdTh5JQeTS5Ik9ZysrXbXUQqfjoyI31cc3xz4BjAeES8pbJVNstVOkqQeULMlrwG25EmSJLVVEa12/we4AXggpfQN4GFge+BISsMaDm9inZIkSbVb8rJWQUHtlrzK+5UkSVLHZKp4AkgpbQu8AzgAeBbwEPAD4EMR8VhhK2yBFU+SJPUoB5NLkiR1rSIqniiHS+c0vSpJkqSsHEwuSZLUFzIHT5IkSW3lYHJJkqSel7nVrhfZaidJUp9zMLkkSVLbNdJqN1T0YiRJkgqzz8lw7EUwd2cglb6Ozc9++/FVpeonYn0V1PKlRa1WkiRp4FjxJEmS+ouDySVJkgqV63DxlNImlHazuzoi7mh1cZIkSYUqYjD5tMr7NJCSJEnaqEwVTymlp4CjI+JbxS8pP1Y8SZKkdZYvbW4wOcCm82DtmpnB1ehYqc3P8EmSJA2YRiqesgZPNwBXRcSHWl1cOxk8SZKkulodTA625UmSpIGUa6td2buAz6eUnga+BjxM1aCEiHiqoVVKkiR1Uq2WvEaqoKB+W57hkyRJEpC94mmq4mLNG0TEcF6LyosVT5IkqSG1qqBGx2BkLHsgZRWUJEnqc0VUPJ1B5q1gJEmSelStKqhF55WOZW3LswpKkiRpnUwVT73KiidJkpSbVoaTWwUlSZL6SO7DxSvueC/gBcDOwJKI+FVK6Q+AhyPit02ttkAGT5IkqTCtDCcfHYPnnw53f8MwSpIk9ZzcW+1SSlsAS4CTgIny7b4O/Ar4v8D9wDuaWq0kSVIvamU4+cQ4LFvCukkGtuRJkqQ+lXW4+CeBY4A/B74LrAYWRsRtKaXXAu+IiL2LXGgzrHiSJElt1UoVFNiSJ0mSekIRw8VPBP42Im5IKVXvXncfsEsjC5QkSepLrVRBQe3B5PffZEueJEnqWVmDpzHg8TrXbQmszWc5kiRJPW6fk2cGQzWroBKZNgy2JU+SJPW4oYzn3QL8RZ3rTgK+l89yJEmS+sw+J8OxF8HcnYFU+rrwjNKA8UyqAqqJcbjmbPjw3nD+vNLX5UvzXrUkSVIuslY8vRf475TSfwNfoPQb0DEppbdTCp4OzfqAKaWjgI8Cw8CnI2Jx1fVnAX8JTAKPAmdExH3l615TXgvA30fE57I+riRJUsdUV0EBLDgo35a86ceRJEnqIpmGiwOklA4GFgMHUQqNArgJeFdEfDfjfQwDPwNeCqygVEl1WkTcVXHOEcAPIuKplNJfA4dHxCkppfnAMmBh+bFvBV4QEb+u93gOF5ckST2jlZY8cDC5JElqmyKGi1MOl16cUhoDtgaeiIinGlzbgcA9EfGL8kIvBY4H1gVPEXFDxfk3Aa8uf38k8M2IWFW+7TeBo4BLGlyDJElS96k1mHyPl8Edn8+2S56DySVJUhfKHDxVWA1MAM3sE7wj8EDF5RXACzdw/uuBazZw2x2rb5BSOhM4E2DBggVNLFGSJKlD8mzJmxiHZRevv2xLniRJ6oCsw8VJKR2TUvoepeDpV8DqlNL3Ukovb+DxUo1jNevHU0qvptRWd2Ejt42IT0bEwohYuN122zWwNEmSpC60z8nw9h/B+U+Uvh79jw0MJq/iYHJJktRmmSqeUkpvAP4duA74W+ARYHvgRODKlNKbIuITGe5qBbBzxeWdgAdrPN5LgPcAh0XEmorbHl512//Jsn5JkqS+UaslL4/B5NX3aVueJEnKQabh4iml+4CvRcRf17ju48AxEbHRvraU0gil4eKLgJWUhoufHhE/rjhnP+CLwFERcXfF8fmUBorvXz50G6Xh4nV/y3K4uCRJGgitDibfZC5MPT3z9qNjcOxFhk+SJGmWRoaLZ2212wa4vM51XwLmZ7mTiJgE3gJcC/wEWBoRP04pXZBSOq582oXAFsAXUkq3p5SuLN92FfB3lMKqW4ALNhQ6SZIkDYx9Ti6FRHN3BlLp68IzsrfkrXly9gBz2/IkSVIOslY8XQXcERHvrXHd3wP7R8QxBayvJVY8SZKkgbZ8afMtebWMjsHzT3enPEmSBlwjFU91ZzyllPaquHgR8OmU0jbAFayf8fSnwNHAXza/XEmSJBWiepe8Wi15o2MwMtbATnlLWNfC5055kiRpI+pWPKWUppg5GKByV7movhwRw/kvrzVWPEmSJFWproJadF7p+KwZUQ0Ymw9zNrcKSpKkAZFLxRNwRE7rkSRJUreoroKqlOdOefffZEueJEnKNuOpV1nxJEmS1KRWd8qrPtdd8iRJ6htF7GpXeecjKaXNqv9rfJmSJEnqWq3ulFcdULlLniRJAynrrnZzgQ9QGia+HTPnOwHgjCdJkqQBkOdOee6SJ0lST8prxlOlzwKHAZ8C7gGebm5pkiRJ6mlZdsrL2pLnLnmSJPW9rMHTIuANEXFJkYuRJElSj5kOiCqroPZ4Gdzx+Yy75NVpyaveec8gSpKknpQ1eLofeKrIhUiSJKlH1dopb8FB7pInSZIyz3g6Bng/8GcRcX/hq8qJM54kSZK6hLvkSZLUN3Kf8RQRX0spvQS4J6V0L/BEjXMObGiVkiRJGhy25EmSNJAyBU8ppX8C3gbcgsPFJUmS1Ix2tORNM5CSJKkrZG21ewL4x4j4QPFLyo+tdpIkST2m1Za8TebB1JqZt7ctT5KkXDXSajeU8T6fAm5tfkmSJElSBvucXAqJ5u4MpNLXhWeUwqMs1jwxu3Vvui3vw3vD+fNKX5cvzX3pkiRptqwVT2cDBwCvjCw36BJWPEmSJPWJ5Uubb8mrZXQMnn+6O+VJktSE3IeLA9sCLwR+mlL6H2YPF4+IODv7EiVJkqQGVM+HqtWSNzoGI2PZAqmJcVi2hHUtfJUzogyfJEnKTdaKp19u5JSIiN3zWVJ+rHiSJEnqY9VVUIvOKx2fNSOqAWPzYc7mVkFJkrQBuVc8RcRurS1JkiRJylmtXfKm5blT3v032ZInSVKTMlU89SorniRJktTyTnnVnA8lSRpwuVc8pZTetLFzIuLfs9yXJEmS1FbTgVBlFdQeL4M7Pt9cS57zoSRJyizrjKepDVwdABExnNei8mLFkyRJkurKe6c850NJkgZEIxVPTbfapZTmAUcCZwOnRcRPm7qjAhk8SZIkKTNb8iRJyiT3VrtaIuIJ4LKU0lzgE8Dhzd6XJEmS1HGZW/IyhlET47Ds4vWXHVYuSRpATQdPFX4JZEq5JEmSpK5Wa6e8BQc5H0qSpCa1tKtdSulZwGeAHSPiebmtKie22kmSJKkQzoeSJA2wIna1e5TZ9cRzgC2B1cCJDa1QkiRJ6mXVlVGtzocaX7U+uKqsgoKZAZeBlCSpx2Rttfs3Zv/UXA2sAL4eEY/nuipJkiSplxQxH+qqt0GshcnVpWPOiJIk9aCWWu26na12kiRJ6qjqlrxW5kOtUxVeuXueJKnN2rKrnSRJkqSNyDKsvOH5UFX/cOzAcklSF6tb8ZRSur6B+4mIWJTPkvJjxZMkSZK6Xq35UKNjMDLmwHJJUlfKq+Ipy9ymZwF/TOapiZIkSZJmqDUfatF5pWN5Dyx3PpQkqc3qBk8R8cp616WUFgBnA68AHgM+nP/SJEmSpAFRqyVvWp4Dy5ddvP6yYZQkqQ0aGi6eUvoD4Fzg1cAjwD8Dn4iIVqYjFsZWO0mSJPWd3AeWO6xcktSYRlrtMgVPKaXnAu8BXgk8AHwQWBIRT7ey0KIZPEmSJGkgVIdRDQ8sr2YYJUmqL7fgKaX0AkqB0/HAz4DFwH9GxNo8Flo0gydJkiQNpFoDyxuZD1VTjTDq2IsMnyRpADUSPA1t4E6uAW4GdgNOjYg9I+JzvRI6SZIkSQNrn5NLodDcnYFU+rrwjFJYNENq4E6rQquJcbjmbPjw3nD+vNLX5UtbXLgkqd/UrXhKKU2Vv10FTNU8qUJEbJ/junJhxZMkSZJUIdN8qBYqo2zJk6SB0EjFU91d7YD357QeSZIkSd2g1u55Cw4qfue8aZWPYyAlSQOhoV3teo0VT5IkSVIT8t45b86WMDUJkxW3tzpKknpW7rva9SqDJ0mSJCknue+cB+6eJ0m9yeCpzOBJkiRJKkghO+fVuA/DKEnqOrnsaidJkiRJdWXdOW90DMbmN3DHNXbPW7akNC+KWD83yh30JKknWPEkSZIkKT/VLXmLzisdz7s6amw+zNncKihJ6oC8drWTJEmSpMbU2jlvWl6750FpvtT0jKnpKqj7b7IlT5K6jBVPkiRJkjoj0+55LVRGOR9KkgrhcPEygydJkiSpx2QKo1pgGCVJLTN4KjN4kiRJkvpAdRj19O/Xt9nlwTBKkhpi8FRm8CRJkiT1oeVL8x9WXs0wSpLqcri4JEmSpP41Hf4UOR9qYhyWXbz+8vQA82nVO/cZSElSTVY8SZIkSeoPRQ8rB9hkK5iamHmfVkdJGjBd3WqXUjoK+CgwDHw6IhZXXX8o8BFgH+DUiPhixXVrgTvLF++PiOM29FgGT5IkSdKAa0cYVes+DKMk9bGuDZ5SSsPAz4CXAiuAW4DTIuKuinN2BbYC3gFcWRU8/S4itsj6eAZPkiRJkmYxjJKklnTzjKcDgXsi4hcAKaVLgeOBdcFTRNxbvm6qzWuTJEmSNAj2OXl22LPgoI2HUaNjMDLWwI56VcHVxDgsW7L++PTcqPtvMoyS1LfaHTztCDxQcXkF8MIGbr9pSmkZMAksjogr8lycJEmSpAGVJYxadF7peEs76hlGSRos7Q6eUo1jjdSvLoiIB1NKuwPXp5TujIifz3iAlM4EzgRYsGBB8yuVJEmSNNhqhVHTcm3VM4yS1L/aHTytAHauuLwT8GDWG0fEg+Wvv0gp/Q+wH/DzqnM+CXwSSjOeWlyvJEmSJM3UbKueYZSkAdTu4OkWYI+U0m7ASuBU4PQsN0wpbQ08FRFrUkrbAgcDHyxspZIkSZKUlWGUJNXU1l3tAFJKxwAfAYaBJRHxDymlC4BlEXFlSukA4MvA1sBq4FcR8dyU0h8DnwCmgCHgIxFx8YYey13tJEmSJHWVtu2oV8Hd9CTlrJFd7doePLWTwZMkSZKkrmcYJanHNBI8tbvVTpIkSZJUqS1telUmxmFZRQPJdJvetOrd/AykJDXJiidJkiRJ6gXtqIwa3Qym1sLaNRXHrI6SNJOtdmUGT5IkSZL6Wifa9KB+GAVWS0kDwOCpzOBJkiRJ0sDpVBg1NAopwdqn1x+zWkrqSwZPZQZPkiRJkkS2MGp0DEbGYHxVzg9eFXIZRkk9z+CpzOBJkiRJkuqoDqOmW+Wuemvx1VGGUVJPM3gqM3iSJEmSpAZ1qlXPMErqGQZPZQZPkiRJkpSDLGFUrRlPeQdUDjWXuoLBU5nBkyRJkiQVpF6rXrurpeoNNT/2otnrMZCScmHwVGbwJEmSJEkd1qnWvbH5MDk+e4C67XtSywyeygyeJEmSJKkLdWyOVI37NYySGmbwVGbwJEmSJEk9oqNhVBVnSUkbZPBUZvAkSZIkST2s2aHmo2MwMgbjq/JbS73HsVpKA8jgqczgSZIkSZL6TJah5tPHrnprGyqmbN3T4DF4KjN4kiRJkqQB1i3te7buqc8YPJUZPEmSJEmSZuiWMMrWPfWwRoKnkaIXI0mSJElS19jn5NlBzoKDmpsl1UpANTUx+9jEOCxbsv4+n3yg1C44zeoo9SArniRJkiRJqpZlllS7qqXmbAlTkzBZ8Ti276mDbLUrM3iSJEmSJBWqW1r3wPY9tY3BU5nBkyRJkiSp7bKEUXm37m1Qxp33wGopZWLwVGbwJEmSJEnqCs227o2OwcgYjK8qdn1WS6kBBk9lBk+SJEmSpJ5SL6C66q2dad/LWi1lGDVQDJ7KDJ4kSZIkSX2h69r3Kti6N3AMnsoMniRJkiRJfaubdt6rlkZhqEbr3rEXzV6jIVXPMXgqM3iSJEmSJA28bqqWmrMlTE3A5OoNP7YtfV3N4KnM4EmSJEmSpBq6uVqq3mPZ0tc1DJ7KDJ4kSZIkSWpBlmqptoZRVayW6giDpzKDJ0mSJEmSctZs697oGIyMwfiqNizSaqkiGTyVGTxJkiRJktQGWVr3po9d9dbu2I3PaqmmGTyVGTxJkiRJktRlun6+VMZqqX1Orv1cBiCkMngqM3iSJEmSJKlHddNufNWmw6jq9QxIS5/BU5nBkyRJkiRJfaTrq6VqPFa9lr5jL+rZ8MngqczgSZIkSZKkAdTN1VLT5u4Mb/9Rex4rZ40ETyNFL0aSJEmSJKmt9jl5djXRgoPyrZZKwxBrm1/jkyuav20PMXiSJEmSJEn9r1YYNX28UnVAVSuMqjfjqZGKqbk7NfoMepLBkyRJkiRJ0rSs1VL7nJwtpKo342m64qrPOeNJkiRJkiQpL1kGoA/QrnZWPEmSJEmSJOUla0vfgBjq9AIkSZIkSZLUnwyeJEmSJEmSVAiDJ0mSJEmSJBXC4EmSJEmSJEmFMHiSJEmSJElSIQyeJEmSJEmSVAiDJ0mSJEmSJBXC4EmSJEmSJEmFSBHR6TUUJqX0KHBfp9eRk22Bxzq9CHWMr//g8rUfbL7+g8vXfnD52g82X//B5Ws/2Hrx9d8lIrbLcmJfB0/9JKW0LCIWdnod6gxf/8Hlaz/YfP0Hl6/94PK1H2y+/oPL136w9fvrb6udJEmSJEmSCmHwJEmSJEmSpEIYPPWOT3Z6AeooX//B5Ws/2Hz9B5ev/eDytR9svv6Dy9d+sPX16++MJ0mSJEmSJBXCiidJkiRJkiQVwuCpB6SUjkop/TSldE9K6ZxOr0fFSSntnFK6IaX0k5TSj1NKf1s+fn5KaWVK6fbyf8d0eq0qRkrp3pTSneXXeVn52PyU0jdTSneXv27d6XUqXyml51S8v29PKf0mpfQ23/v9K6W0JKX0SErpRxXHar7XU8lF5d8DlqeU9u/cytWqOq/9hSml/y2/vl9OKc0rH981pTRe8XfAxzu3cuWhzutf9+/6lNK55ff+T1NKR3Zm1cpDndf+sorX/d6U0u3l4773+8gGPuMNzM99W+26XEppGPgZ8FJgBXALcFpE3NXRhakQKaVnAc+KiNtSSlsCtwInACcDv4uIf+roAlW4lNK9wMKIeKzi2AeBVRGxuBw+bx0RZ3dqjSpW+e/9lcALgdfhe78vpZQOBX4H/EdE7F0+VvO9Xv4Q+jfAMZT+XHw0Il7YqbWrNXVe+5cB10fEZErpHwHKr/2uwNXT56n31Xn9z6fG3/Uppb2AS4ADgR2A/wb+MCLWtnXRykWt177q+n8GnoyIC3zv95cNfMZ7LQPyc9+Kp+53IHBPRPwiIp4GLgWO7/CaVJCIeCgibit//1vgJ8COnV2VusDxwOfK33+O0g8q9a9FwM8j4r5OL0TFiYgbgVVVh+u914+n9EElIuImYF75l1j1oFqvfUR8IyImyxdvAnZq+8LUFnXe+/UcD1waEWsi4pfAPZQ+G6gHbei1TyklSv/QfElbF6W22MBnvIH5uW/w1P12BB6ouLwCg4iBUP6Xjv2AH5QPvaVcarnEVqu+FsA3Ukq3ppTOLB97RkQ8BKUfXMD2HVud2uFUZv7i6Xt/cNR7r/u7wGA5A7im4vJuKaUfppS+lVJ6cacWpcLV+rve9/7geDHwcETcXXHM934fqvqMNzA/9w2eul+qccz+yD6XUtoC+BLwtoj4DfAx4NnAvsBDwD93cHkq1sERsT9wNPDmclm2BkRKaQ5wHPCF8iHf+wJ/FxgYKaX3AJPAf5UPPQQsiIj9gLOAz6eUturU+lSYen/X+94fHKcx8x+dfO/3oRqf8eqeWuNYT7/3DZ663wpg54rLOwEPdmgtaoOU0iilv5D+KyIuB4iIhyNibURMAZ/CMuu+FREPlr8+AnyZ0mv98HR5bfnrI51boQp2NHBbRDwMvvcHUL33ur8LDICU0muAVwCvivIQ1nKL1ePl728Ffg78YedWqSJs4O963/sDIKU0ApwIXDZ9zPd+/6n1GY8B+rlv8NT9bgH2SCntVv6X8FOBKzu8JhWk3N99MfCTiPhQxfHKnt4/BX5UfVv1vpTS5uWBg6SUNgdeRum1vhJ4Tfm01wBf6cwK1QYz/sXT9/7AqfdevxL4i/IuNwdRGj77UCcWqGKklI4CzgaOi4inKo5vV95wgJTS7sAewC86s0oVZQN/118JnJpS2iSltBul1//mdq9PhXsJ8L8RsWL6gO/9/lLvMx4D9HN/pNML0IaVdzd5C3AtMAwsiYgfd3hZKs7BwJ8Dd05vpwq8GzgtpbQvpRLLe4E3dGZ5KtgzgC+XfjYxAnw+Ir6eUroFWJpSej1wP/DKDq5RBUkpbUZpB9PK9/cHfe/3p5TSJcDhwLYppRXA+4DF1H6vf43Szjb3AE9R2u1QParOa38usAnwzfLPgJsi4o3AocAFKaVJYC3wxojIOphaXajO6394rb/rI+LHKaWlwF2UWjDf7I52vavWax8RFzN7tiP43u839T7jDczP/VSu5JUkSZIkSZJyZaudJEmSJEmSCmHwJEmSJEmSpEIYPEmSJEmSJKkQBk+SJEmSJEkqhMGTJEmSJEmSCmHwJEmSlJOU0vkp/b/27ifEqjKM4/j3R2FBqFBghSAMlGDSYhgYUGJCadUyJFpFEoPrFqEGDg4udCEObaRZFLQWZKhFEQkjFToEIoE7ESIhhoYBdaHgn8fFOReG4YKinjODfT+re5/33Jfn3f54n3OztNZ9SJIkrRcGT5IkSZIkSeqEwZMkSZIkSZI6YfAkSZLUkyQjSeaS3ExyK8mPSd5a9cznSa4kuZ1kKcn5JDtXrB9OcjXJnSSLSX5O8kb/p5EkSXq0F9e6AUmSpP+DJC8B54C7wCRwD5gGzibsPAcAAAIISURBVCd5t6qWk0wA3wBTwAVgE7AL2Nzu8SnwFXAQuAK8BuwFXun3NJIkSY/H4EmSJKkf+4FtwPaqugaQZAG4BhwAjgPjwF9VdXzF735Y8Xkc+KWqTq+one20a0mSpKfgqJ0kSVI/xoFLg9AJoKquA38A77Wly8BokpkkE0k2rNrjMvBhkukk40le6KVzSZKkJ2TwJEmS1I83gcUh9UXgVYCq+pXmZtQEMA8sJTmdZDBK9x3NqN3HwAKwmOSYAZQkSVqvDJ4kSZL68S+wZUj9dWB58KWqvq+qsbb+JfAZcKRde1BVM1W1g2Zs7yRNEDXZbeuSJElPxuBJkiSpHwvAWJKRQSHJVmA38Pvqh6vqv6qaBX4D3hmy/k9VnQCuDluXJElaD3y5uCRJ0rO1Icm+IfU5mn+j+ynJFHAfOAosAbMASaZpxu7m2/oo8D5wqF2fpbkddRG4AewB3m73lSRJWncMniRJkp6tjcCZIfU9wAfAKeBbIDQB00dVNRi1+xP4Avik3edvmnDq63b9As1Y3QHgZZrbTpNVNdfBOSRJkp5aqmqte5AkSZIkSdJzyHc8SZIkSZIkqRMGT5IkSZIkSeqEwZMkSZIkSZI6YfAkSZIkSZKkThg8SZIkSZIkqRMGT5IkSZIkSeqEwZMkSZIkSZI6YfAkSZIkSZKkThg8SZIkSZIkqRMPAcYxqQKL2RvMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize= (20,5))\n",
    "ax.plot(avg_valids,marker='o',label='Val')\n",
    "ax.plot(avg_trainings,marker='o',label='Train')\n",
    "ax.set_xlabel('Loss',fontsize=15)\n",
    "ax.set_ylabel('Number of epochs',fontsize=15)\n",
    "ax.set_title('Loss',fontsize=20,fontweight =\"bold\")\n",
    "ax.legend()\n",
    "#fig.savefig(\"Loss_graphWithFC2Wit.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richard/anaconda3/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/richard/anaconda3/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type LSTMEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(classifier, 'SiameseNNFC1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
