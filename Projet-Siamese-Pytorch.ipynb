{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load data\n",
    "with open(\"data.txt\", \"rb\") as fp:   # Unpickling\n",
    "    df = pickle.load(fp)\n",
    "X = df[['left','right']]     \n",
    "Y = df['target']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperate to training, validation, and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 64)\n",
    "validation_size = int(len(X_train) * 0.1)\n",
    "training_size = len(X_train) - validation_size\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=validation_size,random_state= 64)\n",
    "Y_test = Y_test.values\n",
    "Y_train = Y_train.values\n",
    "Y_validation = Y_validation.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 1440\n",
      "Validation size: 160\n",
      "test size: 400\n"
     ]
    }
   ],
   "source": [
    "print('Training size:',X_train.shape[0])\n",
    "print('Validation size:',X_validation.shape[0])\n",
    "print('test size:',X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check shape\n",
    "assert X_train['left'].shape == X_train['right'].shape\n",
    "assert len(X_train['left']) == len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two help function\n",
    "def one_hot(s):\n",
    "    nb_digits=201\n",
    "    batch_size = s.shape[0]\n",
    "    seqlen = s.shape[1]\n",
    "    s_onehot = torch.FloatTensor(batch_size,seqlen,nb_digits)\n",
    "    s_onehot.zero_()\n",
    "    s_onehot.scatter_(2, s.unsqueeze(2), 1)\n",
    "    return s_onehot\n",
    "def padding(data):\n",
    "    left = [] \n",
    "    maxlen= 50\n",
    "    for i in range(data.shape[0]):\n",
    "        diff = maxlen - len((data.iloc[i]['left']))\n",
    "        if diff>=1:\n",
    "            data.iloc[i]['left']+= [0]*diff\n",
    "        left.append((data.iloc[i]['left']))\n",
    "    right = [] \n",
    "    maxlen= 50\n",
    "    for i in range(data.shape[0]):\n",
    "        diff = maxlen - len((data.iloc[i]['right']))\n",
    "        if diff>=1:\n",
    "            data.iloc[i]['right']+= [0]*diff\n",
    "        right.append((data.iloc[i]['right']))\n",
    "    return torch.tensor(np.array([right,left])).transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding and creat the loaders\n",
    "X_train = padding(X_train)\n",
    "Y_train = torch.FloatTensor(np.array(Y_train))\n",
    "train_dataset  = Data.TensorDataset(X_train,Y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32,shuffle=True)\n",
    "\n",
    "X_validation = padding(X_validation)\n",
    "Y_validation = torch.FloatTensor(np.array(Y_validation))\n",
    "val_dataset  = Data.TensorDataset(X_validation,Y_validation)\n",
    "\n",
    "X_test = padding(X_test)\n",
    "Y_test = torch.FloatTensor(np.array(Y_test))\n",
    "test_dataset  = Data.TensorDataset(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    \"\"\" Implements the network type integrated within the Siamese RNN architecture. \"\"\"\n",
    "    def __init__(self, opt, is_train=False):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.node_size = opt['node_size']\n",
    "        self.name = 'sim_encoder'\n",
    "        self.hidden_size= opt['hidden_size']\n",
    "        self.num_layers= opt['num_layers']\n",
    "        self.embedding_dim = opt['embedding_dim']\n",
    "        self.embedding_table = nn.Embedding(num_embeddings=self.node_size, embedding_dim=self.embedding_dim,\n",
    "                                          padding_idx=0, max_norm=None, scale_grad_by_freq=False, sparse=False)\n",
    "        self.lstm_rnn = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.hidden_size, num_layers=self.num_layers)\n",
    "        self.fc1= nn.Linear(self.hidden_size,16)\n",
    "        self.fc2= nn.Linear(16,2)\n",
    "        \n",
    "    def initialize_hidden_plus_cell(self, batch_size):\n",
    "        \"\"\" Re-initializes the hidden state, cell state, and the forget gate bias of the network. \"\"\"\n",
    "        zero_hidden = torch.randn(self.num_layers, batch_size, self.hidden_size)\n",
    "        zero_cell = torch.randn(self.num_layers, batch_size,self.hidden_size)\n",
    "        return zero_hidden, zero_cell\n",
    "\n",
    "    def forward(self, input_data, hidden, cell):\n",
    "        \"\"\" Performs a forward pass through the network. \"\"\"\n",
    "        output = self.embedding_table(input_data)\n",
    "        output, (hidden, cell) = self.lstm_rnn(output, (hidden, cell))\n",
    "        output = nn.functional.relu(self.fc1(output[-1]))\n",
    "        output = self.fc2(output) \n",
    "        return output, hidden[-1], cell[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseClassifier(nn.Module):\n",
    "    \"\"\" Sentence similarity estimator implementing a siamese arcitecture. Uses pretrained word2vec embeddings. \n",
    "    Different to the paper, the weights are untied, to avoid exploding/ vanishing gradients. \"\"\"\n",
    "    def __init__(self, opt, is_train=False):\n",
    "        super(SiameseClassifier, self).__init__()\n",
    "        self.learning_rate= opt['learning_rate']\n",
    "        # Initialize network\n",
    "        self.encoder_a =  LSTMEncoder(opt, is_train)\n",
    "        # Initialize network parameters\n",
    "        self.initialize_parameters()\n",
    "        # Declare loss function\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        # Initialize network optimizers\n",
    "        self.optimizer_a = optim.Adam(self.encoder_a.parameters(), lr=self.learning_rate,\n",
    "                                      betas=(0.9, 0.999),weight_decay=0)\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\" Performs a single forward pass through the siamese architecture. \"\"\"\n",
    "        \n",
    "        # Obtain the input length (each batch consists of padded sentences)\n",
    "        input_length = self.batch_a.size(0)\n",
    "        \n",
    "        # Obtain sentence encodings from each encoder\n",
    "        hidden_a, cell_a = self.encoder_a.initialize_hidden_plus_cell(self.batch_size)\n",
    "        output_a, hidden_a, cell_a = self.encoder_a(self.batch_a, hidden_a, cell_a)\n",
    "\n",
    "        hidden_b, cell_b = self.encoder_a.initialize_hidden_plus_cell(self.batch_size)\n",
    "        output_b, hidden_b, cell_b = self.encoder_a(self.batch_b, hidden_b, cell_b)\n",
    "\n",
    "        # Format sentence encodings as 2D tensors\n",
    "        self.encoding_a = output_a.squeeze()\n",
    "        self.encoding_b = output_b.squeeze()\n",
    "\n",
    "        # Obtain similarity score predictions by calculating the Manhattan distance between sentence encodings\n",
    "        if self.batch_size == 1:\n",
    "            self.prediction = torch.exp(-torch.norm((self.encoding_a - self.encoding_b), 1))\n",
    "        else:\n",
    "            self.prediction = torch.exp(-torch.norm((self.encoding_a - self.encoding_b), 1, 1))\n",
    "            \n",
    "\n",
    "    def get_loss(self):\n",
    "        \"\"\" Calculates the MSE loss between the network predictions and the ground truth. \"\"\"\n",
    "        # Loss is the L1 norm of the difference between the obtained sentence encodings\n",
    "        self.loss = self.loss_function(self.prediction, self.labels)\n",
    "\n",
    "    def load_pretrained_parameters(self,pretrained_state_dict_path):\n",
    "        \"\"\" Loads the parameters learned during the pre-training on the SemEval data. \"\"\"\n",
    "        self.encoder_a.load_state_dict(torch.load(pretrained_state_dict_path))\n",
    "        print('Pretrained parameters have been successfully loaded into the encoder networks.')\n",
    "    \n",
    "    def save_lstm(self,path):\n",
    "        torch.save(self.encoder_a.state_dict(), path)\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\" Initializes network parameters. \"\"\"\n",
    "        state_dict = self.encoder_a.state_dict()\n",
    "        for key in state_dict.keys():\n",
    "            if '.weight' in key:\n",
    "                state_dict[key] = torch.nn.init.xavier_normal_((state_dict[key]),gain=1)\n",
    "            if '.bias' in key:\n",
    "                bias_length = state_dict[key].size()[0]\n",
    "                start, end = bias_length // 4, bias_length // 2\n",
    "                state_dict[key][start:end].fill_(2.5)\n",
    "        self.encoder_a.load_state_dict(state_dict)\n",
    "        \n",
    "    def initialize_parametersX(self):\n",
    "        for p in self.encoder_a.parameters():\n",
    "            nn.init.xavier_normal_(p)\n",
    "\n",
    "    def train_step(self, train_batch_a, train_batch_b, train_labels):\n",
    "        \"\"\" Optimizes the parameters of the active networks, i.e. performs a single training step. \"\"\"\n",
    "        # Get batches\n",
    "        self.batch_a = train_batch_a.transpose(0,1)\n",
    "        self.batch_b = train_batch_b.transpose(0,1)\n",
    "        self.labels = train_labels\n",
    "        self.batch_size = self.batch_a.size(1)\n",
    "        self.encoder_a.zero_grad() \n",
    "        self.forward()\n",
    "        self.get_loss()\n",
    "        #l2_reg = None\n",
    "        #for i in classifier.encoder_a.lstm_rnn.parameters():\n",
    "        #    if l2_reg is None:\n",
    "        #        l2_reg = W.norm(2)\n",
    "        #    else:\n",
    "        #        l2_reg = l2_reg + W.norm(2)\n",
    "        #self.loss += l2_reg\n",
    "        self.loss.backward()\n",
    "        clip_grad_norm(self.encoder_a.parameters(), 0.25)\n",
    "        self.optimizer_a.step()\n",
    "\n",
    "    def test_step(self, test_batch_a, test_batch_b, test_labels):\n",
    "        \"\"\" Performs a single test step. \"\"\"\n",
    "        self.batch_a = test_batch_a.transpose(0,1)\n",
    "        self.batch_b = test_batch_b.transpose(0,1)\n",
    "        self.labels = test_labels\n",
    "        self.batch_size = self.batch_a.size(1)\n",
    "        self.forward()\n",
    "        self.get_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richard/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:89: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/Users/richard/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:41: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training Batch: 10 | Average loss: 0.2856\n",
      "Epoch: 0 | Training Batch: 20 | Average loss: 0.2873\n",
      "Epoch: 0 | Training Batch: 30 | Average loss: 0.3047\n",
      "Epoch: 0 | Training Batch: 40 | Average loss: 0.2914\n",
      "Average training batch loss at epoch 0: 0.2916\n",
      "Average validation fold accuracy at epoch 0: 0.2762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richard/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:71: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Training Batch: 10 | Average loss: 0.2528\n",
      "Epoch: 1 | Training Batch: 20 | Average loss: 0.2726\n",
      "Epoch: 1 | Training Batch: 30 | Average loss: 0.2923\n",
      "Epoch: 1 | Training Batch: 40 | Average loss: 0.3119\n",
      "Average training batch loss at epoch 1: 0.2869\n",
      "Average validation fold accuracy at epoch 1: 0.2981\n",
      "Epoch: 2 | Training Batch: 10 | Average loss: 0.2918\n",
      "Epoch: 2 | Training Batch: 20 | Average loss: 0.3178\n",
      "Epoch: 2 | Training Batch: 30 | Average loss: 0.2890\n",
      "Epoch: 2 | Training Batch: 40 | Average loss: 0.2977\n",
      "Average training batch loss at epoch 2: 0.2906\n",
      "Average validation fold accuracy at epoch 2: 0.2905\n",
      "Epoch: 3 | Training Batch: 10 | Average loss: 0.2867\n",
      "Epoch: 3 | Training Batch: 20 | Average loss: 0.2841\n",
      "Epoch: 3 | Training Batch: 30 | Average loss: 0.2731\n",
      "Epoch: 3 | Training Batch: 40 | Average loss: 0.2738\n",
      "Average training batch loss at epoch 3: 0.2872\n",
      "Average validation fold accuracy at epoch 3: 0.2860\n",
      "Epoch: 4 | Training Batch: 10 | Average loss: 0.2902\n",
      "Epoch: 4 | Training Batch: 20 | Average loss: 0.2863\n",
      "Epoch: 4 | Training Batch: 30 | Average loss: 0.2913\n",
      "Epoch: 4 | Training Batch: 40 | Average loss: 0.2948\n",
      "Average training batch loss at epoch 4: 0.2879\n",
      "Average validation fold accuracy at epoch 4: 0.2854\n",
      "Epoch: 5 | Training Batch: 10 | Average loss: 0.2793\n",
      "Epoch: 5 | Training Batch: 20 | Average loss: 0.2785\n",
      "Epoch: 5 | Training Batch: 30 | Average loss: 0.2773\n",
      "Epoch: 5 | Training Batch: 40 | Average loss: 0.2828\n",
      "Average training batch loss at epoch 5: 0.2866\n",
      "Average validation fold accuracy at epoch 5: 0.2892\n",
      "Epoch: 6 | Training Batch: 10 | Average loss: 0.2888\n",
      "Epoch: 6 | Training Batch: 20 | Average loss: 0.2747\n",
      "Epoch: 6 | Training Batch: 30 | Average loss: 0.2795\n",
      "Epoch: 6 | Training Batch: 40 | Average loss: 0.2702\n",
      "Average training batch loss at epoch 6: 0.2858\n",
      "Average validation fold accuracy at epoch 6: 0.2882\n",
      "Epoch: 7 | Training Batch: 10 | Average loss: 0.2827\n",
      "Epoch: 7 | Training Batch: 20 | Average loss: 0.3027\n",
      "Epoch: 7 | Training Batch: 30 | Average loss: 0.3045\n",
      "Epoch: 7 | Training Batch: 40 | Average loss: 0.3044\n",
      "Average training batch loss at epoch 7: 0.2872\n",
      "Average validation fold accuracy at epoch 7: 0.2852\n",
      "Epoch: 8 | Training Batch: 10 | Average loss: 0.2733\n",
      "Epoch: 8 | Training Batch: 20 | Average loss: 0.2811\n",
      "Epoch: 8 | Training Batch: 30 | Average loss: 0.2594\n",
      "Epoch: 8 | Training Batch: 40 | Average loss: 0.2795\n",
      "Average training batch loss at epoch 8: 0.2858\n",
      "Average validation fold accuracy at epoch 8: 0.2840\n",
      "Epoch: 9 | Training Batch: 10 | Average loss: 0.2911\n",
      "Epoch: 9 | Training Batch: 20 | Average loss: 0.2642\n",
      "Epoch: 9 | Training Batch: 30 | Average loss: 0.2801\n",
      "Epoch: 9 | Training Batch: 40 | Average loss: 0.2743\n",
      "Average training batch loss at epoch 9: 0.2850\n",
      "Average validation fold accuracy at epoch 9: 0.2855\n",
      "Epoch: 10 | Training Batch: 10 | Average loss: 0.2806\n",
      "Epoch: 10 | Training Batch: 20 | Average loss: 0.2806\n",
      "Epoch: 10 | Training Batch: 30 | Average loss: 0.2865\n",
      "Epoch: 10 | Training Batch: 40 | Average loss: 0.3004\n",
      "Average training batch loss at epoch 10: 0.2850\n",
      "Average validation fold accuracy at epoch 10: 0.2834\n",
      "Epoch: 11 | Training Batch: 10 | Average loss: 0.2847\n",
      "Epoch: 11 | Training Batch: 20 | Average loss: 0.2706\n",
      "Epoch: 11 | Training Batch: 30 | Average loss: 0.2941\n",
      "Epoch: 11 | Training Batch: 40 | Average loss: 0.2919\n",
      "Average training batch loss at epoch 11: 0.2848\n",
      "Average validation fold accuracy at epoch 11: 0.2820\n",
      "Epoch: 12 | Training Batch: 10 | Average loss: 0.2594\n",
      "Epoch: 12 | Training Batch: 20 | Average loss: 0.2762\n",
      "Epoch: 12 | Training Batch: 30 | Average loss: 0.2771\n",
      "Epoch: 12 | Training Batch: 40 | Average loss: 0.2968\n",
      "Average training batch loss at epoch 12: 0.2844\n",
      "Average validation fold accuracy at epoch 12: 0.2821\n",
      "Epoch: 13 | Training Batch: 10 | Average loss: 0.2952\n",
      "Epoch: 13 | Training Batch: 20 | Average loss: 0.2814\n",
      "Epoch: 13 | Training Batch: 30 | Average loss: 0.2892\n",
      "Epoch: 13 | Training Batch: 40 | Average loss: 0.2852\n",
      "Average training batch loss at epoch 13: 0.2846\n",
      "Average validation fold accuracy at epoch 13: 0.2838\n",
      "Epoch: 14 | Training Batch: 10 | Average loss: 0.2685\n",
      "Epoch: 14 | Training Batch: 20 | Average loss: 0.2901\n",
      "Epoch: 14 | Training Batch: 30 | Average loss: 0.2796\n",
      "Epoch: 14 | Training Batch: 40 | Average loss: 0.2837\n",
      "Average training batch loss at epoch 14: 0.2843\n",
      "Average validation fold accuracy at epoch 14: 0.2830\n",
      "Epoch: 15 | Training Batch: 10 | Average loss: 0.2883\n",
      "Epoch: 15 | Training Batch: 20 | Average loss: 0.2743\n",
      "Epoch: 15 | Training Batch: 30 | Average loss: 0.2816\n",
      "Epoch: 15 | Training Batch: 40 | Average loss: 0.2707\n",
      "Average training batch loss at epoch 15: 0.2841\n",
      "Average validation fold accuracy at epoch 15: 0.2826\n",
      "Epoch: 16 | Training Batch: 10 | Average loss: 0.2769\n",
      "Epoch: 16 | Training Batch: 20 | Average loss: 0.2706\n",
      "Epoch: 16 | Training Batch: 30 | Average loss: 0.2860\n",
      "Epoch: 16 | Training Batch: 40 | Average loss: 0.2952\n",
      "Average training batch loss at epoch 16: 0.2840\n",
      "Average validation fold accuracy at epoch 16: 0.2830\n",
      "Epoch: 17 | Training Batch: 10 | Average loss: 0.2599\n",
      "Epoch: 17 | Training Batch: 20 | Average loss: 0.2772\n",
      "Epoch: 17 | Training Batch: 30 | Average loss: 0.2908\n",
      "Epoch: 17 | Training Batch: 40 | Average loss: 0.2862\n",
      "Average training batch loss at epoch 17: 0.2837\n",
      "Average validation fold accuracy at epoch 17: 0.2837\n",
      "Epoch: 18 | Training Batch: 10 | Average loss: 0.2741\n",
      "Epoch: 18 | Training Batch: 20 | Average loss: 0.2988\n",
      "Epoch: 18 | Training Batch: 30 | Average loss: 0.2853\n",
      "Epoch: 18 | Training Batch: 40 | Average loss: 0.2719\n",
      "Average training batch loss at epoch 18: 0.2836\n",
      "Average validation fold accuracy at epoch 18: 0.2830\n",
      "Epoch: 19 | Training Batch: 10 | Average loss: 0.2895\n",
      "Epoch: 19 | Training Batch: 20 | Average loss: 0.2848\n",
      "Epoch: 19 | Training Batch: 30 | Average loss: 0.2791\n",
      "Epoch: 19 | Training Batch: 40 | Average loss: 0.2678\n",
      "Average training batch loss at epoch 19: 0.2836\n",
      "Average validation fold accuracy at epoch 19: 0.2820\n",
      "Epoch: 20 | Training Batch: 10 | Average loss: 0.2857\n",
      "Epoch: 20 | Training Batch: 20 | Average loss: 0.2758\n",
      "Epoch: 20 | Training Batch: 30 | Average loss: 0.2761\n",
      "Epoch: 20 | Training Batch: 40 | Average loss: 0.2908\n",
      "Average training batch loss at epoch 20: 0.2836\n",
      "Average validation fold accuracy at epoch 20: 0.2811\n",
      "Epoch: 21 | Training Batch: 10 | Average loss: 0.2797\n",
      "Epoch: 21 | Training Batch: 20 | Average loss: 0.2929\n",
      "Epoch: 21 | Training Batch: 30 | Average loss: 0.2883\n",
      "Epoch: 21 | Training Batch: 40 | Average loss: 0.2827\n",
      "Average training batch loss at epoch 21: 0.2836\n",
      "Average validation fold accuracy at epoch 21: 0.2805\n",
      "Epoch: 22 | Training Batch: 10 | Average loss: 0.2801\n",
      "Epoch: 22 | Training Batch: 20 | Average loss: 0.2770\n",
      "Epoch: 22 | Training Batch: 30 | Average loss: 0.2763\n",
      "Epoch: 22 | Training Batch: 40 | Average loss: 0.2761\n",
      "Average training batch loss at epoch 22: 0.2835\n",
      "Average validation fold accuracy at epoch 22: 0.2808\n",
      "Epoch: 23 | Training Batch: 10 | Average loss: 0.2829\n",
      "Epoch: 23 | Training Batch: 20 | Average loss: 0.2739\n",
      "Epoch: 23 | Training Batch: 30 | Average loss: 0.2861\n",
      "Epoch: 23 | Training Batch: 40 | Average loss: 0.2716\n",
      "Average training batch loss at epoch 23: 0.2833\n",
      "Average validation fold accuracy at epoch 23: 0.2814\n",
      "Epoch: 24 | Training Batch: 10 | Average loss: 0.2811\n",
      "Epoch: 24 | Training Batch: 20 | Average loss: 0.2778\n",
      "Epoch: 24 | Training Batch: 30 | Average loss: 0.2737\n",
      "Epoch: 24 | Training Batch: 40 | Average loss: 0.2679\n",
      "Average training batch loss at epoch 24: 0.2830\n",
      "Average validation fold accuracy at epoch 24: 0.2808\n",
      "Epoch: 25 | Training Batch: 10 | Average loss: 0.2664\n",
      "Epoch: 25 | Training Batch: 20 | Average loss: 0.2825\n",
      "Epoch: 25 | Training Batch: 30 | Average loss: 0.2750\n",
      "Epoch: 25 | Training Batch: 40 | Average loss: 0.2738\n",
      "Average training batch loss at epoch 25: 0.2827\n",
      "Average validation fold accuracy at epoch 25: 0.2813\n",
      "Epoch: 26 | Training Batch: 10 | Average loss: 0.2946\n",
      "Epoch: 26 | Training Batch: 20 | Average loss: 0.2788\n",
      "Epoch: 26 | Training Batch: 30 | Average loss: 0.2934\n",
      "Epoch: 26 | Training Batch: 40 | Average loss: 0.2787\n",
      "Average training batch loss at epoch 26: 0.2828\n",
      "Average validation fold accuracy at epoch 26: 0.2800\n",
      "Epoch: 27 | Training Batch: 10 | Average loss: 0.2903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Training Batch: 20 | Average loss: 0.2530\n",
      "Epoch: 27 | Training Batch: 30 | Average loss: 0.2850\n",
      "Epoch: 27 | Training Batch: 40 | Average loss: 0.2885\n",
      "Average training batch loss at epoch 27: 0.2827\n",
      "Average validation fold accuracy at epoch 27: 0.2800\n",
      "Epoch: 28 | Training Batch: 10 | Average loss: 0.2613\n",
      "Epoch: 28 | Training Batch: 20 | Average loss: 0.2849\n",
      "Epoch: 28 | Training Batch: 30 | Average loss: 0.2738\n",
      "Epoch: 28 | Training Batch: 40 | Average loss: 0.2848\n",
      "Average training batch loss at epoch 28: 0.2825\n",
      "Average validation fold accuracy at epoch 28: 0.2802\n",
      "Epoch: 29 | Training Batch: 10 | Average loss: 0.2919\n",
      "Epoch: 29 | Training Batch: 20 | Average loss: 0.2643\n",
      "Epoch: 29 | Training Batch: 30 | Average loss: 0.2928\n",
      "Epoch: 29 | Training Batch: 40 | Average loss: 0.2732\n",
      "Average training batch loss at epoch 29: 0.2823\n",
      "Average validation fold accuracy at epoch 29: 0.2802\n",
      "Epoch: 30 | Training Batch: 10 | Average loss: 0.2924\n",
      "Epoch: 30 | Training Batch: 20 | Average loss: 0.2690\n",
      "Epoch: 30 | Training Batch: 30 | Average loss: 0.2669\n",
      "Epoch: 30 | Training Batch: 40 | Average loss: 0.2657\n",
      "Average training batch loss at epoch 30: 0.2821\n",
      "Average validation fold accuracy at epoch 30: 0.2799\n",
      "Epoch: 31 | Training Batch: 10 | Average loss: 0.2882\n",
      "Epoch: 31 | Training Batch: 20 | Average loss: 0.2857\n",
      "Epoch: 31 | Training Batch: 30 | Average loss: 0.2801\n",
      "Epoch: 31 | Training Batch: 40 | Average loss: 0.2597\n",
      "Average training batch loss at epoch 31: 0.2819\n",
      "Average validation fold accuracy at epoch 31: 0.2799\n",
      "Epoch: 32 | Training Batch: 10 | Average loss: 0.2709\n",
      "Epoch: 32 | Training Batch: 20 | Average loss: 0.2850\n",
      "Epoch: 32 | Training Batch: 30 | Average loss: 0.2775\n",
      "Epoch: 32 | Training Batch: 40 | Average loss: 0.2789\n",
      "Average training batch loss at epoch 32: 0.2818\n",
      "Average validation fold accuracy at epoch 32: 0.2800\n",
      "Epoch: 33 | Training Batch: 10 | Average loss: 0.2953\n",
      "Epoch: 33 | Training Batch: 20 | Average loss: 0.2664\n",
      "Epoch: 33 | Training Batch: 30 | Average loss: 0.2973\n",
      "Epoch: 33 | Training Batch: 40 | Average loss: 0.2715\n",
      "Average training batch loss at epoch 33: 0.2819\n",
      "Average validation fold accuracy at epoch 33: 0.2806\n",
      "Epoch: 34 | Training Batch: 10 | Average loss: 0.2831\n",
      "Epoch: 34 | Training Batch: 20 | Average loss: 0.2950\n",
      "Epoch: 34 | Training Batch: 30 | Average loss: 0.2822\n",
      "Epoch: 34 | Training Batch: 40 | Average loss: 0.2795\n",
      "Average training batch loss at epoch 34: 0.2819\n",
      "Average validation fold accuracy at epoch 34: 0.2806\n",
      "Epoch: 35 | Training Batch: 10 | Average loss: 0.2949\n",
      "Epoch: 35 | Training Batch: 20 | Average loss: 0.2789\n",
      "Epoch: 35 | Training Batch: 30 | Average loss: 0.2770\n",
      "Epoch: 35 | Training Batch: 40 | Average loss: 0.2684\n",
      "Average training batch loss at epoch 35: 0.2818\n",
      "Average validation fold accuracy at epoch 35: 0.2806\n",
      "Epoch: 36 | Training Batch: 10 | Average loss: 0.2893\n",
      "Epoch: 36 | Training Batch: 20 | Average loss: 0.2972\n",
      "Epoch: 36 | Training Batch: 30 | Average loss: 0.2709\n",
      "Epoch: 36 | Training Batch: 40 | Average loss: 0.2907\n",
      "Average training batch loss at epoch 36: 0.2820\n",
      "Average validation fold accuracy at epoch 36: 0.2804\n",
      "Epoch: 37 | Training Batch: 10 | Average loss: 0.2815\n",
      "Epoch: 37 | Training Batch: 20 | Average loss: 0.2839\n",
      "Epoch: 37 | Training Batch: 30 | Average loss: 0.2734\n",
      "Epoch: 37 | Training Batch: 40 | Average loss: 0.2800\n",
      "Average training batch loss at epoch 37: 0.2819\n",
      "Average validation fold accuracy at epoch 37: 0.2806\n",
      "Epoch: 38 | Training Batch: 10 | Average loss: 0.2804\n",
      "Epoch: 38 | Training Batch: 20 | Average loss: 0.2753\n",
      "Epoch: 38 | Training Batch: 30 | Average loss: 0.2751\n",
      "Epoch: 38 | Training Batch: 40 | Average loss: 0.2722\n",
      "Average training batch loss at epoch 38: 0.2818\n",
      "Average validation fold accuracy at epoch 38: 0.2814\n",
      "Epoch: 39 | Training Batch: 10 | Average loss: 0.2880\n",
      "Epoch: 39 | Training Batch: 20 | Average loss: 0.2758\n",
      "Epoch: 39 | Training Batch: 30 | Average loss: 0.2758\n",
      "Epoch: 39 | Training Batch: 40 | Average loss: 0.2660\n",
      "Average training batch loss at epoch 39: 0.2817\n",
      "Average validation fold accuracy at epoch 39: 0.2815\n",
      "Epoch: 40 | Training Batch: 10 | Average loss: 0.2601\n",
      "Epoch: 40 | Training Batch: 20 | Average loss: 0.2699\n",
      "Epoch: 40 | Training Batch: 30 | Average loss: 0.2860\n",
      "Epoch: 40 | Training Batch: 40 | Average loss: 0.2644\n",
      "Average training batch loss at epoch 40: 0.2814\n",
      "Average validation fold accuracy at epoch 40: 0.2817\n",
      "Epoch: 41 | Training Batch: 10 | Average loss: 0.2846\n",
      "Epoch: 41 | Training Batch: 20 | Average loss: 0.2994\n",
      "Epoch: 41 | Training Batch: 30 | Average loss: 0.2698\n",
      "Epoch: 41 | Training Batch: 40 | Average loss: 0.2645\n",
      "Average training batch loss at epoch 41: 0.2814\n",
      "Average validation fold accuracy at epoch 41: 0.2815\n",
      "Epoch: 42 | Training Batch: 10 | Average loss: 0.2660\n",
      "Epoch: 42 | Training Batch: 20 | Average loss: 0.2582\n",
      "Epoch: 42 | Training Batch: 30 | Average loss: 0.2856\n",
      "Epoch: 42 | Training Batch: 40 | Average loss: 0.2663\n",
      "Average training batch loss at epoch 42: 0.2811\n",
      "Average validation fold accuracy at epoch 42: 0.2816\n",
      "Epoch: 43 | Training Batch: 10 | Average loss: 0.2688\n",
      "Epoch: 43 | Training Batch: 20 | Average loss: 0.2969\n",
      "Epoch: 43 | Training Batch: 30 | Average loss: 0.2865\n",
      "Epoch: 43 | Training Batch: 40 | Average loss: 0.2900\n",
      "Average training batch loss at epoch 43: 0.2811\n",
      "Average validation fold accuracy at epoch 43: 0.2817\n",
      "Epoch: 44 | Training Batch: 10 | Average loss: 0.2734\n",
      "Epoch: 44 | Training Batch: 20 | Average loss: 0.2630\n",
      "Epoch: 44 | Training Batch: 30 | Average loss: 0.2730\n",
      "Epoch: 44 | Training Batch: 40 | Average loss: 0.2982\n",
      "Average training batch loss at epoch 44: 0.2810\n",
      "Average validation fold accuracy at epoch 44: 0.2810\n",
      "Epoch: 45 | Training Batch: 10 | Average loss: 0.2824\n",
      "Epoch: 45 | Training Batch: 20 | Average loss: 0.2762\n",
      "Epoch: 45 | Training Batch: 30 | Average loss: 0.2754\n",
      "Epoch: 45 | Training Batch: 40 | Average loss: 0.2693\n",
      "Average training batch loss at epoch 45: 0.2810\n",
      "Average validation fold accuracy at epoch 45: 0.2811\n",
      "Epoch: 46 | Training Batch: 10 | Average loss: 0.2745\n",
      "Epoch: 46 | Training Batch: 20 | Average loss: 0.2817\n",
      "Epoch: 46 | Training Batch: 30 | Average loss: 0.2750\n",
      "Epoch: 46 | Training Batch: 40 | Average loss: 0.2773\n",
      "Average training batch loss at epoch 46: 0.2809\n",
      "Average validation fold accuracy at epoch 46: 0.2809\n",
      "Epoch: 47 | Training Batch: 10 | Average loss: 0.2884\n",
      "Epoch: 47 | Training Batch: 20 | Average loss: 0.2766\n",
      "Epoch: 47 | Training Batch: 30 | Average loss: 0.2676\n",
      "Epoch: 47 | Training Batch: 40 | Average loss: 0.2775\n",
      "Average training batch loss at epoch 47: 0.2808\n",
      "Average validation fold accuracy at epoch 47: 0.2810\n",
      "Epoch: 48 | Training Batch: 10 | Average loss: 0.2750\n",
      "Epoch: 48 | Training Batch: 20 | Average loss: 0.2927\n",
      "Epoch: 48 | Training Batch: 30 | Average loss: 0.2846\n",
      "Epoch: 48 | Training Batch: 40 | Average loss: 0.2851\n",
      "Average training batch loss at epoch 48: 0.2808\n",
      "Average validation fold accuracy at epoch 48: 0.2804\n",
      "Epoch: 49 | Training Batch: 10 | Average loss: 0.2978\n",
      "Epoch: 49 | Training Batch: 20 | Average loss: 0.2951\n",
      "Epoch: 49 | Training Batch: 30 | Average loss: 0.2751\n",
      "Epoch: 49 | Training Batch: 40 | Average loss: 0.2872\n",
      "Average training batch loss at epoch 49: 0.2810\n",
      "Average validation fold accuracy at epoch 49: 0.2802\n",
      "Epoch: 50 | Training Batch: 10 | Average loss: 0.2690\n",
      "Epoch: 50 | Training Batch: 20 | Average loss: 0.2852\n",
      "Epoch: 50 | Training Batch: 30 | Average loss: 0.2774\n",
      "Epoch: 50 | Training Batch: 40 | Average loss: 0.2904\n",
      "Average training batch loss at epoch 50: 0.2809\n",
      "Average validation fold accuracy at epoch 50: 0.2808\n",
      "Epoch: 51 | Training Batch: 10 | Average loss: 0.2818\n",
      "Epoch: 51 | Training Batch: 20 | Average loss: 0.2889\n",
      "Epoch: 51 | Training Batch: 30 | Average loss: 0.2813\n",
      "Epoch: 51 | Training Batch: 40 | Average loss: 0.2905\n",
      "Average training batch loss at epoch 51: 0.2810\n",
      "Average validation fold accuracy at epoch 51: 0.2808\n",
      "Epoch: 52 | Training Batch: 10 | Average loss: 0.2771\n",
      "Epoch: 52 | Training Batch: 20 | Average loss: 0.2789\n",
      "Epoch: 52 | Training Batch: 30 | Average loss: 0.2731\n",
      "Epoch: 52 | Training Batch: 40 | Average loss: 0.2893\n",
      "Average training batch loss at epoch 52: 0.2810\n",
      "Average validation fold accuracy at epoch 52: 0.2810\n",
      "Epoch: 53 | Training Batch: 10 | Average loss: 0.2830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53 | Training Batch: 20 | Average loss: 0.2768\n",
      "Epoch: 53 | Training Batch: 30 | Average loss: 0.2868\n",
      "Epoch: 53 | Training Batch: 40 | Average loss: 0.2860\n",
      "Average training batch loss at epoch 53: 0.2810\n",
      "Average validation fold accuracy at epoch 53: 0.2811\n",
      "Epoch: 54 | Training Batch: 10 | Average loss: 0.2721\n",
      "Epoch: 54 | Training Batch: 20 | Average loss: 0.2815\n",
      "Epoch: 54 | Training Batch: 30 | Average loss: 0.2836\n",
      "Epoch: 54 | Training Batch: 40 | Average loss: 0.2764\n",
      "Average training batch loss at epoch 54: 0.2810\n",
      "Average validation fold accuracy at epoch 54: 0.2813\n",
      "Epoch: 55 | Training Batch: 10 | Average loss: 0.2803\n",
      "Epoch: 55 | Training Batch: 20 | Average loss: 0.2755\n",
      "Epoch: 55 | Training Batch: 30 | Average loss: 0.2785\n",
      "Epoch: 55 | Training Batch: 40 | Average loss: 0.2742\n",
      "Average training batch loss at epoch 55: 0.2809\n",
      "Average validation fold accuracy at epoch 55: 0.2811\n",
      "Epoch: 56 | Training Batch: 10 | Average loss: 0.2714\n",
      "Epoch: 56 | Training Batch: 20 | Average loss: 0.2736\n",
      "Epoch: 56 | Training Batch: 30 | Average loss: 0.2819\n",
      "Epoch: 56 | Training Batch: 40 | Average loss: 0.2835\n",
      "Average training batch loss at epoch 56: 0.2808\n",
      "Average validation fold accuracy at epoch 56: 0.2811\n",
      "Epoch: 57 | Training Batch: 10 | Average loss: 0.2890\n",
      "Epoch: 57 | Training Batch: 20 | Average loss: 0.2832\n",
      "Epoch: 57 | Training Batch: 30 | Average loss: 0.2765\n",
      "Epoch: 57 | Training Batch: 40 | Average loss: 0.2787\n",
      "Average training batch loss at epoch 57: 0.2809\n",
      "Average validation fold accuracy at epoch 57: 0.2804\n",
      "Epoch: 58 | Training Batch: 10 | Average loss: 0.2690\n",
      "Epoch: 58 | Training Batch: 20 | Average loss: 0.2765\n",
      "Epoch: 58 | Training Batch: 30 | Average loss: 0.2826\n",
      "Epoch: 58 | Training Batch: 40 | Average loss: 0.2740\n",
      "Average training batch loss at epoch 58: 0.2808\n",
      "Average validation fold accuracy at epoch 58: 0.2803\n",
      "Epoch: 59 | Training Batch: 10 | Average loss: 0.2914\n",
      "Epoch: 59 | Training Batch: 20 | Average loss: 0.2828\n",
      "Epoch: 59 | Training Batch: 30 | Average loss: 0.2866\n",
      "Epoch: 59 | Training Batch: 40 | Average loss: 0.2845\n",
      "Average training batch loss at epoch 59: 0.2809\n",
      "Average validation fold accuracy at epoch 59: 0.2803\n",
      "Epoch: 60 | Training Batch: 10 | Average loss: 0.2837\n",
      "Epoch: 60 | Training Batch: 20 | Average loss: 0.2747\n",
      "Epoch: 60 | Training Batch: 30 | Average loss: 0.2750\n",
      "Epoch: 60 | Training Batch: 40 | Average loss: 0.2849\n",
      "Average training batch loss at epoch 60: 0.2809\n",
      "Average validation fold accuracy at epoch 60: 0.2802\n",
      "Epoch: 61 | Training Batch: 10 | Average loss: 0.2755\n",
      "Epoch: 61 | Training Batch: 20 | Average loss: 0.2863\n",
      "Epoch: 61 | Training Batch: 30 | Average loss: 0.2836\n",
      "Epoch: 61 | Training Batch: 40 | Average loss: 0.2823\n",
      "Average training batch loss at epoch 61: 0.2810\n",
      "Average validation fold accuracy at epoch 61: 0.2804\n",
      "Epoch: 62 | Training Batch: 10 | Average loss: 0.2886\n",
      "Epoch: 62 | Training Batch: 20 | Average loss: 0.2683\n",
      "Epoch: 62 | Training Batch: 30 | Average loss: 0.2652\n",
      "Epoch: 62 | Training Batch: 40 | Average loss: 0.2826\n",
      "Average training batch loss at epoch 62: 0.2809\n",
      "Average validation fold accuracy at epoch 62: 0.2805\n",
      "Epoch: 63 | Training Batch: 10 | Average loss: 0.2796\n",
      "Epoch: 63 | Training Batch: 20 | Average loss: 0.2812\n",
      "Epoch: 63 | Training Batch: 30 | Average loss: 0.2903\n",
      "Epoch: 63 | Training Batch: 40 | Average loss: 0.2593\n",
      "Average training batch loss at epoch 63: 0.2808\n",
      "Average validation fold accuracy at epoch 63: 0.2805\n",
      "Epoch: 64 | Training Batch: 10 | Average loss: 0.2697\n",
      "Epoch: 64 | Training Batch: 20 | Average loss: 0.2735\n",
      "Epoch: 64 | Training Batch: 30 | Average loss: 0.2850\n",
      "Epoch: 64 | Training Batch: 40 | Average loss: 0.2761\n",
      "Average training batch loss at epoch 64: 0.2808\n",
      "Average validation fold accuracy at epoch 64: 0.2804\n",
      "Epoch: 65 | Training Batch: 10 | Average loss: 0.2847\n",
      "Epoch: 65 | Training Batch: 20 | Average loss: 0.2773\n",
      "Epoch: 65 | Training Batch: 30 | Average loss: 0.2902\n",
      "Epoch: 65 | Training Batch: 40 | Average loss: 0.2703\n",
      "Average training batch loss at epoch 65: 0.2808\n",
      "Average validation fold accuracy at epoch 65: 0.2802\n",
      "Epoch: 66 | Training Batch: 10 | Average loss: 0.2788\n",
      "Epoch: 66 | Training Batch: 20 | Average loss: 0.2714\n",
      "Epoch: 66 | Training Batch: 30 | Average loss: 0.2755\n",
      "Epoch: 66 | Training Batch: 40 | Average loss: 0.2652\n",
      "Average training batch loss at epoch 66: 0.2807\n",
      "Average validation fold accuracy at epoch 66: 0.2800\n",
      "Epoch: 67 | Training Batch: 10 | Average loss: 0.2721\n",
      "Epoch: 67 | Training Batch: 20 | Average loss: 0.2972\n",
      "Epoch: 67 | Training Batch: 30 | Average loss: 0.2762\n",
      "Epoch: 67 | Training Batch: 40 | Average loss: 0.2905\n",
      "Average training batch loss at epoch 67: 0.2807\n",
      "Average validation fold accuracy at epoch 67: 0.2799\n",
      "Epoch: 68 | Training Batch: 10 | Average loss: 0.2832\n",
      "Epoch: 68 | Training Batch: 20 | Average loss: 0.2861\n",
      "Epoch: 68 | Training Batch: 30 | Average loss: 0.2689\n",
      "Epoch: 68 | Training Batch: 40 | Average loss: 0.3139\n",
      "Average training batch loss at epoch 68: 0.2808\n",
      "Average validation fold accuracy at epoch 68: 0.2792\n",
      "Epoch: 69 | Training Batch: 10 | Average loss: 0.2725\n",
      "Epoch: 69 | Training Batch: 20 | Average loss: 0.2677\n",
      "Epoch: 69 | Training Batch: 30 | Average loss: 0.2827\n",
      "Epoch: 69 | Training Batch: 40 | Average loss: 0.2878\n",
      "Average training batch loss at epoch 69: 0.2807\n",
      "Average validation fold accuracy at epoch 69: 0.2796\n",
      "Epoch: 70 | Training Batch: 10 | Average loss: 0.2780\n",
      "Epoch: 70 | Training Batch: 20 | Average loss: 0.2752\n",
      "Epoch: 70 | Training Batch: 30 | Average loss: 0.2701\n",
      "Epoch: 70 | Training Batch: 40 | Average loss: 0.2690\n",
      "Average training batch loss at epoch 70: 0.2806\n",
      "Average validation fold accuracy at epoch 70: 0.2797\n",
      "Epoch: 71 | Training Batch: 10 | Average loss: 0.2862\n",
      "Epoch: 71 | Training Batch: 20 | Average loss: 0.2914\n",
      "Epoch: 71 | Training Batch: 30 | Average loss: 0.2822\n",
      "Epoch: 71 | Training Batch: 40 | Average loss: 0.2887\n",
      "Average training batch loss at epoch 71: 0.2806\n",
      "Average validation fold accuracy at epoch 71: 0.2795\n",
      "Epoch: 72 | Training Batch: 10 | Average loss: 0.2911\n",
      "Epoch: 72 | Training Batch: 20 | Average loss: 0.2787\n",
      "Epoch: 72 | Training Batch: 30 | Average loss: 0.2870\n",
      "Epoch: 72 | Training Batch: 40 | Average loss: 0.2940\n",
      "Average training batch loss at epoch 72: 0.2807\n",
      "Average validation fold accuracy at epoch 72: 0.2796\n",
      "Epoch: 73 | Training Batch: 10 | Average loss: 0.2725\n",
      "Epoch: 73 | Training Batch: 20 | Average loss: 0.2923\n",
      "Epoch: 73 | Training Batch: 30 | Average loss: 0.2825\n",
      "Epoch: 73 | Training Batch: 40 | Average loss: 0.2647\n",
      "Average training batch loss at epoch 73: 0.2807\n",
      "Average validation fold accuracy at epoch 73: 0.2798\n",
      "Epoch: 74 | Training Batch: 10 | Average loss: 0.2806\n",
      "Epoch: 74 | Training Batch: 20 | Average loss: 0.2710\n",
      "Epoch: 74 | Training Batch: 30 | Average loss: 0.2835\n",
      "Epoch: 74 | Training Batch: 40 | Average loss: 0.2673\n",
      "Average training batch loss at epoch 74: 0.2806\n",
      "Average validation fold accuracy at epoch 74: 0.2796\n",
      "Epoch: 75 | Training Batch: 10 | Average loss: 0.2881\n",
      "Epoch: 75 | Training Batch: 20 | Average loss: 0.2811\n",
      "Epoch: 75 | Training Batch: 30 | Average loss: 0.2871\n",
      "Epoch: 75 | Training Batch: 40 | Average loss: 0.2812\n",
      "Average training batch loss at epoch 75: 0.2806\n",
      "Average validation fold accuracy at epoch 75: 0.2796\n",
      "Epoch: 76 | Training Batch: 10 | Average loss: 0.2713\n",
      "Epoch: 76 | Training Batch: 20 | Average loss: 0.2850\n",
      "Epoch: 76 | Training Batch: 30 | Average loss: 0.2880\n",
      "Epoch: 76 | Training Batch: 40 | Average loss: 0.2567\n",
      "Average training batch loss at epoch 76: 0.2806\n",
      "Average validation fold accuracy at epoch 76: 0.2796\n",
      "Epoch: 77 | Training Batch: 10 | Average loss: 0.2799\n",
      "Epoch: 77 | Training Batch: 20 | Average loss: 0.2924\n",
      "Epoch: 77 | Training Batch: 30 | Average loss: 0.2845\n",
      "Epoch: 77 | Training Batch: 40 | Average loss: 0.2794\n",
      "Average training batch loss at epoch 77: 0.2807\n",
      "Average validation fold accuracy at epoch 77: 0.2795\n",
      "Epoch: 78 | Training Batch: 10 | Average loss: 0.2926\n",
      "Epoch: 78 | Training Batch: 20 | Average loss: 0.2682\n",
      "Epoch: 78 | Training Batch: 30 | Average loss: 0.2851\n",
      "Epoch: 78 | Training Batch: 40 | Average loss: 0.2703\n",
      "Average training batch loss at epoch 78: 0.2807\n",
      "Average validation fold accuracy at epoch 78: 0.2792\n",
      "Epoch: 79 | Training Batch: 10 | Average loss: 0.2720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79 | Training Batch: 20 | Average loss: 0.2835\n",
      "Epoch: 79 | Training Batch: 30 | Average loss: 0.2801\n",
      "Epoch: 79 | Training Batch: 40 | Average loss: 0.2874\n",
      "Average training batch loss at epoch 79: 0.2807\n",
      "Average validation fold accuracy at epoch 79: 0.2792\n",
      "Epoch: 80 | Training Batch: 10 | Average loss: 0.2850\n",
      "Epoch: 80 | Training Batch: 20 | Average loss: 0.2736\n",
      "Epoch: 80 | Training Batch: 30 | Average loss: 0.2758\n",
      "Epoch: 80 | Training Batch: 40 | Average loss: 0.2771\n",
      "Average training batch loss at epoch 80: 0.2806\n",
      "Average validation fold accuracy at epoch 80: 0.2790\n",
      "Epoch: 81 | Training Batch: 10 | Average loss: 0.2809\n",
      "Epoch: 81 | Training Batch: 20 | Average loss: 0.2898\n",
      "Epoch: 81 | Training Batch: 30 | Average loss: 0.2688\n",
      "Epoch: 81 | Training Batch: 40 | Average loss: 0.2807\n",
      "Average training batch loss at epoch 81: 0.2807\n",
      "Average validation fold accuracy at epoch 81: 0.2790\n",
      "Epoch: 82 | Training Batch: 10 | Average loss: 0.2928\n",
      "Epoch: 82 | Training Batch: 20 | Average loss: 0.2704\n",
      "Epoch: 82 | Training Batch: 30 | Average loss: 0.2772\n",
      "Epoch: 82 | Training Batch: 40 | Average loss: 0.2795\n",
      "Average training batch loss at epoch 82: 0.2807\n",
      "Average validation fold accuracy at epoch 82: 0.2791\n",
      "Epoch: 83 | Training Batch: 10 | Average loss: 0.2938\n",
      "Epoch: 83 | Training Batch: 20 | Average loss: 0.2787\n",
      "Epoch: 83 | Training Batch: 30 | Average loss: 0.2702\n",
      "Epoch: 83 | Training Batch: 40 | Average loss: 0.2846\n",
      "Average training batch loss at epoch 83: 0.2806\n",
      "Average validation fold accuracy at epoch 83: 0.2790\n",
      "Epoch: 84 | Training Batch: 10 | Average loss: 0.2743\n",
      "Epoch: 84 | Training Batch: 20 | Average loss: 0.2814\n",
      "Epoch: 84 | Training Batch: 30 | Average loss: 0.2815\n",
      "Epoch: 84 | Training Batch: 40 | Average loss: 0.2724\n",
      "Average training batch loss at epoch 84: 0.2806\n",
      "Average validation fold accuracy at epoch 84: 0.2788\n",
      "Epoch: 85 | Training Batch: 10 | Average loss: 0.2792\n",
      "Epoch: 85 | Training Batch: 20 | Average loss: 0.2858\n",
      "Epoch: 85 | Training Batch: 30 | Average loss: 0.2610\n",
      "Epoch: 85 | Training Batch: 40 | Average loss: 0.2756\n",
      "Average training batch loss at epoch 85: 0.2806\n",
      "Average validation fold accuracy at epoch 85: 0.2790\n",
      "Epoch: 86 | Training Batch: 10 | Average loss: 0.2739\n",
      "Epoch: 86 | Training Batch: 20 | Average loss: 0.2639\n",
      "Epoch: 86 | Training Batch: 30 | Average loss: 0.2891\n",
      "Epoch: 86 | Training Batch: 40 | Average loss: 0.2782\n",
      "Average training batch loss at epoch 86: 0.2805\n",
      "Average validation fold accuracy at epoch 86: 0.2791\n",
      "Epoch: 87 | Training Batch: 10 | Average loss: 0.2941\n",
      "Epoch: 87 | Training Batch: 20 | Average loss: 0.2669\n",
      "Epoch: 87 | Training Batch: 30 | Average loss: 0.2798\n",
      "Epoch: 87 | Training Batch: 40 | Average loss: 0.2846\n",
      "Average training batch loss at epoch 87: 0.2805\n",
      "Average validation fold accuracy at epoch 87: 0.2790\n",
      "Epoch: 88 | Training Batch: 10 | Average loss: 0.2719\n",
      "Epoch: 88 | Training Batch: 20 | Average loss: 0.2745\n",
      "Epoch: 88 | Training Batch: 30 | Average loss: 0.2815\n",
      "Epoch: 88 | Training Batch: 40 | Average loss: 0.2819\n",
      "Average training batch loss at epoch 88: 0.2805\n",
      "Average validation fold accuracy at epoch 88: 0.2791\n",
      "Epoch: 89 | Training Batch: 10 | Average loss: 0.2725\n",
      "Epoch: 89 | Training Batch: 20 | Average loss: 0.2760\n",
      "Epoch: 89 | Training Batch: 30 | Average loss: 0.2880\n",
      "Epoch: 89 | Training Batch: 40 | Average loss: 0.2752\n",
      "Average training batch loss at epoch 89: 0.2805\n",
      "Average validation fold accuracy at epoch 89: 0.2792\n",
      "Epoch: 90 | Training Batch: 10 | Average loss: 0.2726\n",
      "Epoch: 90 | Training Batch: 20 | Average loss: 0.2771\n",
      "Epoch: 90 | Training Batch: 30 | Average loss: 0.2799\n",
      "Epoch: 90 | Training Batch: 40 | Average loss: 0.2757\n",
      "Average training batch loss at epoch 90: 0.2804\n",
      "Average validation fold accuracy at epoch 90: 0.2793\n",
      "Epoch: 91 | Training Batch: 10 | Average loss: 0.2785\n",
      "Epoch: 91 | Training Batch: 20 | Average loss: 0.2913\n",
      "Epoch: 91 | Training Batch: 30 | Average loss: 0.2818\n",
      "Epoch: 91 | Training Batch: 40 | Average loss: 0.2908\n",
      "Average training batch loss at epoch 91: 0.2805\n",
      "Average validation fold accuracy at epoch 91: 0.2790\n",
      "Epoch: 92 | Training Batch: 10 | Average loss: 0.2835\n",
      "Epoch: 92 | Training Batch: 20 | Average loss: 0.2930\n",
      "Epoch: 92 | Training Batch: 30 | Average loss: 0.2742\n",
      "Epoch: 92 | Training Batch: 40 | Average loss: 0.2691\n",
      "Average training batch loss at epoch 92: 0.2804\n",
      "Average validation fold accuracy at epoch 92: 0.2789\n",
      "Epoch: 93 | Training Batch: 10 | Average loss: 0.2577\n",
      "Epoch: 93 | Training Batch: 20 | Average loss: 0.2769\n",
      "Epoch: 93 | Training Batch: 30 | Average loss: 0.2744\n",
      "Epoch: 93 | Training Batch: 40 | Average loss: 0.2825\n",
      "Average training batch loss at epoch 93: 0.2804\n",
      "Average validation fold accuracy at epoch 93: 0.2790\n",
      "Epoch: 94 | Training Batch: 10 | Average loss: 0.2578\n",
      "Epoch: 94 | Training Batch: 20 | Average loss: 0.2759\n",
      "Epoch: 94 | Training Batch: 30 | Average loss: 0.2649\n",
      "Epoch: 94 | Training Batch: 40 | Average loss: 0.2923\n",
      "Average training batch loss at epoch 94: 0.2803\n",
      "Average validation fold accuracy at epoch 94: 0.2790\n",
      "Epoch: 95 | Training Batch: 10 | Average loss: 0.2993\n",
      "Epoch: 95 | Training Batch: 20 | Average loss: 0.2672\n",
      "Epoch: 95 | Training Batch: 30 | Average loss: 0.2902\n",
      "Epoch: 95 | Training Batch: 40 | Average loss: 0.2649\n",
      "Average training batch loss at epoch 95: 0.2803\n",
      "Average validation fold accuracy at epoch 95: 0.2793\n",
      "Epoch: 96 | Training Batch: 10 | Average loss: 0.2690\n",
      "Epoch: 96 | Training Batch: 20 | Average loss: 0.2907\n",
      "Epoch: 96 | Training Batch: 30 | Average loss: 0.2844\n",
      "Epoch: 96 | Training Batch: 40 | Average loss: 0.2668\n",
      "Average training batch loss at epoch 96: 0.2802\n",
      "Average validation fold accuracy at epoch 96: 0.2793\n",
      "Epoch: 97 | Training Batch: 10 | Average loss: 0.2802\n",
      "Epoch: 97 | Training Batch: 20 | Average loss: 0.2860\n",
      "Epoch: 97 | Training Batch: 30 | Average loss: 0.2748\n",
      "Epoch: 97 | Training Batch: 40 | Average loss: 0.2608\n",
      "Average training batch loss at epoch 97: 0.2802\n",
      "Average validation fold accuracy at epoch 97: 0.2791\n",
      "Epoch: 98 | Training Batch: 10 | Average loss: 0.2907\n",
      "Epoch: 98 | Training Batch: 20 | Average loss: 0.2829\n",
      "Epoch: 98 | Training Batch: 30 | Average loss: 0.2920\n",
      "Epoch: 98 | Training Batch: 40 | Average loss: 0.2913\n",
      "Average training batch loss at epoch 98: 0.2802\n",
      "Average validation fold accuracy at epoch 98: 0.2790\n",
      "Epoch: 99 | Training Batch: 10 | Average loss: 0.2842\n",
      "Epoch: 99 | Training Batch: 20 | Average loss: 0.2675\n",
      "Epoch: 99 | Training Batch: 30 | Average loss: 0.2911\n",
      "Epoch: 99 | Training Batch: 40 | Average loss: 0.2902\n",
      "Average training batch loss at epoch 99: 0.2802\n",
      "Average validation fold accuracy at epoch 99: 0.2789\n",
      "Epoch: 100 | Training Batch: 10 | Average loss: 0.2567\n",
      "Epoch: 100 | Training Batch: 20 | Average loss: 0.2888\n",
      "Epoch: 100 | Training Batch: 30 | Average loss: 0.2695\n",
      "Epoch: 100 | Training Batch: 40 | Average loss: 0.2811\n",
      "Average training batch loss at epoch 100: 0.2802\n",
      "Average validation fold accuracy at epoch 100: 0.2788\n",
      "Epoch: 101 | Training Batch: 10 | Average loss: 0.2683\n",
      "Epoch: 101 | Training Batch: 20 | Average loss: 0.2811\n",
      "Epoch: 101 | Training Batch: 30 | Average loss: 0.2880\n",
      "Epoch: 101 | Training Batch: 40 | Average loss: 0.2757\n",
      "Average training batch loss at epoch 101: 0.2802\n",
      "Average validation fold accuracy at epoch 101: 0.2791\n",
      "Epoch: 102 | Training Batch: 10 | Average loss: 0.2780\n",
      "Epoch: 102 | Training Batch: 20 | Average loss: 0.2763\n",
      "Epoch: 102 | Training Batch: 30 | Average loss: 0.2938\n",
      "Epoch: 102 | Training Batch: 40 | Average loss: 0.2747\n",
      "Average training batch loss at epoch 102: 0.2802\n",
      "Average validation fold accuracy at epoch 102: 0.2792\n",
      "Epoch: 103 | Training Batch: 10 | Average loss: 0.2818\n",
      "Epoch: 103 | Training Batch: 20 | Average loss: 0.2827\n",
      "Epoch: 103 | Training Batch: 30 | Average loss: 0.2897\n",
      "Epoch: 103 | Training Batch: 40 | Average loss: 0.2950\n",
      "Average training batch loss at epoch 103: 0.2802\n",
      "Average validation fold accuracy at epoch 103: 0.2793\n",
      "Epoch: 104 | Training Batch: 10 | Average loss: 0.2825\n",
      "Epoch: 104 | Training Batch: 20 | Average loss: 0.3025\n",
      "Epoch: 104 | Training Batch: 30 | Average loss: 0.2677\n",
      "Epoch: 104 | Training Batch: 40 | Average loss: 0.2787\n",
      "Average training batch loss at epoch 104: 0.2803\n",
      "Average validation fold accuracy at epoch 104: 0.2794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 105 | Training Batch: 10 | Average loss: 0.2650\n",
      "Epoch: 105 | Training Batch: 20 | Average loss: 0.2921\n",
      "Epoch: 105 | Training Batch: 30 | Average loss: 0.2844\n",
      "Epoch: 105 | Training Batch: 40 | Average loss: 0.2679\n",
      "Average training batch loss at epoch 105: 0.2802\n",
      "Average validation fold accuracy at epoch 105: 0.2791\n",
      "Epoch: 106 | Training Batch: 10 | Average loss: 0.2767\n",
      "Epoch: 106 | Training Batch: 20 | Average loss: 0.2876\n",
      "Epoch: 106 | Training Batch: 30 | Average loss: 0.2632\n",
      "Epoch: 106 | Training Batch: 40 | Average loss: 0.2870\n",
      "Average training batch loss at epoch 106: 0.2802\n",
      "Average validation fold accuracy at epoch 106: 0.2793\n",
      "Epoch: 107 | Training Batch: 10 | Average loss: 0.2839\n",
      "Epoch: 107 | Training Batch: 20 | Average loss: 0.2847\n",
      "Epoch: 107 | Training Batch: 30 | Average loss: 0.2876\n",
      "Epoch: 107 | Training Batch: 40 | Average loss: 0.2598\n",
      "Average training batch loss at epoch 107: 0.2802\n",
      "Average validation fold accuracy at epoch 107: 0.2791\n",
      "Epoch: 108 | Training Batch: 10 | Average loss: 0.2848\n",
      "Epoch: 108 | Training Batch: 20 | Average loss: 0.2721\n",
      "Epoch: 108 | Training Batch: 30 | Average loss: 0.2796\n",
      "Epoch: 108 | Training Batch: 40 | Average loss: 0.2685\n",
      "Average training batch loss at epoch 108: 0.2802\n",
      "Average validation fold accuracy at epoch 108: 0.2791\n",
      "Epoch: 109 | Training Batch: 10 | Average loss: 0.2764\n",
      "Epoch: 109 | Training Batch: 20 | Average loss: 0.3057\n",
      "Epoch: 109 | Training Batch: 30 | Average loss: 0.2745\n",
      "Epoch: 109 | Training Batch: 40 | Average loss: 0.2809\n",
      "Average training batch loss at epoch 109: 0.2802\n",
      "Average validation fold accuracy at epoch 109: 0.2790\n",
      "Epoch: 110 | Training Batch: 10 | Average loss: 0.2873\n",
      "Epoch: 110 | Training Batch: 20 | Average loss: 0.2866\n",
      "Epoch: 110 | Training Batch: 30 | Average loss: 0.2642\n",
      "Epoch: 110 | Training Batch: 40 | Average loss: 0.2749\n",
      "Average training batch loss at epoch 110: 0.2802\n",
      "Average validation fold accuracy at epoch 110: 0.2789\n",
      "Epoch: 111 | Training Batch: 10 | Average loss: 0.2658\n",
      "Epoch: 111 | Training Batch: 20 | Average loss: 0.2722\n",
      "Epoch: 111 | Training Batch: 30 | Average loss: 0.2664\n",
      "Epoch: 111 | Training Batch: 40 | Average loss: 0.2814\n",
      "Average training batch loss at epoch 111: 0.2801\n",
      "Average validation fold accuracy at epoch 111: 0.2790\n",
      "Epoch: 112 | Training Batch: 10 | Average loss: 0.2825\n",
      "Epoch: 112 | Training Batch: 20 | Average loss: 0.2422\n",
      "Epoch: 112 | Training Batch: 30 | Average loss: 0.2746\n",
      "Epoch: 112 | Training Batch: 40 | Average loss: 0.2681\n",
      "Average training batch loss at epoch 112: 0.2800\n",
      "Average validation fold accuracy at epoch 112: 0.2790\n",
      "Epoch: 113 | Training Batch: 10 | Average loss: 0.2737\n",
      "Epoch: 113 | Training Batch: 20 | Average loss: 0.2811\n",
      "Epoch: 113 | Training Batch: 30 | Average loss: 0.2750\n",
      "Epoch: 113 | Training Batch: 40 | Average loss: 0.2679\n",
      "Average training batch loss at epoch 113: 0.2800\n",
      "Average validation fold accuracy at epoch 113: 0.2788\n",
      "Epoch: 114 | Training Batch: 10 | Average loss: 0.2716\n",
      "Epoch: 114 | Training Batch: 20 | Average loss: 0.2670\n",
      "Epoch: 114 | Training Batch: 30 | Average loss: 0.2916\n",
      "Epoch: 114 | Training Batch: 40 | Average loss: 0.2694\n",
      "Average training batch loss at epoch 114: 0.2799\n",
      "Average validation fold accuracy at epoch 114: 0.2789\n",
      "Epoch: 115 | Training Batch: 10 | Average loss: 0.2708\n",
      "Epoch: 115 | Training Batch: 20 | Average loss: 0.2681\n",
      "Epoch: 115 | Training Batch: 30 | Average loss: 0.2730\n",
      "Epoch: 115 | Training Batch: 40 | Average loss: 0.2733\n",
      "Average training batch loss at epoch 115: 0.2799\n",
      "Average validation fold accuracy at epoch 115: 0.2788\n",
      "Epoch: 116 | Training Batch: 10 | Average loss: 0.2800\n",
      "Epoch: 116 | Training Batch: 20 | Average loss: 0.2787\n",
      "Epoch: 116 | Training Batch: 30 | Average loss: 0.2726\n",
      "Epoch: 116 | Training Batch: 40 | Average loss: 0.2647\n",
      "Average training batch loss at epoch 116: 0.2798\n",
      "Average validation fold accuracy at epoch 116: 0.2788\n",
      "Epoch: 117 | Training Batch: 10 | Average loss: 0.2783\n",
      "Epoch: 117 | Training Batch: 20 | Average loss: 0.2770\n",
      "Epoch: 117 | Training Batch: 30 | Average loss: 0.2702\n",
      "Epoch: 117 | Training Batch: 40 | Average loss: 0.2863\n",
      "Average training batch loss at epoch 117: 0.2798\n",
      "Average validation fold accuracy at epoch 117: 0.2788\n",
      "Epoch: 118 | Training Batch: 10 | Average loss: 0.2756\n",
      "Epoch: 118 | Training Batch: 20 | Average loss: 0.2603\n",
      "Epoch: 118 | Training Batch: 30 | Average loss: 0.2720\n",
      "Epoch: 118 | Training Batch: 40 | Average loss: 0.2810\n",
      "Average training batch loss at epoch 118: 0.2797\n",
      "Average validation fold accuracy at epoch 118: 0.2788\n",
      "Epoch: 119 | Training Batch: 10 | Average loss: 0.2656\n",
      "Epoch: 119 | Training Batch: 20 | Average loss: 0.2911\n",
      "Epoch: 119 | Training Batch: 30 | Average loss: 0.2755\n",
      "Epoch: 119 | Training Batch: 40 | Average loss: 0.2666\n",
      "Average training batch loss at epoch 119: 0.2797\n",
      "Average validation fold accuracy at epoch 119: 0.2788\n",
      "Epoch: 120 | Training Batch: 10 | Average loss: 0.2763\n",
      "Epoch: 120 | Training Batch: 20 | Average loss: 0.2980\n",
      "Epoch: 120 | Training Batch: 30 | Average loss: 0.2831\n",
      "Epoch: 120 | Training Batch: 40 | Average loss: 0.2726\n",
      "Average training batch loss at epoch 120: 0.2797\n",
      "Average validation fold accuracy at epoch 120: 0.2786\n",
      "Epoch: 121 | Training Batch: 10 | Average loss: 0.2717\n",
      "Epoch: 121 | Training Batch: 20 | Average loss: 0.2759\n",
      "Epoch: 121 | Training Batch: 30 | Average loss: 0.2801\n",
      "Epoch: 121 | Training Batch: 40 | Average loss: 0.2761\n",
      "Average training batch loss at epoch 121: 0.2797\n",
      "Average validation fold accuracy at epoch 121: 0.2787\n",
      "Epoch: 122 | Training Batch: 10 | Average loss: 0.2750\n",
      "Epoch: 122 | Training Batch: 20 | Average loss: 0.2812\n",
      "Epoch: 122 | Training Batch: 30 | Average loss: 0.2695\n",
      "Epoch: 122 | Training Batch: 40 | Average loss: 0.2779\n",
      "Average training batch loss at epoch 122: 0.2796\n",
      "Average validation fold accuracy at epoch 122: 0.2788\n",
      "Epoch: 123 | Training Batch: 10 | Average loss: 0.2703\n",
      "Epoch: 123 | Training Batch: 20 | Average loss: 0.3027\n",
      "Epoch: 123 | Training Batch: 30 | Average loss: 0.2757\n",
      "Epoch: 123 | Training Batch: 40 | Average loss: 0.2838\n",
      "Average training batch loss at epoch 123: 0.2797\n",
      "Average validation fold accuracy at epoch 123: 0.2787\n",
      "Epoch: 124 | Training Batch: 10 | Average loss: 0.2703\n",
      "Epoch: 124 | Training Batch: 20 | Average loss: 0.2884\n",
      "Epoch: 124 | Training Batch: 30 | Average loss: 0.2806\n",
      "Epoch: 124 | Training Batch: 40 | Average loss: 0.2867\n",
      "Average training batch loss at epoch 124: 0.2797\n",
      "Average validation fold accuracy at epoch 124: 0.2787\n",
      "Epoch: 125 | Training Batch: 10 | Average loss: 0.2868\n",
      "Epoch: 125 | Training Batch: 20 | Average loss: 0.2746\n",
      "Epoch: 125 | Training Batch: 30 | Average loss: 0.2954\n",
      "Epoch: 125 | Training Batch: 40 | Average loss: 0.2785\n",
      "Average training batch loss at epoch 125: 0.2797\n",
      "Average validation fold accuracy at epoch 125: 0.2785\n",
      "Epoch: 126 | Training Batch: 10 | Average loss: 0.2888\n",
      "Epoch: 126 | Training Batch: 20 | Average loss: 0.2707\n",
      "Epoch: 126 | Training Batch: 30 | Average loss: 0.2740\n",
      "Epoch: 126 | Training Batch: 40 | Average loss: 0.2684\n",
      "Average training batch loss at epoch 126: 0.2797\n",
      "Average validation fold accuracy at epoch 126: 0.2784\n",
      "Epoch: 127 | Training Batch: 10 | Average loss: 0.2984\n",
      "Epoch: 127 | Training Batch: 20 | Average loss: 0.2850\n",
      "Epoch: 127 | Training Batch: 30 | Average loss: 0.2789\n",
      "Epoch: 127 | Training Batch: 40 | Average loss: 0.2954\n",
      "Average training batch loss at epoch 127: 0.2797\n",
      "Average validation fold accuracy at epoch 127: 0.2783\n",
      "Epoch: 128 | Training Batch: 10 | Average loss: 0.2757\n",
      "Epoch: 128 | Training Batch: 20 | Average loss: 0.2848\n",
      "Epoch: 128 | Training Batch: 30 | Average loss: 0.2713\n",
      "Epoch: 128 | Training Batch: 40 | Average loss: 0.2674\n",
      "Average training batch loss at epoch 128: 0.2797\n",
      "Average validation fold accuracy at epoch 128: 0.2785\n",
      "Epoch: 129 | Training Batch: 10 | Average loss: 0.2781\n",
      "Epoch: 129 | Training Batch: 20 | Average loss: 0.2896\n",
      "Epoch: 129 | Training Batch: 30 | Average loss: 0.2845\n",
      "Epoch: 129 | Training Batch: 40 | Average loss: 0.2723\n",
      "Average training batch loss at epoch 129: 0.2797\n",
      "Average validation fold accuracy at epoch 129: 0.2784\n",
      "Epoch: 130 | Training Batch: 10 | Average loss: 0.2915\n",
      "Epoch: 130 | Training Batch: 20 | Average loss: 0.2662\n",
      "Epoch: 130 | Training Batch: 30 | Average loss: 0.2757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 130 | Training Batch: 40 | Average loss: 0.2819\n",
      "Average training batch loss at epoch 130: 0.2797\n",
      "Average validation fold accuracy at epoch 130: 0.2784\n",
      "Epoch: 131 | Training Batch: 10 | Average loss: 0.2707\n",
      "Epoch: 131 | Training Batch: 20 | Average loss: 0.2870\n",
      "Epoch: 131 | Training Batch: 30 | Average loss: 0.2756\n",
      "Epoch: 131 | Training Batch: 40 | Average loss: 0.2653\n",
      "Average training batch loss at epoch 131: 0.2797\n",
      "Average validation fold accuracy at epoch 131: 0.2784\n",
      "Epoch: 132 | Training Batch: 10 | Average loss: 0.2668\n",
      "Epoch: 132 | Training Batch: 20 | Average loss: 0.2991\n",
      "Epoch: 132 | Training Batch: 30 | Average loss: 0.2792\n",
      "Epoch: 132 | Training Batch: 40 | Average loss: 0.2713\n",
      "Average training batch loss at epoch 132: 0.2797\n",
      "Average validation fold accuracy at epoch 132: 0.2783\n",
      "Epoch: 133 | Training Batch: 10 | Average loss: 0.2965\n",
      "Epoch: 133 | Training Batch: 20 | Average loss: 0.2798\n",
      "Epoch: 133 | Training Batch: 30 | Average loss: 0.2700\n",
      "Epoch: 133 | Training Batch: 40 | Average loss: 0.2679\n",
      "Average training batch loss at epoch 133: 0.2797\n",
      "Average validation fold accuracy at epoch 133: 0.2783\n",
      "Epoch: 134 | Training Batch: 10 | Average loss: 0.2782\n",
      "Epoch: 134 | Training Batch: 20 | Average loss: 0.2782\n",
      "Epoch: 134 | Training Batch: 30 | Average loss: 0.2884\n",
      "Epoch: 134 | Training Batch: 40 | Average loss: 0.2756\n",
      "Average training batch loss at epoch 134: 0.2797\n",
      "Average validation fold accuracy at epoch 134: 0.2783\n",
      "Epoch: 135 | Training Batch: 10 | Average loss: 0.2643\n",
      "Epoch: 135 | Training Batch: 20 | Average loss: 0.2705\n",
      "Epoch: 135 | Training Batch: 30 | Average loss: 0.2905\n",
      "Epoch: 135 | Training Batch: 40 | Average loss: 0.2802\n",
      "Average training batch loss at epoch 135: 0.2796\n",
      "Average validation fold accuracy at epoch 135: 0.2782\n",
      "Epoch: 136 | Training Batch: 10 | Average loss: 0.2900\n",
      "Epoch: 136 | Training Batch: 20 | Average loss: 0.2728\n",
      "Epoch: 136 | Training Batch: 30 | Average loss: 0.3028\n",
      "Epoch: 136 | Training Batch: 40 | Average loss: 0.2749\n",
      "Average training batch loss at epoch 136: 0.2797\n",
      "Average validation fold accuracy at epoch 136: 0.2783\n",
      "Epoch: 137 | Training Batch: 10 | Average loss: 0.2742\n",
      "Epoch: 137 | Training Batch: 20 | Average loss: 0.2818\n",
      "Epoch: 137 | Training Batch: 30 | Average loss: 0.2746\n",
      "Epoch: 137 | Training Batch: 40 | Average loss: 0.2758\n",
      "Average training batch loss at epoch 137: 0.2797\n",
      "Average validation fold accuracy at epoch 137: 0.2782\n",
      "Epoch: 138 | Training Batch: 10 | Average loss: 0.2903\n",
      "Epoch: 138 | Training Batch: 20 | Average loss: 0.3040\n",
      "Epoch: 138 | Training Batch: 30 | Average loss: 0.2720\n",
      "Epoch: 138 | Training Batch: 40 | Average loss: 0.2725\n",
      "Average training batch loss at epoch 138: 0.2797\n",
      "Average validation fold accuracy at epoch 138: 0.2783\n",
      "Epoch: 139 | Training Batch: 10 | Average loss: 0.2780\n",
      "Epoch: 139 | Training Batch: 20 | Average loss: 0.2723\n",
      "Epoch: 139 | Training Batch: 30 | Average loss: 0.2614\n",
      "Epoch: 139 | Training Batch: 40 | Average loss: 0.2606\n",
      "Average training batch loss at epoch 139: 0.2796\n",
      "Average validation fold accuracy at epoch 139: 0.2782\n",
      "Epoch: 140 | Training Batch: 10 | Average loss: 0.2763\n",
      "Epoch: 140 | Training Batch: 20 | Average loss: 0.2718\n",
      "Epoch: 140 | Training Batch: 30 | Average loss: 0.2925\n",
      "Epoch: 140 | Training Batch: 40 | Average loss: 0.2747\n",
      "Average training batch loss at epoch 140: 0.2796\n",
      "Average validation fold accuracy at epoch 140: 0.2784\n",
      "Epoch: 141 | Training Batch: 10 | Average loss: 0.2794\n",
      "Epoch: 141 | Training Batch: 20 | Average loss: 0.2742\n",
      "Epoch: 141 | Training Batch: 30 | Average loss: 0.2928\n",
      "Epoch: 141 | Training Batch: 40 | Average loss: 0.2777\n",
      "Average training batch loss at epoch 141: 0.2796\n",
      "Average validation fold accuracy at epoch 141: 0.2784\n",
      "Epoch: 142 | Training Batch: 10 | Average loss: 0.2788\n",
      "Epoch: 142 | Training Batch: 20 | Average loss: 0.2673\n",
      "Epoch: 142 | Training Batch: 30 | Average loss: 0.2805\n",
      "Epoch: 142 | Training Batch: 40 | Average loss: 0.2852\n",
      "Average training batch loss at epoch 142: 0.2796\n",
      "Average validation fold accuracy at epoch 142: 0.2784\n",
      "Epoch: 143 | Training Batch: 10 | Average loss: 0.2863\n",
      "Epoch: 143 | Training Batch: 20 | Average loss: 0.2726\n",
      "Epoch: 143 | Training Batch: 30 | Average loss: 0.2833\n",
      "Epoch: 143 | Training Batch: 40 | Average loss: 0.2746\n",
      "Average training batch loss at epoch 143: 0.2796\n",
      "Average validation fold accuracy at epoch 143: 0.2785\n",
      "Epoch: 144 | Training Batch: 10 | Average loss: 0.2668\n",
      "Epoch: 144 | Training Batch: 20 | Average loss: 0.2701\n",
      "Epoch: 144 | Training Batch: 30 | Average loss: 0.2847\n",
      "Epoch: 144 | Training Batch: 40 | Average loss: 0.2716\n",
      "Average training batch loss at epoch 144: 0.2796\n",
      "Average validation fold accuracy at epoch 144: 0.2786\n",
      "Epoch: 145 | Training Batch: 10 | Average loss: 0.2644\n",
      "Epoch: 145 | Training Batch: 20 | Average loss: 0.2649\n",
      "Epoch: 145 | Training Batch: 30 | Average loss: 0.2728\n",
      "Epoch: 145 | Training Batch: 40 | Average loss: 0.2799\n",
      "Average training batch loss at epoch 145: 0.2795\n",
      "Average validation fold accuracy at epoch 145: 0.2786\n",
      "Epoch: 146 | Training Batch: 10 | Average loss: 0.2839\n",
      "Epoch: 146 | Training Batch: 20 | Average loss: 0.2733\n",
      "Epoch: 146 | Training Batch: 30 | Average loss: 0.2862\n",
      "Epoch: 146 | Training Batch: 40 | Average loss: 0.2875\n",
      "Average training batch loss at epoch 146: 0.2795\n",
      "Average validation fold accuracy at epoch 146: 0.2787\n",
      "Epoch: 147 | Training Batch: 10 | Average loss: 0.2746\n",
      "Epoch: 147 | Training Batch: 20 | Average loss: 0.2839\n",
      "Epoch: 147 | Training Batch: 30 | Average loss: 0.2635\n",
      "Epoch: 147 | Training Batch: 40 | Average loss: 0.2890\n",
      "Average training batch loss at epoch 147: 0.2795\n",
      "Average validation fold accuracy at epoch 147: 0.2786\n",
      "Epoch: 148 | Training Batch: 10 | Average loss: 0.2752\n",
      "Epoch: 148 | Training Batch: 20 | Average loss: 0.2763\n",
      "Epoch: 148 | Training Batch: 30 | Average loss: 0.2690\n",
      "Epoch: 148 | Training Batch: 40 | Average loss: 0.2775\n",
      "Average training batch loss at epoch 148: 0.2795\n",
      "Average validation fold accuracy at epoch 148: 0.2787\n",
      "Epoch: 149 | Training Batch: 10 | Average loss: 0.2892\n",
      "Epoch: 149 | Training Batch: 20 | Average loss: 0.2642\n",
      "Epoch: 149 | Training Batch: 30 | Average loss: 0.2599\n",
      "Epoch: 149 | Training Batch: 40 | Average loss: 0.2727\n",
      "Average training batch loss at epoch 149: 0.2794\n",
      "Average validation fold accuracy at epoch 149: 0.2786\n",
      "Epoch: 150 | Training Batch: 10 | Average loss: 0.2810\n",
      "Epoch: 150 | Training Batch: 20 | Average loss: 0.2823\n",
      "Epoch: 150 | Training Batch: 30 | Average loss: 0.2773\n",
      "Epoch: 150 | Training Batch: 40 | Average loss: 0.2794\n",
      "Average training batch loss at epoch 150: 0.2794\n",
      "Average validation fold accuracy at epoch 150: 0.2786\n",
      "Epoch: 151 | Training Batch: 10 | Average loss: 0.2580\n",
      "Epoch: 151 | Training Batch: 20 | Average loss: 0.2752\n",
      "Epoch: 151 | Training Batch: 30 | Average loss: 0.2888\n",
      "Epoch: 151 | Training Batch: 40 | Average loss: 0.2778\n",
      "Average training batch loss at epoch 151: 0.2794\n",
      "Average validation fold accuracy at epoch 151: 0.2787\n",
      "Epoch: 152 | Training Batch: 10 | Average loss: 0.2869\n",
      "Epoch: 152 | Training Batch: 20 | Average loss: 0.2614\n",
      "Epoch: 152 | Training Batch: 30 | Average loss: 0.2742\n",
      "Epoch: 152 | Training Batch: 40 | Average loss: 0.2726\n",
      "Average training batch loss at epoch 152: 0.2793\n",
      "Average validation fold accuracy at epoch 152: 0.2785\n",
      "Epoch: 153 | Training Batch: 10 | Average loss: 0.2733\n",
      "Epoch: 153 | Training Batch: 20 | Average loss: 0.2878\n",
      "Epoch: 153 | Training Batch: 30 | Average loss: 0.2674\n",
      "Epoch: 153 | Training Batch: 40 | Average loss: 0.2892\n",
      "Average training batch loss at epoch 153: 0.2793\n",
      "Average validation fold accuracy at epoch 153: 0.2784\n",
      "Epoch: 154 | Training Batch: 10 | Average loss: 0.2668\n",
      "Epoch: 154 | Training Batch: 20 | Average loss: 0.2785\n",
      "Epoch: 154 | Training Batch: 30 | Average loss: 0.2745\n",
      "Epoch: 154 | Training Batch: 40 | Average loss: 0.2628\n",
      "Average training batch loss at epoch 154: 0.2793\n",
      "Average validation fold accuracy at epoch 154: 0.2784\n",
      "Epoch: 155 | Training Batch: 10 | Average loss: 0.2855\n",
      "Epoch: 155 | Training Batch: 20 | Average loss: 0.2814\n",
      "Epoch: 155 | Training Batch: 30 | Average loss: 0.2729\n",
      "Epoch: 155 | Training Batch: 40 | Average loss: 0.2710\n",
      "Average training batch loss at epoch 155: 0.2793\n",
      "Average validation fold accuracy at epoch 155: 0.2783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 156 | Training Batch: 10 | Average loss: 0.2896\n",
      "Epoch: 156 | Training Batch: 20 | Average loss: 0.2702\n",
      "Epoch: 156 | Training Batch: 30 | Average loss: 0.2728\n",
      "Epoch: 156 | Training Batch: 40 | Average loss: 0.2893\n",
      "Average training batch loss at epoch 156: 0.2793\n",
      "Average validation fold accuracy at epoch 156: 0.2783\n",
      "Epoch: 157 | Training Batch: 10 | Average loss: 0.2797\n",
      "Epoch: 157 | Training Batch: 20 | Average loss: 0.2745\n",
      "Epoch: 157 | Training Batch: 30 | Average loss: 0.2936\n",
      "Epoch: 157 | Training Batch: 40 | Average loss: 0.2788\n",
      "Average training batch loss at epoch 157: 0.2793\n",
      "Average validation fold accuracy at epoch 157: 0.2783\n",
      "Epoch: 158 | Training Batch: 10 | Average loss: 0.2621\n",
      "Epoch: 158 | Training Batch: 20 | Average loss: 0.2825\n",
      "Epoch: 158 | Training Batch: 30 | Average loss: 0.2934\n",
      "Epoch: 158 | Training Batch: 40 | Average loss: 0.2845\n",
      "Average training batch loss at epoch 158: 0.2793\n",
      "Average validation fold accuracy at epoch 158: 0.2782\n",
      "Epoch: 159 | Training Batch: 10 | Average loss: 0.2754\n",
      "Epoch: 159 | Training Batch: 20 | Average loss: 0.2782\n",
      "Epoch: 159 | Training Batch: 30 | Average loss: 0.2919\n",
      "Epoch: 159 | Training Batch: 40 | Average loss: 0.2718\n",
      "Average training batch loss at epoch 159: 0.2793\n",
      "Average validation fold accuracy at epoch 159: 0.2782\n",
      "Epoch: 160 | Training Batch: 10 | Average loss: 0.2765\n",
      "Epoch: 160 | Training Batch: 20 | Average loss: 0.2893\n",
      "Epoch: 160 | Training Batch: 30 | Average loss: 0.2875\n",
      "Epoch: 160 | Training Batch: 40 | Average loss: 0.2685\n",
      "Average training batch loss at epoch 160: 0.2793\n",
      "Average validation fold accuracy at epoch 160: 0.2783\n",
      "Epoch: 161 | Training Batch: 10 | Average loss: 0.2719\n",
      "Epoch: 161 | Training Batch: 20 | Average loss: 0.2776\n",
      "Epoch: 161 | Training Batch: 30 | Average loss: 0.2755\n",
      "Epoch: 161 | Training Batch: 40 | Average loss: 0.2730\n",
      "Average training batch loss at epoch 161: 0.2793\n",
      "Average validation fold accuracy at epoch 161: 0.2783\n",
      "Epoch: 162 | Training Batch: 10 | Average loss: 0.2807\n",
      "Epoch: 162 | Training Batch: 20 | Average loss: 0.2862\n",
      "Epoch: 162 | Training Batch: 30 | Average loss: 0.2915\n",
      "Epoch: 162 | Training Batch: 40 | Average loss: 0.2654\n",
      "Average training batch loss at epoch 162: 0.2793\n",
      "Average validation fold accuracy at epoch 162: 0.2784\n",
      "Epoch: 163 | Training Batch: 10 | Average loss: 0.2731\n",
      "Epoch: 163 | Training Batch: 20 | Average loss: 0.2727\n",
      "Epoch: 163 | Training Batch: 30 | Average loss: 0.2839\n",
      "Epoch: 163 | Training Batch: 40 | Average loss: 0.2805\n",
      "Average training batch loss at epoch 163: 0.2793\n",
      "Average validation fold accuracy at epoch 163: 0.2784\n",
      "Epoch: 164 | Training Batch: 10 | Average loss: 0.2712\n",
      "Epoch: 164 | Training Batch: 20 | Average loss: 0.2714\n",
      "Epoch: 164 | Training Batch: 30 | Average loss: 0.2849\n",
      "Epoch: 164 | Training Batch: 40 | Average loss: 0.2966\n",
      "Average training batch loss at epoch 164: 0.2793\n",
      "Average validation fold accuracy at epoch 164: 0.2784\n",
      "Epoch: 165 | Training Batch: 10 | Average loss: 0.2888\n",
      "Epoch: 165 | Training Batch: 20 | Average loss: 0.2780\n",
      "Epoch: 165 | Training Batch: 30 | Average loss: 0.2852\n",
      "Epoch: 165 | Training Batch: 40 | Average loss: 0.2710\n",
      "Average training batch loss at epoch 165: 0.2793\n",
      "Average validation fold accuracy at epoch 165: 0.2783\n",
      "Epoch: 166 | Training Batch: 10 | Average loss: 0.2691\n",
      "Epoch: 166 | Training Batch: 20 | Average loss: 0.2773\n",
      "Epoch: 166 | Training Batch: 30 | Average loss: 0.2641\n",
      "Epoch: 166 | Training Batch: 40 | Average loss: 0.2807\n",
      "Average training batch loss at epoch 166: 0.2793\n",
      "Average validation fold accuracy at epoch 166: 0.2783\n",
      "Epoch: 167 | Training Batch: 10 | Average loss: 0.2732\n",
      "Epoch: 167 | Training Batch: 20 | Average loss: 0.2884\n",
      "Epoch: 167 | Training Batch: 30 | Average loss: 0.2725\n",
      "Epoch: 167 | Training Batch: 40 | Average loss: 0.2901\n",
      "Average training batch loss at epoch 167: 0.2793\n",
      "Average validation fold accuracy at epoch 167: 0.2785\n",
      "Epoch: 168 | Training Batch: 10 | Average loss: 0.2821\n",
      "Epoch: 168 | Training Batch: 20 | Average loss: 0.2718\n",
      "Epoch: 168 | Training Batch: 30 | Average loss: 0.2599\n",
      "Epoch: 168 | Training Batch: 40 | Average loss: 0.3000\n",
      "Average training batch loss at epoch 168: 0.2793\n",
      "Average validation fold accuracy at epoch 168: 0.2785\n",
      "Epoch: 169 | Training Batch: 10 | Average loss: 0.2957\n",
      "Epoch: 169 | Training Batch: 20 | Average loss: 0.2740\n",
      "Epoch: 169 | Training Batch: 30 | Average loss: 0.2761\n",
      "Epoch: 169 | Training Batch: 40 | Average loss: 0.2860\n",
      "Average training batch loss at epoch 169: 0.2793\n",
      "Average validation fold accuracy at epoch 169: 0.2785\n",
      "Epoch: 170 | Training Batch: 10 | Average loss: 0.2913\n",
      "Epoch: 170 | Training Batch: 20 | Average loss: 0.2749\n",
      "Epoch: 170 | Training Batch: 30 | Average loss: 0.2702\n",
      "Epoch: 170 | Training Batch: 40 | Average loss: 0.2788\n",
      "Average training batch loss at epoch 170: 0.2793\n",
      "Average validation fold accuracy at epoch 170: 0.2785\n",
      "Epoch: 171 | Training Batch: 10 | Average loss: 0.2931\n",
      "Epoch: 171 | Training Batch: 20 | Average loss: 0.2711\n",
      "Epoch: 171 | Training Batch: 30 | Average loss: 0.2818\n",
      "Epoch: 171 | Training Batch: 40 | Average loss: 0.2699\n",
      "Average training batch loss at epoch 171: 0.2793\n",
      "Average validation fold accuracy at epoch 171: 0.2786\n",
      "Epoch: 172 | Training Batch: 10 | Average loss: 0.2717\n",
      "Epoch: 172 | Training Batch: 20 | Average loss: 0.2919\n",
      "Epoch: 172 | Training Batch: 30 | Average loss: 0.2823\n",
      "Epoch: 172 | Training Batch: 40 | Average loss: 0.2850\n",
      "Average training batch loss at epoch 172: 0.2793\n",
      "Average validation fold accuracy at epoch 172: 0.2785\n",
      "Epoch: 173 | Training Batch: 10 | Average loss: 0.2824\n",
      "Epoch: 173 | Training Batch: 20 | Average loss: 0.2701\n",
      "Epoch: 173 | Training Batch: 30 | Average loss: 0.2790\n",
      "Epoch: 173 | Training Batch: 40 | Average loss: 0.2794\n",
      "Average training batch loss at epoch 173: 0.2793\n",
      "Average validation fold accuracy at epoch 173: 0.2784\n",
      "Epoch: 174 | Training Batch: 10 | Average loss: 0.2775\n",
      "Epoch: 174 | Training Batch: 20 | Average loss: 0.2703\n",
      "Epoch: 174 | Training Batch: 30 | Average loss: 0.2774\n",
      "Epoch: 174 | Training Batch: 40 | Average loss: 0.2924\n",
      "Average training batch loss at epoch 174: 0.2793\n",
      "Average validation fold accuracy at epoch 174: 0.2785\n",
      "Epoch: 175 | Training Batch: 10 | Average loss: 0.2859\n",
      "Epoch: 175 | Training Batch: 20 | Average loss: 0.2868\n",
      "Epoch: 175 | Training Batch: 30 | Average loss: 0.2758\n",
      "Epoch: 175 | Training Batch: 40 | Average loss: 0.2717\n",
      "Average training batch loss at epoch 175: 0.2794\n",
      "Average validation fold accuracy at epoch 175: 0.2785\n",
      "Epoch: 176 | Training Batch: 10 | Average loss: 0.2797\n",
      "Epoch: 176 | Training Batch: 20 | Average loss: 0.2855\n",
      "Epoch: 176 | Training Batch: 30 | Average loss: 0.2819\n",
      "Epoch: 176 | Training Batch: 40 | Average loss: 0.2784\n",
      "Average training batch loss at epoch 176: 0.2794\n",
      "Average validation fold accuracy at epoch 176: 0.2784\n",
      "Epoch: 177 | Training Batch: 10 | Average loss: 0.2848\n",
      "Epoch: 177 | Training Batch: 20 | Average loss: 0.2930\n",
      "Epoch: 177 | Training Batch: 30 | Average loss: 0.2722\n",
      "Epoch: 177 | Training Batch: 40 | Average loss: 0.2812\n",
      "Average training batch loss at epoch 177: 0.2794\n",
      "Average validation fold accuracy at epoch 177: 0.2782\n",
      "Epoch: 178 | Training Batch: 10 | Average loss: 0.2679\n",
      "Epoch: 178 | Training Batch: 20 | Average loss: 0.2745\n",
      "Epoch: 178 | Training Batch: 30 | Average loss: 0.2720\n",
      "Epoch: 178 | Training Batch: 40 | Average loss: 0.2851\n",
      "Average training batch loss at epoch 178: 0.2793\n",
      "Average validation fold accuracy at epoch 178: 0.2782\n",
      "Epoch: 179 | Training Batch: 10 | Average loss: 0.2903\n",
      "Epoch: 179 | Training Batch: 20 | Average loss: 0.2760\n",
      "Epoch: 179 | Training Batch: 30 | Average loss: 0.2718\n",
      "Epoch: 179 | Training Batch: 40 | Average loss: 0.2718\n",
      "Average training batch loss at epoch 179: 0.2793\n",
      "Average validation fold accuracy at epoch 179: 0.2782\n",
      "Epoch: 180 | Training Batch: 10 | Average loss: 0.2683\n",
      "Epoch: 180 | Training Batch: 20 | Average loss: 0.2576\n",
      "Epoch: 180 | Training Batch: 30 | Average loss: 0.2944\n",
      "Epoch: 180 | Training Batch: 40 | Average loss: 0.2784\n",
      "Average training batch loss at epoch 180: 0.2793\n",
      "Average validation fold accuracy at epoch 180: 0.2783\n",
      "Epoch: 181 | Training Batch: 10 | Average loss: 0.2699\n",
      "Epoch: 181 | Training Batch: 20 | Average loss: 0.2683\n",
      "Epoch: 181 | Training Batch: 30 | Average loss: 0.2853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 181 | Training Batch: 40 | Average loss: 0.2747\n",
      "Average training batch loss at epoch 181: 0.2793\n",
      "Average validation fold accuracy at epoch 181: 0.2782\n",
      "Epoch: 182 | Training Batch: 10 | Average loss: 0.2629\n",
      "Epoch: 182 | Training Batch: 20 | Average loss: 0.2627\n",
      "Epoch: 182 | Training Batch: 30 | Average loss: 0.2836\n",
      "Epoch: 182 | Training Batch: 40 | Average loss: 0.2870\n",
      "Average training batch loss at epoch 182: 0.2793\n",
      "Average validation fold accuracy at epoch 182: 0.2782\n",
      "Epoch: 183 | Training Batch: 10 | Average loss: 0.2868\n",
      "Epoch: 183 | Training Batch: 20 | Average loss: 0.2697\n",
      "Epoch: 183 | Training Batch: 30 | Average loss: 0.2726\n",
      "Epoch: 183 | Training Batch: 40 | Average loss: 0.2833\n",
      "Average training batch loss at epoch 183: 0.2792\n",
      "Average validation fold accuracy at epoch 183: 0.2783\n",
      "Epoch: 184 | Training Batch: 10 | Average loss: 0.2781\n",
      "Epoch: 184 | Training Batch: 20 | Average loss: 0.2765\n",
      "Epoch: 184 | Training Batch: 30 | Average loss: 0.2825\n",
      "Epoch: 184 | Training Batch: 40 | Average loss: 0.3000\n",
      "Average training batch loss at epoch 184: 0.2793\n",
      "Average validation fold accuracy at epoch 184: 0.2783\n",
      "Epoch: 185 | Training Batch: 10 | Average loss: 0.2870\n",
      "Epoch: 185 | Training Batch: 20 | Average loss: 0.2839\n",
      "Epoch: 185 | Training Batch: 30 | Average loss: 0.2825\n",
      "Epoch: 185 | Training Batch: 40 | Average loss: 0.2725\n",
      "Average training batch loss at epoch 185: 0.2793\n",
      "Average validation fold accuracy at epoch 185: 0.2783\n",
      "Epoch: 186 | Training Batch: 10 | Average loss: 0.2703\n",
      "Epoch: 186 | Training Batch: 20 | Average loss: 0.2895\n",
      "Epoch: 186 | Training Batch: 30 | Average loss: 0.2658\n",
      "Epoch: 186 | Training Batch: 40 | Average loss: 0.2752\n",
      "Average training batch loss at epoch 186: 0.2793\n",
      "Average validation fold accuracy at epoch 186: 0.2782\n",
      "Epoch: 187 | Training Batch: 10 | Average loss: 0.2675\n",
      "Epoch: 187 | Training Batch: 20 | Average loss: 0.2817\n",
      "Epoch: 187 | Training Batch: 30 | Average loss: 0.2733\n",
      "Epoch: 187 | Training Batch: 40 | Average loss: 0.2860\n",
      "Average training batch loss at epoch 187: 0.2792\n",
      "Average validation fold accuracy at epoch 187: 0.2783\n",
      "Epoch: 188 | Training Batch: 10 | Average loss: 0.2722\n",
      "Epoch: 188 | Training Batch: 20 | Average loss: 0.2709\n",
      "Epoch: 188 | Training Batch: 30 | Average loss: 0.2788\n",
      "Epoch: 188 | Training Batch: 40 | Average loss: 0.2598\n",
      "Average training batch loss at epoch 188: 0.2792\n",
      "Average validation fold accuracy at epoch 188: 0.2783\n",
      "Epoch: 189 | Training Batch: 10 | Average loss: 0.2741\n",
      "Epoch: 189 | Training Batch: 20 | Average loss: 0.2739\n",
      "Epoch: 189 | Training Batch: 30 | Average loss: 0.2699\n",
      "Epoch: 189 | Training Batch: 40 | Average loss: 0.2750\n",
      "Average training batch loss at epoch 189: 0.2792\n",
      "Average validation fold accuracy at epoch 189: 0.2783\n",
      "Epoch: 190 | Training Batch: 10 | Average loss: 0.2699\n",
      "Epoch: 190 | Training Batch: 20 | Average loss: 0.2586\n",
      "Epoch: 190 | Training Batch: 30 | Average loss: 0.2870\n",
      "Epoch: 190 | Training Batch: 40 | Average loss: 0.2761\n",
      "Average training batch loss at epoch 190: 0.2791\n",
      "Average validation fold accuracy at epoch 190: 0.2784\n",
      "Epoch: 191 | Training Batch: 10 | Average loss: 0.2796\n",
      "Epoch: 191 | Training Batch: 20 | Average loss: 0.2668\n",
      "Epoch: 191 | Training Batch: 30 | Average loss: 0.2813\n",
      "Epoch: 191 | Training Batch: 40 | Average loss: 0.2796\n",
      "Average training batch loss at epoch 191: 0.2792\n",
      "Average validation fold accuracy at epoch 191: 0.2784\n",
      "Epoch: 192 | Training Batch: 10 | Average loss: 0.2834\n",
      "Epoch: 192 | Training Batch: 20 | Average loss: 0.2664\n",
      "Epoch: 192 | Training Batch: 30 | Average loss: 0.2648\n",
      "Epoch: 192 | Training Batch: 40 | Average loss: 0.2825\n",
      "Average training batch loss at epoch 192: 0.2791\n",
      "Average validation fold accuracy at epoch 192: 0.2783\n",
      "Epoch: 193 | Training Batch: 10 | Average loss: 0.2801\n",
      "Epoch: 193 | Training Batch: 20 | Average loss: 0.2806\n",
      "Epoch: 193 | Training Batch: 30 | Average loss: 0.2724\n",
      "Epoch: 193 | Training Batch: 40 | Average loss: 0.2812\n",
      "Average training batch loss at epoch 193: 0.2791\n",
      "Average validation fold accuracy at epoch 193: 0.2783\n",
      "Epoch: 194 | Training Batch: 10 | Average loss: 0.2754\n",
      "Epoch: 194 | Training Batch: 20 | Average loss: 0.2845\n",
      "Epoch: 194 | Training Batch: 30 | Average loss: 0.2874\n",
      "Epoch: 194 | Training Batch: 40 | Average loss: 0.2771\n",
      "Average training batch loss at epoch 194: 0.2791\n",
      "Average validation fold accuracy at epoch 194: 0.2783\n",
      "Epoch: 195 | Training Batch: 10 | Average loss: 0.2728\n",
      "Epoch: 195 | Training Batch: 20 | Average loss: 0.2850\n",
      "Epoch: 195 | Training Batch: 30 | Average loss: 0.2677\n",
      "Epoch: 195 | Training Batch: 40 | Average loss: 0.2699\n",
      "Average training batch loss at epoch 195: 0.2791\n",
      "Average validation fold accuracy at epoch 195: 0.2782\n",
      "Epoch: 196 | Training Batch: 10 | Average loss: 0.2986\n",
      "Epoch: 196 | Training Batch: 20 | Average loss: 0.2672\n",
      "Epoch: 196 | Training Batch: 30 | Average loss: 0.2830\n",
      "Epoch: 196 | Training Batch: 40 | Average loss: 0.2857\n",
      "Average training batch loss at epoch 196: 0.2792\n",
      "Average validation fold accuracy at epoch 196: 0.2781\n",
      "Epoch: 197 | Training Batch: 10 | Average loss: 0.2752\n",
      "Epoch: 197 | Training Batch: 20 | Average loss: 0.2771\n",
      "Epoch: 197 | Training Batch: 30 | Average loss: 0.2736\n",
      "Epoch: 197 | Training Batch: 40 | Average loss: 0.2756\n",
      "Average training batch loss at epoch 197: 0.2791\n",
      "Average validation fold accuracy at epoch 197: 0.2781\n",
      "Epoch: 198 | Training Batch: 10 | Average loss: 0.2571\n",
      "Epoch: 198 | Training Batch: 20 | Average loss: 0.2648\n",
      "Epoch: 198 | Training Batch: 30 | Average loss: 0.2916\n",
      "Epoch: 198 | Training Batch: 40 | Average loss: 0.2789\n",
      "Average training batch loss at epoch 198: 0.2791\n",
      "Average validation fold accuracy at epoch 198: 0.2780\n",
      "Epoch: 199 | Training Batch: 10 | Average loss: 0.2702\n",
      "Epoch: 199 | Training Batch: 20 | Average loss: 0.2875\n",
      "Epoch: 199 | Training Batch: 30 | Average loss: 0.2622\n",
      "Epoch: 199 | Training Batch: 40 | Average loss: 0.2771\n",
      "Average training batch loss at epoch 199: 0.2791\n",
      "Average validation fold accuracy at epoch 199: 0.2780\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "pretrain=False\n",
    "batch_size = 32\n",
    "opt = {\n",
    "    'node_size':201,\n",
    "    'hidden_size':30,\n",
    "    'num_layers':1,\n",
    "    'embedding_dim':50,\n",
    "    'learning_rate':0.0001\n",
    "}\n",
    "\n",
    "# Initialize global tracking variables\n",
    "best_validation_accuracy = 0\n",
    "epochs_without_improvement = 0\n",
    "total_train_loss = list()\n",
    "total_valid_loss = []\n",
    "avg_trainings = []\n",
    "avg_valids = []\n",
    "\n",
    "\n",
    "# Loading model\n",
    "if pretrain:\n",
    "    classifier = torch.load('SiameseNN1.pt')\n",
    "else:\n",
    "    classifier = SiameseClassifier(opt, is_train=True)\n",
    "    # Initialize parameters\n",
    "    classifier.initialize_parameters()\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Initiate the training data loader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
    "    running_loss = list()\n",
    "    # Training loop\n",
    "    for i, (batch_x,label_var) in enumerate(train_loader):\n",
    "        s1_var = batch_x[:,0,:]\n",
    "        s2_var  = batch_x[:,1,:]\n",
    "        #s1_var = one_hot(s1_var)\n",
    "        #s2_var = one_hot(s2_var)\n",
    "        classifier.train_step(s1_var, s2_var, label_var)\n",
    "        train_batch_loss = classifier.loss.data[0]\n",
    "        running_loss.append(train_batch_loss)\n",
    "        total_train_loss.append(train_batch_loss)\n",
    "\n",
    "        if i % 10 == 0 and i != 0:\n",
    "            running_avg_loss = sum(running_loss) / len(running_loss)\n",
    "            print('Epoch: %d | Training Batch: %d | Average loss: %.4f' %\n",
    "                  (epoch, i , running_avg_loss))\n",
    "            running_loss = []\n",
    "            \n",
    "\n",
    "    # Report epoch statistics\n",
    "    avg_training_accuracy = sum(total_train_loss) / len(total_train_loss)\n",
    "    print('Average training batch loss at epoch %d: %.4f' % (epoch, avg_training_accuracy))\n",
    "    avg_trainings.append(avg_training_accuracy) \n",
    "    \n",
    "\n",
    "    # Validate after each epoch; set tracking variables\n",
    "    if epoch >= 0:\n",
    "        # Initiate the training data loader\n",
    "        valid_loader = DataLoader(val_dataset, batch_size=32,shuffle=True)\n",
    "        \n",
    "        # Validation loop (i.e. perform inference on the validation set)\n",
    "        for i, (batch_x,label_var) in enumerate(valid_loader):\n",
    "            s1_var = batch_x[:,0,:]\n",
    "            s2_var  = batch_x[:,1,:]\n",
    "            #s1_var = one_hot(s1_var)\n",
    "            #s2_var = one_hot(s2_var)\n",
    "            # Get predictions and update tracking values\n",
    "            classifier.test_step(s1_var, s2_var, label_var)\n",
    "            valid_batch_loss = classifier.loss.data[0]\n",
    "            total_valid_loss.append(valid_batch_loss)\n",
    "\n",
    "        # Report fold statistics\n",
    "        avg_valid_accuracy = sum(total_valid_loss) / len(total_valid_loss)\n",
    "        print('Average validation fold accuracy at epoch %d: %.4f' % (epoch, avg_valid_accuracy))\n",
    "        avg_valids.append(avg_valid_accuracy)\n",
    "        # Save network parameters if performance has improved\n",
    "        if avg_valid_accuracy <= best_validation_accuracy:\n",
    "            epochs_without_improvement += 1\n",
    "        else:\n",
    "            best_validation_accuracy = avg_valid_accuracy\n",
    "            epochs_without_improvement = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize= (20,5))\n",
    "ax.plot(avg_valids,marker='o',label='Val')\n",
    "ax.plot(avg_trainings,marker='o',label='Train')\n",
    "ax.set_xlabel('Loss',fontsize=15)\n",
    "ax.set_ylabel('Number of epochs',fontsize=15)\n",
    "ax.set_title('Loss',fontsize=20,fontweight =\"bold\")\n",
    "ax.legend()\n",
    "fig.savefig(\"Loss_graphWithFCWith0.0001lr.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richard/anaconda3/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/richard/anaconda3/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type LSTMEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(classifier, 'SiameseNNFC1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['embedding_table.weight', 'lstm_rnn.weight_ih_l0', 'lstm_rnn.weight_hh_l0', 'lstm_rnn.bias_ih_l0', 'lstm_rnn.bias_hh_l0', 'lstm_rnn.weight_ih_l1', 'lstm_rnn.weight_hh_l1', 'lstm_rnn.bias_ih_l1', 'lstm_rnn.bias_hh_l1', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.encoder_a.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "print(\"number of parameters in this model: \",get_n_params(classifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
