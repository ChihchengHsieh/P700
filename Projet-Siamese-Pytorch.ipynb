{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Load data\n",
    "with open(\"Fulldata.txt\", \"rb\") as fp:   # Unpickling\n",
    "    df = pickle.load(fp)\n",
    "X = df[['left','right']]     \n",
    "Y = df['target']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperate to training, validation, and test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 64)\n",
    "validation_size = int(len(X_train) * 0.1)\n",
    "training_size = len(X_train) - validation_size\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size=validation_size,random_state= 64)\n",
    "Y_test = Y_test.values\n",
    "Y_train = Y_train.values\n",
    "Y_validation = Y_validation.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 14400\n",
      "Validation size: 1600\n",
      "test size: 4000\n"
     ]
    }
   ],
   "source": [
    "print('Training size:',X_train.shape[0])\n",
    "print('Validation size:',X_validation.shape[0])\n",
    "print('test size:',X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check shape\n",
    "assert X_train['left'].shape == X_train['right'].shape\n",
    "assert len(X_train['left']) == len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two help function\n",
    "def one_hot(s):\n",
    "    nb_digits=201\n",
    "    batch_size = s.shape[0]\n",
    "    seqlen = s.shape[1]\n",
    "    s_onehot = torch.FloatTensor(batch_size,seqlen,nb_digits)\n",
    "    s_onehot.zero_()\n",
    "    s_onehot.scatter_(2, s.unsqueeze(2), 1)\n",
    "    return s_onehot\n",
    "def padding(data):\n",
    "    left = [] \n",
    "    maxlen= 50\n",
    "    for i in range(data.shape[0]):\n",
    "        diff = maxlen - len((data.iloc[i]['left']))\n",
    "        if diff>=1:\n",
    "            data.iloc[i]['left']+= [0]*diff\n",
    "        left.append((data.iloc[i]['left']))\n",
    "    right = [] \n",
    "    maxlen= 50\n",
    "    for i in range(data.shape[0]):\n",
    "        diff = maxlen - len((data.iloc[i]['right']))\n",
    "        if diff>=1:\n",
    "            data.iloc[i]['right']+= [0]*diff\n",
    "        right.append((data.iloc[i]['right']))\n",
    "    return torch.tensor(np.array([right,left])).transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding and creat the loaders\n",
    "X_train = padding(X_train)\n",
    "Y_train = torch.FloatTensor(np.array(Y_train))\n",
    "train_dataset  = Data.TensorDataset(X_train,Y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32,shuffle=True)\n",
    "\n",
    "X_validation = padding(X_validation)\n",
    "Y_validation = torch.FloatTensor(np.array(Y_validation))\n",
    "val_dataset  = Data.TensorDataset(X_validation,Y_validation)\n",
    "\n",
    "X_test = padding(X_test)\n",
    "Y_test = torch.FloatTensor(np.array(Y_test))\n",
    "test_dataset  = Data.TensorDataset(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    \"\"\" Implements the network type integrated within the Siamese RNN architecture. \"\"\"\n",
    "    def __init__(self, opt, is_train=False):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.node_size = opt['node_size']\n",
    "        self.name = 'sim_encoder'\n",
    "        self.hidden_size= opt['hidden_size']\n",
    "        self.num_layers= opt['num_layers']\n",
    "        self.embedding_dim = opt['embedding_dim']\n",
    "        self.bidirection_lstm = opt['bidirection_lstm']\n",
    "        self.seqlen = opt['seqlen']\n",
    "        self.batch_size = opt['batch_size']\n",
    "        self.embedding_table = nn.Embedding(num_embeddings=self.node_size, embedding_dim=self.embedding_dim,\n",
    "                                          padding_idx=0, max_norm=None, scale_grad_by_freq=False, sparse=False)\n",
    "        self.lstm_rnn = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.hidden_size,\n",
    "                                num_layers=self.num_layers,bidirectional=self.bidirection_lstm)\n",
    "        if self.bidirection_lstm:\n",
    "            fc_size1= self.hidden_size*self.seqlen*2\n",
    "            fc_size2= 128\n",
    "        else:\n",
    "            fc_size1= self.hidden_size\n",
    "            fc_size2= 16\n",
    "        self.fc1= nn.Linear(fc_size1,fc_size2)\n",
    "        self.fc2= nn.Linear(fc_size2,2)\n",
    "        \n",
    "    def initialize_hidden_plus_cell(self, batch_size):\n",
    "        \"\"\" Re-initializes the hidden state, cell state, and the forget gate bias of the network. \"\"\"\n",
    "        if self.bidirection_lstm: \n",
    "            h0_size= self.num_layers*2\n",
    "        else:\n",
    "            h0_size= self.num_layers\n",
    "        zero_hidden = torch.randn(h0_size, batch_size, self.hidden_size)\n",
    "        zero_cell = torch.randn(h0_size, batch_size,self.hidden_size)\n",
    "        return zero_hidden, zero_cell\n",
    "\n",
    "    def forward(self, input_data, hidden, cell):\n",
    "        \"\"\" Performs a forward pass through the network. \"\"\"\n",
    "        output = self.embedding_table(input_data)\n",
    "        output, (hidden, cell) = self.lstm_rnn(output, (hidden, cell))\n",
    "        #print(output.shape)\n",
    "        #if self.bidirection_lstm:\n",
    "        #    output = nn.functional.relu(self.fc1(output.transpose(1,0).contiguous().view(self.batch_size,-1)))\n",
    "        #else:\n",
    "        #    output = nn.functional.relu(self.fc1(output[-1]))\n",
    "        #output = self.fc2(output) \n",
    "        return output[-1], hidden[-1], cell[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseClassifier(nn.Module):\n",
    "    \"\"\" Sentence similarity estimator implementing a siamese arcitecture. Uses pretrained word2vec embeddings. \n",
    "    Different to the paper, the weights are untied, to avoid exploding/ vanishing gradients. \"\"\"\n",
    "    def __init__(self, opt, is_train=False):\n",
    "        super(SiameseClassifier, self).__init__()\n",
    "        self.learning_rate= opt['learning_rate']\n",
    "        # Initialize network\n",
    "        self.encoder_a =  LSTMEncoder(opt, is_train)\n",
    "        # Initialize network parameters\n",
    "        self.initialize_parameters()\n",
    "        # Declare loss function\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        # Initialize network optimizers\n",
    "        self.optimizer_a = optim.Adam(self.encoder_a.parameters(), lr=self.learning_rate,\n",
    "                                      betas=(0.9, 0.999),weight_decay=0)\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\" Performs a single forward pass through the siamese architecture. \"\"\"\n",
    "        \n",
    "        # Obtain the input length (each batch consists of padded sentences)\n",
    "        input_length = self.batch_a.size(0)\n",
    "        \n",
    "        # Obtain sentence encodings from each encoder\n",
    "        hidden_a, cell_a = self.encoder_a.initialize_hidden_plus_cell(self.batch_size)\n",
    "        output_a, hidden_a, cell_a = self.encoder_a(self.batch_a, hidden_a, cell_a)\n",
    "\n",
    "        hidden_b, cell_b = self.encoder_a.initialize_hidden_plus_cell(self.batch_size)\n",
    "        output_b, hidden_b, cell_b = self.encoder_a(self.batch_b, hidden_b, cell_b)\n",
    "\n",
    "        # Format sentence encodings as 2D tensors\n",
    "        self.encoding_a = output_a.squeeze()\n",
    "        self.encoding_b = output_b.squeeze()\n",
    "\n",
    "        # Obtain similarity score predictions by calculating the Manhattan distance between sentence encodings\n",
    "        if self.batch_size == 1:\n",
    "            self.prediction = torch.exp(-torch.norm((self.encoding_a - self.encoding_b), 1))\n",
    "        else:\n",
    "            self.prediction = torch.exp(-torch.norm((self.encoding_a - self.encoding_b), 1, 1))\n",
    "            \n",
    "\n",
    "    def get_loss(self):\n",
    "        \"\"\" Calculates the MSE loss between the network predictions and the ground truth. \"\"\"\n",
    "        # Loss is the L1 norm of the difference between the obtained sentence encodings\n",
    "        self.loss = self.loss_function(self.prediction, self.labels)\n",
    "\n",
    "    def load_pretrained_parameters(self,pretrained_state_dict_path):\n",
    "        \"\"\" Loads the parameters learned during the pre-training on the SemEval data. \"\"\"\n",
    "        self.encoder_a.load_state_dict(torch.load(pretrained_state_dict_path))\n",
    "        print('Pretrained parameters have been successfully loaded into the encoder networks.')\n",
    "    \n",
    "    def save_lstm(self,path):\n",
    "        torch.save(self.encoder_a.state_dict(), path)\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\" Initializes network parameters. \"\"\"\n",
    "        state_dict = self.encoder_a.state_dict()\n",
    "        for key in state_dict.keys():\n",
    "            if '.weight' in key:\n",
    "                state_dict[key] = torch.nn.init.xavier_normal_((state_dict[key]),gain=1)\n",
    "            if '.bias' in key:\n",
    "                bias_length = state_dict[key].size()[0]\n",
    "                start, end = bias_length // 4, bias_length // 2\n",
    "                state_dict[key][start:end].fill_(2.5)\n",
    "        self.encoder_a.load_state_dict(state_dict)\n",
    "        \n",
    "    def initialize_parametersX(self):\n",
    "        for p in self.encoder_a.parameters():\n",
    "            nn.init.xavier_normal_(p)\n",
    "\n",
    "    def train_step(self, train_batch_a, train_batch_b, train_labels):\n",
    "        \"\"\" Optimizes the parameters of the active networks, i.e. performs a single training step. \"\"\"\n",
    "        # Get batches\n",
    "        self.batch_a = train_batch_a.transpose(0,1)\n",
    "        self.batch_b = train_batch_b.transpose(0,1)\n",
    "        self.labels = train_labels\n",
    "        self.batch_size = self.batch_a.size(1)\n",
    "        self.encoder_a.zero_grad() \n",
    "        self.forward()\n",
    "        self.get_loss()\n",
    "        #l2_reg = None\n",
    "        #for i in classifier.encoder_a.lstm_rnn.parameters():\n",
    "        #    if l2_reg is None:\n",
    "        #        l2_reg = W.norm(2)\n",
    "        #    else:\n",
    "        #        l2_reg = l2_reg + W.norm(2)\n",
    "        #self.loss += l2_reg\n",
    "        self.loss.backward()\n",
    "        clip_grad_norm(self.encoder_a.parameters(), 0.25)\n",
    "        self.optimizer_a.step()\n",
    "\n",
    "    def test_step(self, test_batch_a, test_batch_b, test_labels):\n",
    "        \"\"\" Performs a single test step. \"\"\"\n",
    "        self.batch_a = test_batch_a.transpose(0,1)\n",
    "        self.batch_b = test_batch_b.transpose(0,1)\n",
    "        self.labels = test_labels\n",
    "        self.batch_size = self.batch_a.size(1)\n",
    "        self.forward()\n",
    "        self.get_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate:  0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richard/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:89: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/Users/richard/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training Batch: 10 | Average loss: 0.4636\n",
      "Epoch: 0 | Training Batch: 20 | Average loss: 0.4552\n",
      "Epoch: 0 | Training Batch: 30 | Average loss: 0.4838\n",
      "Epoch: 0 | Training Batch: 40 | Average loss: 0.4478\n",
      "Epoch: 0 | Training Batch: 50 | Average loss: 0.4709\n",
      "Epoch: 0 | Training Batch: 60 | Average loss: 0.4216\n",
      "Epoch: 0 | Training Batch: 70 | Average loss: 0.4202\n",
      "Epoch: 0 | Training Batch: 80 | Average loss: 0.3849\n",
      "Epoch: 0 | Training Batch: 90 | Average loss: 0.3757\n",
      "Epoch: 0 | Training Batch: 100 | Average loss: 0.3959\n",
      "Epoch: 0 | Training Batch: 110 | Average loss: 0.3767\n",
      "Epoch: 0 | Training Batch: 120 | Average loss: 0.3903\n",
      "Epoch: 0 | Training Batch: 130 | Average loss: 0.3786\n",
      "Epoch: 0 | Training Batch: 140 | Average loss: 0.3356\n",
      "Epoch: 0 | Training Batch: 150 | Average loss: 0.3538\n",
      "Epoch: 0 | Training Batch: 160 | Average loss: 0.3254\n",
      "Epoch: 0 | Training Batch: 170 | Average loss: 0.3202\n",
      "Epoch: 0 | Training Batch: 180 | Average loss: 0.3360\n",
      "Epoch: 0 | Training Batch: 190 | Average loss: 0.3370\n",
      "Epoch: 0 | Training Batch: 200 | Average loss: 0.3206\n",
      "Epoch: 0 | Training Batch: 210 | Average loss: 0.3238\n",
      "Epoch: 0 | Training Batch: 220 | Average loss: 0.3251\n",
      "Epoch: 0 | Training Batch: 230 | Average loss: 0.2824\n",
      "Epoch: 0 | Training Batch: 240 | Average loss: 0.3081\n",
      "Epoch: 0 | Training Batch: 250 | Average loss: 0.3070\n",
      "Epoch: 0 | Training Batch: 260 | Average loss: 0.3104\n",
      "Epoch: 0 | Training Batch: 270 | Average loss: 0.3146\n",
      "Epoch: 0 | Training Batch: 280 | Average loss: 0.3215\n",
      "Epoch: 0 | Training Batch: 290 | Average loss: 0.3052\n",
      "Epoch: 0 | Training Batch: 300 | Average loss: 0.2895\n",
      "Epoch: 0 | Training Batch: 310 | Average loss: 0.3095\n",
      "Epoch: 0 | Training Batch: 320 | Average loss: 0.2709\n",
      "Epoch: 0 | Training Batch: 330 | Average loss: 0.3077\n",
      "Epoch: 0 | Training Batch: 340 | Average loss: 0.3088\n",
      "Epoch: 0 | Training Batch: 350 | Average loss: 0.2847\n",
      "Epoch: 0 | Training Batch: 360 | Average loss: 0.3016\n",
      "Epoch: 0 | Training Batch: 370 | Average loss: 0.3056\n",
      "Epoch: 0 | Training Batch: 380 | Average loss: 0.2867\n",
      "Epoch: 0 | Training Batch: 390 | Average loss: 0.2852\n",
      "Epoch: 0 | Training Batch: 400 | Average loss: 0.2716\n",
      "Epoch: 0 | Training Batch: 410 | Average loss: 0.2944\n",
      "Epoch: 0 | Training Batch: 420 | Average loss: 0.2892\n",
      "Epoch: 0 | Training Batch: 430 | Average loss: 0.2741\n",
      "Epoch: 0 | Training Batch: 440 | Average loss: 0.2892\n",
      "Average training batch loss at epoch 0: 0.3391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richard/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:75: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation fold accuracy at epoch 0: 0.2896\n",
      "Epoch: 1 | Training Batch: 10 | Average loss: 0.2752\n",
      "Epoch: 1 | Training Batch: 20 | Average loss: 0.2893\n",
      "Epoch: 1 | Training Batch: 30 | Average loss: 0.2895\n",
      "Epoch: 1 | Training Batch: 40 | Average loss: 0.2707\n",
      "Epoch: 1 | Training Batch: 50 | Average loss: 0.2669\n",
      "Epoch: 1 | Training Batch: 60 | Average loss: 0.2896\n",
      "Epoch: 1 | Training Batch: 70 | Average loss: 0.2617\n",
      "Epoch: 1 | Training Batch: 80 | Average loss: 0.2746\n",
      "Epoch: 1 | Training Batch: 90 | Average loss: 0.2839\n",
      "Epoch: 1 | Training Batch: 100 | Average loss: 0.2705\n",
      "Epoch: 1 | Training Batch: 110 | Average loss: 0.2960\n",
      "Epoch: 1 | Training Batch: 120 | Average loss: 0.2671\n",
      "Epoch: 1 | Training Batch: 130 | Average loss: 0.2860\n",
      "Epoch: 1 | Training Batch: 140 | Average loss: 0.2862\n",
      "Epoch: 1 | Training Batch: 150 | Average loss: 0.2780\n",
      "Epoch: 1 | Training Batch: 160 | Average loss: 0.2654\n",
      "Epoch: 1 | Training Batch: 170 | Average loss: 0.2812\n",
      "Epoch: 1 | Training Batch: 180 | Average loss: 0.2730\n",
      "Epoch: 1 | Training Batch: 190 | Average loss: 0.2790\n",
      "Epoch: 1 | Training Batch: 200 | Average loss: 0.2892\n",
      "Epoch: 1 | Training Batch: 210 | Average loss: 0.2775\n",
      "Epoch: 1 | Training Batch: 220 | Average loss: 0.2783\n",
      "Epoch: 1 | Training Batch: 230 | Average loss: 0.2809\n",
      "Epoch: 1 | Training Batch: 240 | Average loss: 0.2836\n",
      "Epoch: 1 | Training Batch: 250 | Average loss: 0.2824\n",
      "Epoch: 1 | Training Batch: 260 | Average loss: 0.2714\n",
      "Epoch: 1 | Training Batch: 270 | Average loss: 0.2910\n",
      "Epoch: 1 | Training Batch: 280 | Average loss: 0.2789\n",
      "Epoch: 1 | Training Batch: 290 | Average loss: 0.2726\n",
      "Epoch: 1 | Training Batch: 300 | Average loss: 0.2898\n",
      "Epoch: 1 | Training Batch: 310 | Average loss: 0.2779\n",
      "Epoch: 1 | Training Batch: 320 | Average loss: 0.2736\n",
      "Epoch: 1 | Training Batch: 330 | Average loss: 0.2704\n",
      "Epoch: 1 | Training Batch: 340 | Average loss: 0.2668\n",
      "Epoch: 1 | Training Batch: 350 | Average loss: 0.2804\n",
      "Epoch: 1 | Training Batch: 360 | Average loss: 0.2667\n",
      "Epoch: 1 | Training Batch: 370 | Average loss: 0.2766\n",
      "Epoch: 1 | Training Batch: 380 | Average loss: 0.2770\n",
      "Epoch: 1 | Training Batch: 390 | Average loss: 0.2647\n",
      "Epoch: 1 | Training Batch: 400 | Average loss: 0.2611\n",
      "Epoch: 1 | Training Batch: 410 | Average loss: 0.2683\n",
      "Epoch: 1 | Training Batch: 420 | Average loss: 0.2700\n",
      "Epoch: 1 | Training Batch: 430 | Average loss: 0.2764\n",
      "Epoch: 1 | Training Batch: 440 | Average loss: 0.2734\n",
      "Average training batch loss at epoch 1: 0.3081\n",
      "Average validation fold accuracy at epoch 1: 0.2834\n",
      "Epoch: 2 | Training Batch: 10 | Average loss: 0.2686\n",
      "Epoch: 2 | Training Batch: 20 | Average loss: 0.2643\n",
      "Epoch: 2 | Training Batch: 30 | Average loss: 0.2643\n",
      "Epoch: 2 | Training Batch: 40 | Average loss: 0.2648\n",
      "Epoch: 2 | Training Batch: 50 | Average loss: 0.2831\n",
      "Epoch: 2 | Training Batch: 60 | Average loss: 0.2666\n",
      "Epoch: 2 | Training Batch: 70 | Average loss: 0.2847\n",
      "Epoch: 2 | Training Batch: 80 | Average loss: 0.2647\n",
      "Epoch: 2 | Training Batch: 90 | Average loss: 0.2736\n",
      "Epoch: 2 | Training Batch: 100 | Average loss: 0.2712\n",
      "Epoch: 2 | Training Batch: 110 | Average loss: 0.2588\n",
      "Epoch: 2 | Training Batch: 120 | Average loss: 0.2730\n",
      "Epoch: 2 | Training Batch: 130 | Average loss: 0.2610\n",
      "Epoch: 2 | Training Batch: 140 | Average loss: 0.2709\n",
      "Epoch: 2 | Training Batch: 150 | Average loss: 0.2557\n",
      "Epoch: 2 | Training Batch: 160 | Average loss: 0.2591\n",
      "Epoch: 2 | Training Batch: 170 | Average loss: 0.2701\n",
      "Epoch: 2 | Training Batch: 180 | Average loss: 0.2690\n",
      "Epoch: 2 | Training Batch: 190 | Average loss: 0.2773\n",
      "Epoch: 2 | Training Batch: 200 | Average loss: 0.2583\n",
      "Epoch: 2 | Training Batch: 210 | Average loss: 0.2783\n",
      "Epoch: 2 | Training Batch: 220 | Average loss: 0.2644\n",
      "Epoch: 2 | Training Batch: 230 | Average loss: 0.2705\n",
      "Epoch: 2 | Training Batch: 240 | Average loss: 0.2603\n",
      "Epoch: 2 | Training Batch: 250 | Average loss: 0.2673\n",
      "Epoch: 2 | Training Batch: 260 | Average loss: 0.2683\n",
      "Epoch: 2 | Training Batch: 270 | Average loss: 0.2755\n",
      "Epoch: 2 | Training Batch: 280 | Average loss: 0.2685\n",
      "Epoch: 2 | Training Batch: 290 | Average loss: 0.2650\n",
      "Epoch: 2 | Training Batch: 300 | Average loss: 0.2678\n",
      "Epoch: 2 | Training Batch: 310 | Average loss: 0.2537\n",
      "Epoch: 2 | Training Batch: 320 | Average loss: 0.2771\n",
      "Epoch: 2 | Training Batch: 330 | Average loss: 0.2528\n",
      "Epoch: 2 | Training Batch: 340 | Average loss: 0.2569\n",
      "Epoch: 2 | Training Batch: 350 | Average loss: 0.2630\n",
      "Epoch: 2 | Training Batch: 360 | Average loss: 0.2676\n",
      "Epoch: 2 | Training Batch: 370 | Average loss: 0.2602\n",
      "Epoch: 2 | Training Batch: 380 | Average loss: 0.2805\n",
      "Epoch: 2 | Training Batch: 390 | Average loss: 0.2774\n",
      "Epoch: 2 | Training Batch: 400 | Average loss: 0.2567\n",
      "Epoch: 2 | Training Batch: 410 | Average loss: 0.2765\n",
      "Epoch: 2 | Training Batch: 420 | Average loss: 0.2809\n",
      "Epoch: 2 | Training Batch: 430 | Average loss: 0.2720\n",
      "Epoch: 2 | Training Batch: 440 | Average loss: 0.2661\n",
      "Average training batch loss at epoch 2: 0.2947\n",
      "Average validation fold accuracy at epoch 2: 0.2775\n",
      "Epoch: 3 | Training Batch: 10 | Average loss: 0.2602\n",
      "Epoch: 3 | Training Batch: 20 | Average loss: 0.2726\n",
      "Epoch: 3 | Training Batch: 30 | Average loss: 0.2689\n",
      "Epoch: 3 | Training Batch: 40 | Average loss: 0.2643\n",
      "Epoch: 3 | Training Batch: 50 | Average loss: 0.2721\n",
      "Epoch: 3 | Training Batch: 60 | Average loss: 0.2602\n",
      "Epoch: 3 | Training Batch: 70 | Average loss: 0.2839\n",
      "Epoch: 3 | Training Batch: 80 | Average loss: 0.2674\n",
      "Epoch: 3 | Training Batch: 90 | Average loss: 0.2679\n",
      "Epoch: 3 | Training Batch: 100 | Average loss: 0.2718\n",
      "Epoch: 3 | Training Batch: 110 | Average loss: 0.2722\n",
      "Epoch: 3 | Training Batch: 120 | Average loss: 0.2675\n",
      "Epoch: 3 | Training Batch: 130 | Average loss: 0.2708\n",
      "Epoch: 3 | Training Batch: 140 | Average loss: 0.2691\n",
      "Epoch: 3 | Training Batch: 150 | Average loss: 0.2642\n",
      "Epoch: 3 | Training Batch: 160 | Average loss: 0.2609\n",
      "Epoch: 3 | Training Batch: 170 | Average loss: 0.2691\n",
      "Epoch: 3 | Training Batch: 180 | Average loss: 0.2676\n",
      "Epoch: 3 | Training Batch: 190 | Average loss: 0.2801\n",
      "Epoch: 3 | Training Batch: 200 | Average loss: 0.2828\n",
      "Epoch: 3 | Training Batch: 210 | Average loss: 0.2884\n",
      "Epoch: 3 | Training Batch: 220 | Average loss: 0.2794\n",
      "Epoch: 3 | Training Batch: 230 | Average loss: 0.2796\n",
      "Epoch: 3 | Training Batch: 240 | Average loss: 0.2653\n",
      "Epoch: 3 | Training Batch: 250 | Average loss: 0.2794\n",
      "Epoch: 3 | Training Batch: 260 | Average loss: 0.2744\n",
      "Epoch: 3 | Training Batch: 270 | Average loss: 0.2505\n",
      "Epoch: 3 | Training Batch: 280 | Average loss: 0.2582\n",
      "Epoch: 3 | Training Batch: 290 | Average loss: 0.2792\n",
      "Epoch: 3 | Training Batch: 300 | Average loss: 0.2571\n",
      "Epoch: 3 | Training Batch: 310 | Average loss: 0.2730\n",
      "Epoch: 3 | Training Batch: 320 | Average loss: 0.2670\n",
      "Epoch: 3 | Training Batch: 330 | Average loss: 0.2655\n",
      "Epoch: 3 | Training Batch: 340 | Average loss: 0.2666\n",
      "Epoch: 3 | Training Batch: 350 | Average loss: 0.2772\n",
      "Epoch: 3 | Training Batch: 360 | Average loss: 0.2636\n",
      "Epoch: 3 | Training Batch: 370 | Average loss: 0.2699\n",
      "Epoch: 3 | Training Batch: 380 | Average loss: 0.2716\n",
      "Epoch: 3 | Training Batch: 390 | Average loss: 0.2726\n",
      "Epoch: 3 | Training Batch: 400 | Average loss: 0.2600\n",
      "Epoch: 3 | Training Batch: 410 | Average loss: 0.2617\n",
      "Epoch: 3 | Training Batch: 420 | Average loss: 0.2777\n",
      "Epoch: 3 | Training Batch: 430 | Average loss: 0.2732\n",
      "Epoch: 3 | Training Batch: 440 | Average loss: 0.2527\n",
      "Average training batch loss at epoch 3: 0.2885\n",
      "Average validation fold accuracy at epoch 3: 0.2754\n",
      "Epoch: 4 | Training Batch: 10 | Average loss: 0.2723\n",
      "Epoch: 4 | Training Batch: 20 | Average loss: 0.2734\n",
      "Epoch: 4 | Training Batch: 30 | Average loss: 0.2685\n",
      "Epoch: 4 | Training Batch: 40 | Average loss: 0.2730\n",
      "Epoch: 4 | Training Batch: 50 | Average loss: 0.2684\n",
      "Epoch: 4 | Training Batch: 60 | Average loss: 0.2691\n",
      "Epoch: 4 | Training Batch: 70 | Average loss: 0.2626\n",
      "Epoch: 4 | Training Batch: 80 | Average loss: 0.2534\n",
      "Epoch: 4 | Training Batch: 90 | Average loss: 0.2736\n",
      "Epoch: 4 | Training Batch: 100 | Average loss: 0.2568\n",
      "Epoch: 4 | Training Batch: 110 | Average loss: 0.2712\n",
      "Epoch: 4 | Training Batch: 120 | Average loss: 0.2575\n",
      "Epoch: 4 | Training Batch: 130 | Average loss: 0.2666\n",
      "Epoch: 4 | Training Batch: 140 | Average loss: 0.2426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Training Batch: 150 | Average loss: 0.2725\n",
      "Epoch: 4 | Training Batch: 160 | Average loss: 0.2718\n",
      "Epoch: 4 | Training Batch: 170 | Average loss: 0.2792\n",
      "Epoch: 4 | Training Batch: 180 | Average loss: 0.2759\n",
      "Epoch: 4 | Training Batch: 190 | Average loss: 0.2586\n",
      "Epoch: 4 | Training Batch: 200 | Average loss: 0.2647\n",
      "Epoch: 4 | Training Batch: 210 | Average loss: 0.2771\n",
      "Epoch: 4 | Training Batch: 220 | Average loss: 0.2632\n",
      "Epoch: 4 | Training Batch: 230 | Average loss: 0.2757\n",
      "Epoch: 4 | Training Batch: 240 | Average loss: 0.2756\n",
      "Epoch: 4 | Training Batch: 250 | Average loss: 0.2742\n",
      "Epoch: 4 | Training Batch: 260 | Average loss: 0.2742\n",
      "Epoch: 4 | Training Batch: 270 | Average loss: 0.2692\n",
      "Epoch: 4 | Training Batch: 280 | Average loss: 0.2798\n",
      "Epoch: 4 | Training Batch: 290 | Average loss: 0.2735\n",
      "Epoch: 4 | Training Batch: 300 | Average loss: 0.2641\n",
      "Epoch: 4 | Training Batch: 310 | Average loss: 0.2673\n",
      "Epoch: 4 | Training Batch: 320 | Average loss: 0.2670\n",
      "Epoch: 4 | Training Batch: 330 | Average loss: 0.2638\n",
      "Epoch: 4 | Training Batch: 340 | Average loss: 0.2607\n",
      "Epoch: 4 | Training Batch: 350 | Average loss: 0.2575\n",
      "Epoch: 4 | Training Batch: 360 | Average loss: 0.2613\n",
      "Epoch: 4 | Training Batch: 370 | Average loss: 0.2630\n",
      "Epoch: 4 | Training Batch: 380 | Average loss: 0.2729\n",
      "Epoch: 4 | Training Batch: 390 | Average loss: 0.2541\n",
      "Epoch: 4 | Training Batch: 400 | Average loss: 0.2705\n",
      "Epoch: 4 | Training Batch: 410 | Average loss: 0.2618\n",
      "Epoch: 4 | Training Batch: 420 | Average loss: 0.2678\n",
      "Epoch: 4 | Training Batch: 430 | Average loss: 0.2591\n",
      "Epoch: 4 | Training Batch: 440 | Average loss: 0.2593\n",
      "Average training batch loss at epoch 4: 0.2842\n",
      "Average validation fold accuracy at epoch 4: 0.2731\n",
      "Epoch: 5 | Training Batch: 10 | Average loss: 0.2688\n",
      "Epoch: 5 | Training Batch: 20 | Average loss: 0.2655\n",
      "Epoch: 5 | Training Batch: 30 | Average loss: 0.2801\n",
      "Epoch: 5 | Training Batch: 40 | Average loss: 0.2563\n",
      "Epoch: 5 | Training Batch: 50 | Average loss: 0.2707\n",
      "Epoch: 5 | Training Batch: 60 | Average loss: 0.2696\n",
      "Epoch: 5 | Training Batch: 70 | Average loss: 0.2601\n",
      "Epoch: 5 | Training Batch: 80 | Average loss: 0.2695\n",
      "Epoch: 5 | Training Batch: 90 | Average loss: 0.2764\n",
      "Epoch: 5 | Training Batch: 100 | Average loss: 0.2628\n",
      "Epoch: 5 | Training Batch: 110 | Average loss: 0.2733\n",
      "Epoch: 5 | Training Batch: 120 | Average loss: 0.2668\n",
      "Epoch: 5 | Training Batch: 130 | Average loss: 0.2575\n",
      "Epoch: 5 | Training Batch: 140 | Average loss: 0.2612\n",
      "Epoch: 5 | Training Batch: 150 | Average loss: 0.2633\n",
      "Epoch: 5 | Training Batch: 160 | Average loss: 0.2614\n",
      "Epoch: 5 | Training Batch: 170 | Average loss: 0.2667\n",
      "Epoch: 5 | Training Batch: 180 | Average loss: 0.2683\n",
      "Epoch: 5 | Training Batch: 190 | Average loss: 0.2669\n",
      "Epoch: 5 | Training Batch: 200 | Average loss: 0.2604\n",
      "Epoch: 5 | Training Batch: 210 | Average loss: 0.2599\n",
      "Epoch: 5 | Training Batch: 220 | Average loss: 0.2590\n",
      "Epoch: 5 | Training Batch: 230 | Average loss: 0.2630\n",
      "Epoch: 5 | Training Batch: 240 | Average loss: 0.2785\n",
      "Epoch: 5 | Training Batch: 250 | Average loss: 0.2613\n",
      "Epoch: 5 | Training Batch: 260 | Average loss: 0.2618\n",
      "Epoch: 5 | Training Batch: 270 | Average loss: 0.2684\n",
      "Epoch: 5 | Training Batch: 280 | Average loss: 0.2673\n",
      "Epoch: 5 | Training Batch: 290 | Average loss: 0.2640\n",
      "Epoch: 5 | Training Batch: 300 | Average loss: 0.2721\n",
      "Epoch: 5 | Training Batch: 310 | Average loss: 0.2553\n",
      "Epoch: 5 | Training Batch: 320 | Average loss: 0.2543\n",
      "Epoch: 5 | Training Batch: 330 | Average loss: 0.2622\n",
      "Epoch: 5 | Training Batch: 340 | Average loss: 0.2656\n",
      "Epoch: 5 | Training Batch: 350 | Average loss: 0.2613\n",
      "Epoch: 5 | Training Batch: 360 | Average loss: 0.2685\n",
      "Epoch: 5 | Training Batch: 370 | Average loss: 0.2577\n",
      "Epoch: 5 | Training Batch: 380 | Average loss: 0.2724\n",
      "Epoch: 5 | Training Batch: 390 | Average loss: 0.2623\n",
      "Epoch: 5 | Training Batch: 400 | Average loss: 0.2757\n",
      "Epoch: 5 | Training Batch: 410 | Average loss: 0.2667\n",
      "Epoch: 5 | Training Batch: 420 | Average loss: 0.2669\n",
      "Epoch: 5 | Training Batch: 430 | Average loss: 0.2780\n",
      "Epoch: 5 | Training Batch: 440 | Average loss: 0.2622\n",
      "Average training batch loss at epoch 5: 0.2811\n",
      "Average validation fold accuracy at epoch 5: 0.2707\n",
      "Epoch: 6 | Training Batch: 10 | Average loss: 0.2653\n",
      "Epoch: 6 | Training Batch: 20 | Average loss: 0.2624\n",
      "Epoch: 6 | Training Batch: 30 | Average loss: 0.2675\n",
      "Epoch: 6 | Training Batch: 40 | Average loss: 0.2597\n",
      "Epoch: 6 | Training Batch: 50 | Average loss: 0.2669\n",
      "Epoch: 6 | Training Batch: 60 | Average loss: 0.2701\n",
      "Epoch: 6 | Training Batch: 70 | Average loss: 0.2656\n",
      "Epoch: 6 | Training Batch: 80 | Average loss: 0.2592\n",
      "Epoch: 6 | Training Batch: 90 | Average loss: 0.2569\n",
      "Epoch: 6 | Training Batch: 100 | Average loss: 0.2675\n",
      "Epoch: 6 | Training Batch: 110 | Average loss: 0.2698\n",
      "Epoch: 6 | Training Batch: 120 | Average loss: 0.2570\n",
      "Epoch: 6 | Training Batch: 130 | Average loss: 0.2688\n",
      "Epoch: 6 | Training Batch: 140 | Average loss: 0.2563\n",
      "Epoch: 6 | Training Batch: 150 | Average loss: 0.2542\n",
      "Epoch: 6 | Training Batch: 160 | Average loss: 0.2600\n",
      "Epoch: 6 | Training Batch: 170 | Average loss: 0.2722\n",
      "Epoch: 6 | Training Batch: 180 | Average loss: 0.2626\n",
      "Epoch: 6 | Training Batch: 190 | Average loss: 0.2589\n",
      "Epoch: 6 | Training Batch: 200 | Average loss: 0.2571\n",
      "Epoch: 6 | Training Batch: 210 | Average loss: 0.2560\n",
      "Epoch: 6 | Training Batch: 220 | Average loss: 0.2745\n",
      "Epoch: 6 | Training Batch: 230 | Average loss: 0.2695\n",
      "Epoch: 6 | Training Batch: 240 | Average loss: 0.2612\n",
      "Epoch: 6 | Training Batch: 250 | Average loss: 0.2575\n",
      "Epoch: 6 | Training Batch: 260 | Average loss: 0.2726\n",
      "Epoch: 6 | Training Batch: 270 | Average loss: 0.2480\n",
      "Epoch: 6 | Training Batch: 280 | Average loss: 0.2725\n",
      "Epoch: 6 | Training Batch: 290 | Average loss: 0.2554\n",
      "Epoch: 6 | Training Batch: 300 | Average loss: 0.2643\n",
      "Epoch: 6 | Training Batch: 310 | Average loss: 0.2584\n",
      "Epoch: 6 | Training Batch: 320 | Average loss: 0.2649\n",
      "Epoch: 6 | Training Batch: 330 | Average loss: 0.2660\n",
      "Epoch: 6 | Training Batch: 340 | Average loss: 0.2610\n",
      "Epoch: 6 | Training Batch: 350 | Average loss: 0.2542\n",
      "Epoch: 6 | Training Batch: 360 | Average loss: 0.2652\n",
      "Epoch: 6 | Training Batch: 370 | Average loss: 0.2535\n",
      "Epoch: 6 | Training Batch: 380 | Average loss: 0.2684\n",
      "Epoch: 6 | Training Batch: 390 | Average loss: 0.2655\n",
      "Epoch: 6 | Training Batch: 400 | Average loss: 0.2573\n",
      "Epoch: 6 | Training Batch: 410 | Average loss: 0.2639\n",
      "Epoch: 6 | Training Batch: 420 | Average loss: 0.2613\n",
      "Epoch: 6 | Training Batch: 430 | Average loss: 0.2594\n",
      "Epoch: 6 | Training Batch: 440 | Average loss: 0.2639\n",
      "Average training batch loss at epoch 6: 0.2784\n",
      "Average validation fold accuracy at epoch 6: 0.2701\n",
      "Epoch: 7 | Training Batch: 10 | Average loss: 0.2649\n",
      "Epoch: 7 | Training Batch: 20 | Average loss: 0.2638\n",
      "Epoch: 7 | Training Batch: 30 | Average loss: 0.2627\n",
      "Epoch: 7 | Training Batch: 40 | Average loss: 0.2638\n",
      "Epoch: 7 | Training Batch: 50 | Average loss: 0.2671\n",
      "Epoch: 7 | Training Batch: 60 | Average loss: 0.2660\n",
      "Epoch: 7 | Training Batch: 70 | Average loss: 0.2670\n",
      "Epoch: 7 | Training Batch: 80 | Average loss: 0.2672\n",
      "Epoch: 7 | Training Batch: 90 | Average loss: 0.2645\n",
      "Epoch: 7 | Training Batch: 100 | Average loss: 0.2607\n",
      "Epoch: 7 | Training Batch: 110 | Average loss: 0.2596\n",
      "Epoch: 7 | Training Batch: 120 | Average loss: 0.2609\n",
      "Epoch: 7 | Training Batch: 130 | Average loss: 0.2582\n",
      "Epoch: 7 | Training Batch: 140 | Average loss: 0.2610\n",
      "Epoch: 7 | Training Batch: 150 | Average loss: 0.2701\n",
      "Epoch: 7 | Training Batch: 160 | Average loss: 0.2587\n",
      "Epoch: 7 | Training Batch: 170 | Average loss: 0.2598\n",
      "Epoch: 7 | Training Batch: 180 | Average loss: 0.2581\n",
      "Epoch: 7 | Training Batch: 190 | Average loss: 0.2568\n",
      "Epoch: 7 | Training Batch: 200 | Average loss: 0.2543\n",
      "Epoch: 7 | Training Batch: 210 | Average loss: 0.2532\n",
      "Epoch: 7 | Training Batch: 220 | Average loss: 0.2629\n",
      "Epoch: 7 | Training Batch: 230 | Average loss: 0.2665\n",
      "Epoch: 7 | Training Batch: 240 | Average loss: 0.2681\n",
      "Epoch: 7 | Training Batch: 250 | Average loss: 0.2623\n",
      "Epoch: 7 | Training Batch: 260 | Average loss: 0.2605\n",
      "Epoch: 7 | Training Batch: 270 | Average loss: 0.2719\n",
      "Epoch: 7 | Training Batch: 280 | Average loss: 0.2680\n",
      "Epoch: 7 | Training Batch: 290 | Average loss: 0.2634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Training Batch: 300 | Average loss: 0.2605\n",
      "Epoch: 7 | Training Batch: 310 | Average loss: 0.2684\n",
      "Epoch: 7 | Training Batch: 320 | Average loss: 0.2532\n",
      "Epoch: 7 | Training Batch: 330 | Average loss: 0.2636\n",
      "Epoch: 7 | Training Batch: 340 | Average loss: 0.2712\n",
      "Epoch: 7 | Training Batch: 350 | Average loss: 0.2720\n",
      "Epoch: 7 | Training Batch: 360 | Average loss: 0.2671\n",
      "Epoch: 7 | Training Batch: 370 | Average loss: 0.2734\n",
      "Epoch: 7 | Training Batch: 380 | Average loss: 0.2503\n",
      "Epoch: 7 | Training Batch: 390 | Average loss: 0.2627\n",
      "Epoch: 7 | Training Batch: 400 | Average loss: 0.2581\n",
      "Epoch: 7 | Training Batch: 410 | Average loss: 0.2668\n",
      "Epoch: 7 | Training Batch: 420 | Average loss: 0.2591\n",
      "Epoch: 7 | Training Batch: 430 | Average loss: 0.2650\n",
      "Epoch: 7 | Training Batch: 440 | Average loss: 0.2642\n",
      "Average training batch loss at epoch 7: 0.2765\n",
      "Average validation fold accuracy at epoch 7: 0.2688\n",
      "Epoch: 8 | Training Batch: 10 | Average loss: 0.2636\n",
      "Epoch: 8 | Training Batch: 20 | Average loss: 0.2692\n",
      "Epoch: 8 | Training Batch: 30 | Average loss: 0.2630\n",
      "Epoch: 8 | Training Batch: 40 | Average loss: 0.2600\n",
      "Epoch: 8 | Training Batch: 50 | Average loss: 0.2737\n",
      "Epoch: 8 | Training Batch: 60 | Average loss: 0.2594\n",
      "Epoch: 8 | Training Batch: 70 | Average loss: 0.2680\n",
      "Epoch: 8 | Training Batch: 80 | Average loss: 0.2686\n",
      "Epoch: 8 | Training Batch: 90 | Average loss: 0.2698\n",
      "Epoch: 8 | Training Batch: 100 | Average loss: 0.2617\n",
      "Epoch: 8 | Training Batch: 110 | Average loss: 0.2550\n",
      "Epoch: 8 | Training Batch: 120 | Average loss: 0.2645\n",
      "Epoch: 8 | Training Batch: 130 | Average loss: 0.2539\n",
      "Epoch: 8 | Training Batch: 140 | Average loss: 0.2607\n",
      "Epoch: 8 | Training Batch: 150 | Average loss: 0.2663\n",
      "Epoch: 8 | Training Batch: 160 | Average loss: 0.2647\n",
      "Epoch: 8 | Training Batch: 170 | Average loss: 0.2714\n",
      "Epoch: 8 | Training Batch: 180 | Average loss: 0.2672\n",
      "Epoch: 8 | Training Batch: 190 | Average loss: 0.2613\n",
      "Epoch: 8 | Training Batch: 200 | Average loss: 0.2667\n",
      "Epoch: 8 | Training Batch: 210 | Average loss: 0.2553\n",
      "Epoch: 8 | Training Batch: 220 | Average loss: 0.2592\n",
      "Epoch: 8 | Training Batch: 230 | Average loss: 0.2647\n",
      "Epoch: 8 | Training Batch: 240 | Average loss: 0.2481\n",
      "Epoch: 8 | Training Batch: 250 | Average loss: 0.2538\n",
      "Epoch: 8 | Training Batch: 260 | Average loss: 0.2651\n",
      "Epoch: 8 | Training Batch: 270 | Average loss: 0.2713\n",
      "Epoch: 8 | Training Batch: 280 | Average loss: 0.2549\n",
      "Epoch: 8 | Training Batch: 290 | Average loss: 0.2629\n",
      "Epoch: 8 | Training Batch: 300 | Average loss: 0.2508\n",
      "Epoch: 8 | Training Batch: 310 | Average loss: 0.2676\n",
      "Epoch: 8 | Training Batch: 320 | Average loss: 0.2523\n",
      "Epoch: 8 | Training Batch: 330 | Average loss: 0.2616\n",
      "Epoch: 8 | Training Batch: 340 | Average loss: 0.2678\n",
      "Epoch: 8 | Training Batch: 350 | Average loss: 0.2597\n",
      "Epoch: 8 | Training Batch: 360 | Average loss: 0.2590\n",
      "Epoch: 8 | Training Batch: 370 | Average loss: 0.2643\n",
      "Epoch: 8 | Training Batch: 380 | Average loss: 0.2634\n",
      "Epoch: 8 | Training Batch: 390 | Average loss: 0.2672\n",
      "Epoch: 8 | Training Batch: 400 | Average loss: 0.2670\n",
      "Epoch: 8 | Training Batch: 410 | Average loss: 0.2540\n",
      "Epoch: 8 | Training Batch: 420 | Average loss: 0.2619\n",
      "Epoch: 8 | Training Batch: 430 | Average loss: 0.2763\n",
      "Epoch: 8 | Training Batch: 440 | Average loss: 0.2587\n",
      "Average training batch loss at epoch 8: 0.2750\n",
      "Average validation fold accuracy at epoch 8: 0.2678\n",
      "Epoch: 9 | Training Batch: 10 | Average loss: 0.2608\n",
      "Epoch: 9 | Training Batch: 20 | Average loss: 0.2589\n",
      "Epoch: 9 | Training Batch: 30 | Average loss: 0.2572\n",
      "Epoch: 9 | Training Batch: 40 | Average loss: 0.2591\n",
      "Epoch: 9 | Training Batch: 50 | Average loss: 0.2638\n",
      "Epoch: 9 | Training Batch: 60 | Average loss: 0.2673\n",
      "Epoch: 9 | Training Batch: 70 | Average loss: 0.2581\n",
      "Epoch: 9 | Training Batch: 80 | Average loss: 0.2603\n",
      "Epoch: 9 | Training Batch: 90 | Average loss: 0.2477\n",
      "Epoch: 9 | Training Batch: 100 | Average loss: 0.2668\n",
      "Epoch: 9 | Training Batch: 110 | Average loss: 0.2674\n",
      "Epoch: 9 | Training Batch: 120 | Average loss: 0.2603\n",
      "Epoch: 9 | Training Batch: 130 | Average loss: 0.2602\n",
      "Epoch: 9 | Training Batch: 140 | Average loss: 0.2574\n",
      "Epoch: 9 | Training Batch: 150 | Average loss: 0.2551\n",
      "Epoch: 9 | Training Batch: 160 | Average loss: 0.2654\n",
      "Epoch: 9 | Training Batch: 170 | Average loss: 0.2578\n",
      "Epoch: 9 | Training Batch: 180 | Average loss: 0.2626\n",
      "Epoch: 9 | Training Batch: 190 | Average loss: 0.2598\n",
      "Epoch: 9 | Training Batch: 200 | Average loss: 0.2604\n",
      "Epoch: 9 | Training Batch: 210 | Average loss: 0.2688\n",
      "Epoch: 9 | Training Batch: 220 | Average loss: 0.2560\n",
      "Epoch: 9 | Training Batch: 230 | Average loss: 0.2651\n",
      "Epoch: 9 | Training Batch: 240 | Average loss: 0.2679\n",
      "Epoch: 9 | Training Batch: 250 | Average loss: 0.2614\n",
      "Epoch: 9 | Training Batch: 260 | Average loss: 0.2677\n",
      "Epoch: 9 | Training Batch: 270 | Average loss: 0.2633\n",
      "Epoch: 9 | Training Batch: 280 | Average loss: 0.2718\n",
      "Epoch: 9 | Training Batch: 290 | Average loss: 0.2592\n",
      "Epoch: 9 | Training Batch: 300 | Average loss: 0.2612\n",
      "Epoch: 9 | Training Batch: 310 | Average loss: 0.2632\n",
      "Epoch: 9 | Training Batch: 320 | Average loss: 0.2548\n",
      "Epoch: 9 | Training Batch: 330 | Average loss: 0.2641\n",
      "Epoch: 9 | Training Batch: 340 | Average loss: 0.2568\n",
      "Epoch: 9 | Training Batch: 350 | Average loss: 0.2677\n",
      "Epoch: 9 | Training Batch: 360 | Average loss: 0.2585\n",
      "Epoch: 9 | Training Batch: 370 | Average loss: 0.2597\n",
      "Epoch: 9 | Training Batch: 380 | Average loss: 0.2578\n",
      "Epoch: 9 | Training Batch: 390 | Average loss: 0.2524\n",
      "Epoch: 9 | Training Batch: 400 | Average loss: 0.2600\n",
      "Epoch: 9 | Training Batch: 410 | Average loss: 0.2673\n",
      "Epoch: 9 | Training Batch: 420 | Average loss: 0.2519\n",
      "Epoch: 9 | Training Batch: 430 | Average loss: 0.2559\n",
      "Epoch: 9 | Training Batch: 440 | Average loss: 0.2579\n",
      "Average training batch loss at epoch 9: 0.2736\n",
      "Average validation fold accuracy at epoch 9: 0.2667\n",
      "Epoch: 10 | Training Batch: 10 | Average loss: 0.2556\n",
      "Epoch: 10 | Training Batch: 20 | Average loss: 0.2641\n",
      "Epoch: 10 | Training Batch: 30 | Average loss: 0.2561\n",
      "Epoch: 10 | Training Batch: 40 | Average loss: 0.2592\n",
      "Epoch: 10 | Training Batch: 50 | Average loss: 0.2516\n",
      "Epoch: 10 | Training Batch: 60 | Average loss: 0.2640\n",
      "Epoch: 10 | Training Batch: 70 | Average loss: 0.2576\n",
      "Epoch: 10 | Training Batch: 80 | Average loss: 0.2538\n",
      "Epoch: 10 | Training Batch: 90 | Average loss: 0.2552\n",
      "Epoch: 10 | Training Batch: 100 | Average loss: 0.2584\n",
      "Epoch: 10 | Training Batch: 110 | Average loss: 0.2622\n",
      "Epoch: 10 | Training Batch: 120 | Average loss: 0.2606\n",
      "Epoch: 10 | Training Batch: 130 | Average loss: 0.2548\n",
      "Epoch: 10 | Training Batch: 140 | Average loss: 0.2594\n",
      "Epoch: 10 | Training Batch: 150 | Average loss: 0.2712\n",
      "Epoch: 10 | Training Batch: 160 | Average loss: 0.2509\n",
      "Epoch: 10 | Training Batch: 170 | Average loss: 0.2613\n",
      "Epoch: 10 | Training Batch: 180 | Average loss: 0.2596\n",
      "Epoch: 10 | Training Batch: 190 | Average loss: 0.2644\n",
      "Epoch: 10 | Training Batch: 200 | Average loss: 0.2679\n",
      "Epoch: 10 | Training Batch: 210 | Average loss: 0.2609\n",
      "Epoch: 10 | Training Batch: 220 | Average loss: 0.2597\n",
      "Epoch: 10 | Training Batch: 230 | Average loss: 0.2629\n",
      "Epoch: 10 | Training Batch: 240 | Average loss: 0.2712\n",
      "Epoch: 10 | Training Batch: 250 | Average loss: 0.2618\n",
      "Epoch: 10 | Training Batch: 260 | Average loss: 0.2686\n",
      "Epoch: 10 | Training Batch: 270 | Average loss: 0.2562\n",
      "Epoch: 10 | Training Batch: 280 | Average loss: 0.2675\n",
      "Epoch: 10 | Training Batch: 290 | Average loss: 0.2621\n",
      "Epoch: 10 | Training Batch: 300 | Average loss: 0.2544\n",
      "Epoch: 10 | Training Batch: 310 | Average loss: 0.2623\n",
      "Epoch: 10 | Training Batch: 320 | Average loss: 0.2606\n",
      "Epoch: 10 | Training Batch: 330 | Average loss: 0.2586\n",
      "Epoch: 10 | Training Batch: 340 | Average loss: 0.2643\n",
      "Epoch: 10 | Training Batch: 350 | Average loss: 0.2609\n",
      "Epoch: 10 | Training Batch: 360 | Average loss: 0.2663\n",
      "Epoch: 10 | Training Batch: 370 | Average loss: 0.2486\n",
      "Epoch: 10 | Training Batch: 380 | Average loss: 0.2724\n",
      "Epoch: 10 | Training Batch: 390 | Average loss: 0.2495\n",
      "Epoch: 10 | Training Batch: 400 | Average loss: 0.2600\n",
      "Epoch: 10 | Training Batch: 410 | Average loss: 0.2519\n",
      "Epoch: 10 | Training Batch: 420 | Average loss: 0.2638\n",
      "Epoch: 10 | Training Batch: 430 | Average loss: 0.2666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Training Batch: 440 | Average loss: 0.2646\n",
      "Average training batch loss at epoch 10: 0.2724\n",
      "Average validation fold accuracy at epoch 10: 0.2661\n",
      "Epoch: 11 | Training Batch: 10 | Average loss: 0.2617\n",
      "Epoch: 11 | Training Batch: 20 | Average loss: 0.2641\n",
      "Epoch: 11 | Training Batch: 30 | Average loss: 0.2598\n",
      "Epoch: 11 | Training Batch: 40 | Average loss: 0.2624\n",
      "Epoch: 11 | Training Batch: 50 | Average loss: 0.2552\n",
      "Epoch: 11 | Training Batch: 60 | Average loss: 0.2455\n",
      "Epoch: 11 | Training Batch: 70 | Average loss: 0.2626\n",
      "Epoch: 11 | Training Batch: 80 | Average loss: 0.2501\n",
      "Epoch: 11 | Training Batch: 90 | Average loss: 0.2604\n",
      "Epoch: 11 | Training Batch: 100 | Average loss: 0.2636\n",
      "Epoch: 11 | Training Batch: 110 | Average loss: 0.2565\n",
      "Epoch: 11 | Training Batch: 120 | Average loss: 0.2559\n",
      "Epoch: 11 | Training Batch: 130 | Average loss: 0.2658\n",
      "Epoch: 11 | Training Batch: 140 | Average loss: 0.2533\n",
      "Epoch: 11 | Training Batch: 150 | Average loss: 0.2557\n",
      "Epoch: 11 | Training Batch: 160 | Average loss: 0.2642\n",
      "Epoch: 11 | Training Batch: 170 | Average loss: 0.2667\n",
      "Epoch: 11 | Training Batch: 180 | Average loss: 0.2610\n",
      "Epoch: 11 | Training Batch: 190 | Average loss: 0.2526\n",
      "Epoch: 11 | Training Batch: 200 | Average loss: 0.2569\n",
      "Epoch: 11 | Training Batch: 210 | Average loss: 0.2662\n",
      "Epoch: 11 | Training Batch: 220 | Average loss: 0.2566\n",
      "Epoch: 11 | Training Batch: 230 | Average loss: 0.2643\n",
      "Epoch: 11 | Training Batch: 240 | Average loss: 0.2569\n",
      "Epoch: 11 | Training Batch: 250 | Average loss: 0.2561\n",
      "Epoch: 11 | Training Batch: 260 | Average loss: 0.2604\n",
      "Epoch: 11 | Training Batch: 270 | Average loss: 0.2489\n",
      "Epoch: 11 | Training Batch: 280 | Average loss: 0.2576\n",
      "Epoch: 11 | Training Batch: 290 | Average loss: 0.2724\n",
      "Epoch: 11 | Training Batch: 300 | Average loss: 0.2706\n",
      "Epoch: 11 | Training Batch: 310 | Average loss: 0.2674\n",
      "Epoch: 11 | Training Batch: 320 | Average loss: 0.2528\n",
      "Epoch: 11 | Training Batch: 330 | Average loss: 0.2546\n",
      "Epoch: 11 | Training Batch: 340 | Average loss: 0.2655\n",
      "Epoch: 11 | Training Batch: 350 | Average loss: 0.2667\n",
      "Epoch: 11 | Training Batch: 360 | Average loss: 0.2646\n",
      "Epoch: 11 | Training Batch: 370 | Average loss: 0.2615\n",
      "Epoch: 11 | Training Batch: 380 | Average loss: 0.2547\n",
      "Epoch: 11 | Training Batch: 390 | Average loss: 0.2531\n",
      "Epoch: 11 | Training Batch: 400 | Average loss: 0.2738\n",
      "Epoch: 11 | Training Batch: 410 | Average loss: 0.2602\n",
      "Epoch: 11 | Training Batch: 420 | Average loss: 0.2604\n",
      "Epoch: 11 | Training Batch: 430 | Average loss: 0.2609\n",
      "Epoch: 11 | Training Batch: 440 | Average loss: 0.2479\n",
      "Average training batch loss at epoch 11: 0.2714\n",
      "Average validation fold accuracy at epoch 11: 0.2656\n",
      "Epoch: 12 | Training Batch: 10 | Average loss: 0.2509\n",
      "Epoch: 12 | Training Batch: 20 | Average loss: 0.2543\n",
      "Epoch: 12 | Training Batch: 30 | Average loss: 0.2623\n",
      "Epoch: 12 | Training Batch: 40 | Average loss: 0.2754\n",
      "Epoch: 12 | Training Batch: 50 | Average loss: 0.2626\n",
      "Epoch: 12 | Training Batch: 60 | Average loss: 0.2565\n",
      "Epoch: 12 | Training Batch: 70 | Average loss: 0.2517\n",
      "Epoch: 12 | Training Batch: 80 | Average loss: 0.2585\n",
      "Epoch: 12 | Training Batch: 90 | Average loss: 0.2594\n",
      "Epoch: 12 | Training Batch: 100 | Average loss: 0.2599\n",
      "Epoch: 12 | Training Batch: 110 | Average loss: 0.2618\n",
      "Epoch: 12 | Training Batch: 120 | Average loss: 0.2579\n",
      "Epoch: 12 | Training Batch: 130 | Average loss: 0.2568\n",
      "Epoch: 12 | Training Batch: 140 | Average loss: 0.2643\n",
      "Epoch: 12 | Training Batch: 150 | Average loss: 0.2659\n",
      "Epoch: 12 | Training Batch: 160 | Average loss: 0.2643\n",
      "Epoch: 12 | Training Batch: 170 | Average loss: 0.2583\n",
      "Epoch: 12 | Training Batch: 180 | Average loss: 0.2578\n",
      "Epoch: 12 | Training Batch: 190 | Average loss: 0.2615\n",
      "Epoch: 12 | Training Batch: 200 | Average loss: 0.2679\n",
      "Epoch: 12 | Training Batch: 210 | Average loss: 0.2688\n",
      "Epoch: 12 | Training Batch: 220 | Average loss: 0.2618\n",
      "Epoch: 12 | Training Batch: 230 | Average loss: 0.2673\n",
      "Epoch: 12 | Training Batch: 240 | Average loss: 0.2640\n",
      "Epoch: 12 | Training Batch: 250 | Average loss: 0.2583\n",
      "Epoch: 12 | Training Batch: 260 | Average loss: 0.2604\n",
      "Epoch: 12 | Training Batch: 270 | Average loss: 0.2621\n",
      "Epoch: 12 | Training Batch: 280 | Average loss: 0.2690\n",
      "Epoch: 12 | Training Batch: 290 | Average loss: 0.2558\n",
      "Epoch: 12 | Training Batch: 300 | Average loss: 0.2603\n",
      "Epoch: 12 | Training Batch: 310 | Average loss: 0.2563\n",
      "Epoch: 12 | Training Batch: 320 | Average loss: 0.2622\n",
      "Epoch: 12 | Training Batch: 330 | Average loss: 0.2680\n",
      "Epoch: 12 | Training Batch: 340 | Average loss: 0.2668\n",
      "Epoch: 12 | Training Batch: 350 | Average loss: 0.2570\n",
      "Epoch: 12 | Training Batch: 360 | Average loss: 0.2620\n",
      "Epoch: 12 | Training Batch: 370 | Average loss: 0.2622\n",
      "Epoch: 12 | Training Batch: 380 | Average loss: 0.2630\n",
      "Epoch: 12 | Training Batch: 390 | Average loss: 0.2529\n",
      "Epoch: 12 | Training Batch: 400 | Average loss: 0.2639\n",
      "Epoch: 12 | Training Batch: 410 | Average loss: 0.2673\n",
      "Epoch: 12 | Training Batch: 420 | Average loss: 0.2547\n",
      "Epoch: 12 | Training Batch: 430 | Average loss: 0.2562\n",
      "Epoch: 12 | Training Batch: 440 | Average loss: 0.2628\n",
      "Average training batch loss at epoch 12: 0.2706\n",
      "Average validation fold accuracy at epoch 12: 0.2650\n",
      "Epoch: 13 | Training Batch: 10 | Average loss: 0.2513\n",
      "Epoch: 13 | Training Batch: 20 | Average loss: 0.2626\n",
      "Epoch: 13 | Training Batch: 30 | Average loss: 0.2586\n",
      "Epoch: 13 | Training Batch: 40 | Average loss: 0.2583\n",
      "Epoch: 13 | Training Batch: 50 | Average loss: 0.2649\n",
      "Epoch: 13 | Training Batch: 60 | Average loss: 0.2624\n",
      "Epoch: 13 | Training Batch: 70 | Average loss: 0.2572\n",
      "Epoch: 13 | Training Batch: 80 | Average loss: 0.2662\n",
      "Epoch: 13 | Training Batch: 90 | Average loss: 0.2616\n",
      "Epoch: 13 | Training Batch: 100 | Average loss: 0.2650\n",
      "Epoch: 13 | Training Batch: 110 | Average loss: 0.2716\n",
      "Epoch: 13 | Training Batch: 120 | Average loss: 0.2562\n",
      "Epoch: 13 | Training Batch: 130 | Average loss: 0.2570\n",
      "Epoch: 13 | Training Batch: 140 | Average loss: 0.2615\n",
      "Epoch: 13 | Training Batch: 150 | Average loss: 0.2596\n",
      "Epoch: 13 | Training Batch: 160 | Average loss: 0.2650\n",
      "Epoch: 13 | Training Batch: 170 | Average loss: 0.2573\n",
      "Epoch: 13 | Training Batch: 180 | Average loss: 0.2648\n",
      "Epoch: 13 | Training Batch: 190 | Average loss: 0.2607\n",
      "Epoch: 13 | Training Batch: 200 | Average loss: 0.2618\n",
      "Epoch: 13 | Training Batch: 210 | Average loss: 0.2540\n",
      "Epoch: 13 | Training Batch: 220 | Average loss: 0.2503\n",
      "Epoch: 13 | Training Batch: 230 | Average loss: 0.2553\n",
      "Epoch: 13 | Training Batch: 240 | Average loss: 0.2557\n",
      "Epoch: 13 | Training Batch: 250 | Average loss: 0.2531\n",
      "Epoch: 13 | Training Batch: 260 | Average loss: 0.2596\n",
      "Epoch: 13 | Training Batch: 270 | Average loss: 0.2606\n",
      "Epoch: 13 | Training Batch: 280 | Average loss: 0.2633\n",
      "Epoch: 13 | Training Batch: 290 | Average loss: 0.2627\n",
      "Epoch: 13 | Training Batch: 300 | Average loss: 0.2664\n",
      "Epoch: 13 | Training Batch: 310 | Average loss: 0.2579\n",
      "Epoch: 13 | Training Batch: 320 | Average loss: 0.2572\n",
      "Epoch: 13 | Training Batch: 330 | Average loss: 0.2548\n",
      "Epoch: 13 | Training Batch: 340 | Average loss: 0.2586\n",
      "Epoch: 13 | Training Batch: 350 | Average loss: 0.2615\n",
      "Epoch: 13 | Training Batch: 360 | Average loss: 0.2604\n",
      "Epoch: 13 | Training Batch: 370 | Average loss: 0.2499\n",
      "Epoch: 13 | Training Batch: 380 | Average loss: 0.2643\n",
      "Epoch: 13 | Training Batch: 390 | Average loss: 0.2621\n",
      "Epoch: 13 | Training Batch: 400 | Average loss: 0.2529\n",
      "Epoch: 13 | Training Batch: 410 | Average loss: 0.2476\n",
      "Epoch: 13 | Training Batch: 420 | Average loss: 0.2598\n",
      "Epoch: 13 | Training Batch: 430 | Average loss: 0.2651\n",
      "Epoch: 13 | Training Batch: 440 | Average loss: 0.2536\n",
      "Average training batch loss at epoch 13: 0.2698\n",
      "Average validation fold accuracy at epoch 13: 0.2646\n",
      "Epoch: 14 | Training Batch: 10 | Average loss: 0.2584\n",
      "Epoch: 14 | Training Batch: 20 | Average loss: 0.2646\n",
      "Epoch: 14 | Training Batch: 30 | Average loss: 0.2550\n",
      "Epoch: 14 | Training Batch: 40 | Average loss: 0.2592\n",
      "Epoch: 14 | Training Batch: 50 | Average loss: 0.2557\n",
      "Epoch: 14 | Training Batch: 60 | Average loss: 0.2616\n",
      "Epoch: 14 | Training Batch: 70 | Average loss: 0.2657\n",
      "Epoch: 14 | Training Batch: 80 | Average loss: 0.2675\n",
      "Epoch: 14 | Training Batch: 90 | Average loss: 0.2583\n",
      "Epoch: 14 | Training Batch: 100 | Average loss: 0.2552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Training Batch: 110 | Average loss: 0.2588\n",
      "Epoch: 14 | Training Batch: 120 | Average loss: 0.2635\n",
      "Epoch: 14 | Training Batch: 130 | Average loss: 0.2549\n",
      "Epoch: 14 | Training Batch: 140 | Average loss: 0.2600\n",
      "Epoch: 14 | Training Batch: 150 | Average loss: 0.2569\n",
      "Epoch: 14 | Training Batch: 160 | Average loss: 0.2552\n",
      "Epoch: 14 | Training Batch: 170 | Average loss: 0.2683\n",
      "Epoch: 14 | Training Batch: 180 | Average loss: 0.2714\n",
      "Epoch: 14 | Training Batch: 190 | Average loss: 0.2538\n",
      "Epoch: 14 | Training Batch: 200 | Average loss: 0.2627\n",
      "Epoch: 14 | Training Batch: 210 | Average loss: 0.2638\n",
      "Epoch: 14 | Training Batch: 220 | Average loss: 0.2559\n",
      "Epoch: 14 | Training Batch: 230 | Average loss: 0.2651\n",
      "Epoch: 14 | Training Batch: 240 | Average loss: 0.2520\n",
      "Epoch: 14 | Training Batch: 250 | Average loss: 0.2561\n",
      "Epoch: 14 | Training Batch: 260 | Average loss: 0.2551\n",
      "Epoch: 14 | Training Batch: 270 | Average loss: 0.2661\n",
      "Epoch: 14 | Training Batch: 280 | Average loss: 0.2635\n",
      "Epoch: 14 | Training Batch: 290 | Average loss: 0.2607\n",
      "Epoch: 14 | Training Batch: 300 | Average loss: 0.2616\n",
      "Epoch: 14 | Training Batch: 310 | Average loss: 0.2604\n",
      "Epoch: 14 | Training Batch: 320 | Average loss: 0.2533\n",
      "Epoch: 14 | Training Batch: 330 | Average loss: 0.2692\n",
      "Epoch: 14 | Training Batch: 340 | Average loss: 0.2596\n",
      "Epoch: 14 | Training Batch: 350 | Average loss: 0.2557\n",
      "Epoch: 14 | Training Batch: 360 | Average loss: 0.2503\n",
      "Epoch: 14 | Training Batch: 370 | Average loss: 0.2550\n",
      "Epoch: 14 | Training Batch: 380 | Average loss: 0.2645\n",
      "Epoch: 14 | Training Batch: 390 | Average loss: 0.2616\n",
      "Epoch: 14 | Training Batch: 400 | Average loss: 0.2586\n",
      "Epoch: 14 | Training Batch: 410 | Average loss: 0.2515\n",
      "Epoch: 14 | Training Batch: 420 | Average loss: 0.2593\n",
      "Epoch: 14 | Training Batch: 430 | Average loss: 0.2539\n",
      "Epoch: 14 | Training Batch: 440 | Average loss: 0.2658\n",
      "Average training batch loss at epoch 14: 0.2691\n",
      "Average validation fold accuracy at epoch 14: 0.2646\n",
      "Epoch: 15 | Training Batch: 10 | Average loss: 0.2584\n",
      "Epoch: 15 | Training Batch: 20 | Average loss: 0.2646\n",
      "Epoch: 15 | Training Batch: 30 | Average loss: 0.2503\n",
      "Epoch: 15 | Training Batch: 40 | Average loss: 0.2722\n",
      "Epoch: 15 | Training Batch: 50 | Average loss: 0.2508\n",
      "Epoch: 15 | Training Batch: 60 | Average loss: 0.2569\n",
      "Epoch: 15 | Training Batch: 70 | Average loss: 0.2540\n",
      "Epoch: 15 | Training Batch: 80 | Average loss: 0.2561\n",
      "Epoch: 15 | Training Batch: 90 | Average loss: 0.2629\n",
      "Epoch: 15 | Training Batch: 100 | Average loss: 0.2557\n",
      "Epoch: 15 | Training Batch: 110 | Average loss: 0.2610\n",
      "Epoch: 15 | Training Batch: 120 | Average loss: 0.2596\n",
      "Epoch: 15 | Training Batch: 130 | Average loss: 0.2600\n",
      "Epoch: 15 | Training Batch: 140 | Average loss: 0.2538\n",
      "Epoch: 15 | Training Batch: 150 | Average loss: 0.2632\n",
      "Epoch: 15 | Training Batch: 160 | Average loss: 0.2565\n",
      "Epoch: 15 | Training Batch: 170 | Average loss: 0.2563\n",
      "Epoch: 15 | Training Batch: 180 | Average loss: 0.2601\n",
      "Epoch: 15 | Training Batch: 190 | Average loss: 0.2615\n",
      "Epoch: 15 | Training Batch: 200 | Average loss: 0.2810\n",
      "Epoch: 15 | Training Batch: 210 | Average loss: 0.2642\n",
      "Epoch: 15 | Training Batch: 220 | Average loss: 0.2659\n",
      "Epoch: 15 | Training Batch: 230 | Average loss: 0.2479\n",
      "Epoch: 15 | Training Batch: 240 | Average loss: 0.2572\n",
      "Epoch: 15 | Training Batch: 250 | Average loss: 0.2625\n",
      "Epoch: 15 | Training Batch: 260 | Average loss: 0.2566\n",
      "Epoch: 15 | Training Batch: 270 | Average loss: 0.2562\n",
      "Epoch: 15 | Training Batch: 280 | Average loss: 0.2643\n",
      "Epoch: 15 | Training Batch: 290 | Average loss: 0.2625\n",
      "Epoch: 15 | Training Batch: 300 | Average loss: 0.2552\n",
      "Epoch: 15 | Training Batch: 310 | Average loss: 0.2527\n",
      "Epoch: 15 | Training Batch: 320 | Average loss: 0.2584\n",
      "Epoch: 15 | Training Batch: 330 | Average loss: 0.2574\n",
      "Epoch: 15 | Training Batch: 340 | Average loss: 0.2489\n",
      "Epoch: 15 | Training Batch: 350 | Average loss: 0.2557\n",
      "Epoch: 15 | Training Batch: 360 | Average loss: 0.2606\n",
      "Epoch: 15 | Training Batch: 370 | Average loss: 0.2532\n",
      "Epoch: 15 | Training Batch: 380 | Average loss: 0.2593\n",
      "Epoch: 15 | Training Batch: 390 | Average loss: 0.2571\n",
      "Epoch: 15 | Training Batch: 400 | Average loss: 0.2587\n",
      "Epoch: 15 | Training Batch: 410 | Average loss: 0.2579\n",
      "Epoch: 15 | Training Batch: 420 | Average loss: 0.2610\n",
      "Epoch: 15 | Training Batch: 430 | Average loss: 0.2598\n",
      "Epoch: 15 | Training Batch: 440 | Average loss: 0.2556\n",
      "Average training batch loss at epoch 15: 0.2684\n",
      "Average validation fold accuracy at epoch 15: 0.2642\n",
      "Epoch: 16 | Training Batch: 10 | Average loss: 0.2593\n",
      "Epoch: 16 | Training Batch: 20 | Average loss: 0.2569\n",
      "Epoch: 16 | Training Batch: 30 | Average loss: 0.2550\n",
      "Epoch: 16 | Training Batch: 40 | Average loss: 0.2588\n",
      "Epoch: 16 | Training Batch: 50 | Average loss: 0.2523\n",
      "Epoch: 16 | Training Batch: 60 | Average loss: 0.2665\n",
      "Epoch: 16 | Training Batch: 70 | Average loss: 0.2499\n",
      "Epoch: 16 | Training Batch: 80 | Average loss: 0.2549\n",
      "Epoch: 16 | Training Batch: 90 | Average loss: 0.2566\n",
      "Epoch: 16 | Training Batch: 100 | Average loss: 0.2520\n",
      "Epoch: 16 | Training Batch: 110 | Average loss: 0.2620\n",
      "Epoch: 16 | Training Batch: 120 | Average loss: 0.2662\n",
      "Epoch: 16 | Training Batch: 130 | Average loss: 0.2637\n",
      "Epoch: 16 | Training Batch: 140 | Average loss: 0.2604\n",
      "Epoch: 16 | Training Batch: 150 | Average loss: 0.2638\n",
      "Epoch: 16 | Training Batch: 160 | Average loss: 0.2659\n",
      "Epoch: 16 | Training Batch: 170 | Average loss: 0.2603\n",
      "Epoch: 16 | Training Batch: 180 | Average loss: 0.2557\n",
      "Epoch: 16 | Training Batch: 190 | Average loss: 0.2596\n",
      "Epoch: 16 | Training Batch: 200 | Average loss: 0.2533\n",
      "Epoch: 16 | Training Batch: 210 | Average loss: 0.2600\n",
      "Epoch: 16 | Training Batch: 220 | Average loss: 0.2538\n",
      "Epoch: 16 | Training Batch: 230 | Average loss: 0.2614\n",
      "Epoch: 16 | Training Batch: 240 | Average loss: 0.2657\n",
      "Epoch: 16 | Training Batch: 250 | Average loss: 0.2551\n",
      "Epoch: 16 | Training Batch: 260 | Average loss: 0.2627\n",
      "Epoch: 16 | Training Batch: 270 | Average loss: 0.2599\n",
      "Epoch: 16 | Training Batch: 280 | Average loss: 0.2587\n",
      "Epoch: 16 | Training Batch: 290 | Average loss: 0.2570\n",
      "Epoch: 16 | Training Batch: 300 | Average loss: 0.2592\n",
      "Epoch: 16 | Training Batch: 310 | Average loss: 0.2589\n",
      "Epoch: 16 | Training Batch: 320 | Average loss: 0.2584\n",
      "Epoch: 16 | Training Batch: 330 | Average loss: 0.2599\n",
      "Epoch: 16 | Training Batch: 340 | Average loss: 0.2550\n",
      "Epoch: 16 | Training Batch: 350 | Average loss: 0.2520\n",
      "Epoch: 16 | Training Batch: 360 | Average loss: 0.2573\n",
      "Epoch: 16 | Training Batch: 370 | Average loss: 0.2633\n",
      "Epoch: 16 | Training Batch: 380 | Average loss: 0.2548\n",
      "Epoch: 16 | Training Batch: 390 | Average loss: 0.2519\n",
      "Epoch: 16 | Training Batch: 400 | Average loss: 0.2638\n",
      "Epoch: 16 | Training Batch: 410 | Average loss: 0.2549\n",
      "Epoch: 16 | Training Batch: 420 | Average loss: 0.2569\n",
      "Epoch: 16 | Training Batch: 430 | Average loss: 0.2585\n",
      "Epoch: 16 | Training Batch: 440 | Average loss: 0.2649\n",
      "Average training batch loss at epoch 16: 0.2679\n",
      "Average validation fold accuracy at epoch 16: 0.2639\n",
      "Epoch: 17 | Training Batch: 10 | Average loss: 0.2601\n",
      "Epoch: 17 | Training Batch: 20 | Average loss: 0.2660\n",
      "Epoch: 17 | Training Batch: 30 | Average loss: 0.2584\n",
      "Epoch: 17 | Training Batch: 40 | Average loss: 0.2606\n",
      "Epoch: 17 | Training Batch: 50 | Average loss: 0.2524\n",
      "Epoch: 17 | Training Batch: 60 | Average loss: 0.2585\n",
      "Epoch: 17 | Training Batch: 70 | Average loss: 0.2597\n",
      "Epoch: 17 | Training Batch: 80 | Average loss: 0.2485\n",
      "Epoch: 17 | Training Batch: 90 | Average loss: 0.2631\n",
      "Epoch: 17 | Training Batch: 100 | Average loss: 0.2662\n",
      "Epoch: 17 | Training Batch: 110 | Average loss: 0.2659\n",
      "Epoch: 17 | Training Batch: 120 | Average loss: 0.2567\n",
      "Epoch: 17 | Training Batch: 130 | Average loss: 0.2681\n",
      "Epoch: 17 | Training Batch: 140 | Average loss: 0.2561\n",
      "Epoch: 17 | Training Batch: 150 | Average loss: 0.2581\n",
      "Epoch: 17 | Training Batch: 160 | Average loss: 0.2498\n",
      "Epoch: 17 | Training Batch: 170 | Average loss: 0.2550\n",
      "Epoch: 17 | Training Batch: 180 | Average loss: 0.2513\n",
      "Epoch: 17 | Training Batch: 190 | Average loss: 0.2650\n",
      "Epoch: 17 | Training Batch: 200 | Average loss: 0.2598\n",
      "Epoch: 17 | Training Batch: 210 | Average loss: 0.2569\n",
      "Epoch: 17 | Training Batch: 220 | Average loss: 0.2631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Training Batch: 230 | Average loss: 0.2533\n",
      "Epoch: 17 | Training Batch: 240 | Average loss: 0.2625\n",
      "Epoch: 17 | Training Batch: 250 | Average loss: 0.2554\n",
      "Epoch: 17 | Training Batch: 260 | Average loss: 0.2665\n",
      "Epoch: 17 | Training Batch: 270 | Average loss: 0.2535\n",
      "Epoch: 17 | Training Batch: 280 | Average loss: 0.2581\n",
      "Epoch: 17 | Training Batch: 290 | Average loss: 0.2694\n",
      "Epoch: 17 | Training Batch: 300 | Average loss: 0.2467\n",
      "Epoch: 17 | Training Batch: 310 | Average loss: 0.2537\n",
      "Epoch: 17 | Training Batch: 320 | Average loss: 0.2448\n",
      "Epoch: 17 | Training Batch: 330 | Average loss: 0.2540\n",
      "Epoch: 17 | Training Batch: 340 | Average loss: 0.2549\n",
      "Epoch: 17 | Training Batch: 350 | Average loss: 0.2520\n",
      "Epoch: 17 | Training Batch: 360 | Average loss: 0.2639\n",
      "Epoch: 17 | Training Batch: 370 | Average loss: 0.2630\n",
      "Epoch: 17 | Training Batch: 380 | Average loss: 0.2622\n",
      "Epoch: 17 | Training Batch: 390 | Average loss: 0.2630\n",
      "Epoch: 17 | Training Batch: 400 | Average loss: 0.2638\n",
      "Epoch: 17 | Training Batch: 410 | Average loss: 0.2519\n",
      "Epoch: 17 | Training Batch: 420 | Average loss: 0.2506\n",
      "Epoch: 17 | Training Batch: 430 | Average loss: 0.2541\n",
      "Epoch: 17 | Training Batch: 440 | Average loss: 0.2568\n",
      "Average training batch loss at epoch 17: 0.2673\n",
      "Average validation fold accuracy at epoch 17: 0.2636\n",
      "Epoch: 18 | Training Batch: 10 | Average loss: 0.2580\n",
      "Epoch: 18 | Training Batch: 20 | Average loss: 0.2585\n",
      "Epoch: 18 | Training Batch: 30 | Average loss: 0.2567\n",
      "Epoch: 18 | Training Batch: 40 | Average loss: 0.2623\n",
      "Epoch: 18 | Training Batch: 50 | Average loss: 0.2511\n",
      "Epoch: 18 | Training Batch: 60 | Average loss: 0.2556\n",
      "Epoch: 18 | Training Batch: 70 | Average loss: 0.2561\n",
      "Epoch: 18 | Training Batch: 80 | Average loss: 0.2573\n",
      "Epoch: 18 | Training Batch: 90 | Average loss: 0.2574\n",
      "Epoch: 18 | Training Batch: 100 | Average loss: 0.2498\n",
      "Epoch: 18 | Training Batch: 110 | Average loss: 0.2487\n",
      "Epoch: 18 | Training Batch: 120 | Average loss: 0.2628\n",
      "Epoch: 18 | Training Batch: 130 | Average loss: 0.2576\n",
      "Epoch: 18 | Training Batch: 140 | Average loss: 0.2556\n",
      "Epoch: 18 | Training Batch: 150 | Average loss: 0.2487\n",
      "Epoch: 18 | Training Batch: 160 | Average loss: 0.2580\n",
      "Epoch: 18 | Training Batch: 170 | Average loss: 0.2604\n",
      "Epoch: 18 | Training Batch: 180 | Average loss: 0.2498\n",
      "Epoch: 18 | Training Batch: 190 | Average loss: 0.2508\n",
      "Epoch: 18 | Training Batch: 200 | Average loss: 0.2605\n",
      "Epoch: 18 | Training Batch: 210 | Average loss: 0.2607\n",
      "Epoch: 18 | Training Batch: 220 | Average loss: 0.2541\n",
      "Epoch: 18 | Training Batch: 230 | Average loss: 0.2603\n",
      "Epoch: 18 | Training Batch: 240 | Average loss: 0.2645\n",
      "Epoch: 18 | Training Batch: 250 | Average loss: 0.2592\n",
      "Epoch: 18 | Training Batch: 260 | Average loss: 0.2521\n",
      "Epoch: 18 | Training Batch: 270 | Average loss: 0.2524\n",
      "Epoch: 18 | Training Batch: 280 | Average loss: 0.2550\n",
      "Epoch: 18 | Training Batch: 290 | Average loss: 0.2586\n",
      "Epoch: 18 | Training Batch: 300 | Average loss: 0.2599\n",
      "Epoch: 18 | Training Batch: 310 | Average loss: 0.2602\n",
      "Epoch: 18 | Training Batch: 320 | Average loss: 0.2521\n",
      "Epoch: 18 | Training Batch: 330 | Average loss: 0.2567\n",
      "Epoch: 18 | Training Batch: 340 | Average loss: 0.2564\n",
      "Epoch: 18 | Training Batch: 350 | Average loss: 0.2547\n",
      "Epoch: 18 | Training Batch: 360 | Average loss: 0.2575\n",
      "Epoch: 18 | Training Batch: 370 | Average loss: 0.2528\n",
      "Epoch: 18 | Training Batch: 380 | Average loss: 0.2556\n",
      "Epoch: 18 | Training Batch: 390 | Average loss: 0.2549\n",
      "Epoch: 18 | Training Batch: 400 | Average loss: 0.2629\n",
      "Epoch: 18 | Training Batch: 410 | Average loss: 0.2691\n",
      "Epoch: 18 | Training Batch: 420 | Average loss: 0.2619\n",
      "Epoch: 18 | Training Batch: 430 | Average loss: 0.2617\n",
      "Epoch: 18 | Training Batch: 440 | Average loss: 0.2548\n",
      "Average training batch loss at epoch 18: 0.2668\n",
      "Average validation fold accuracy at epoch 18: 0.2633\n",
      "Epoch: 19 | Training Batch: 10 | Average loss: 0.2577\n",
      "Epoch: 19 | Training Batch: 20 | Average loss: 0.2518\n",
      "Epoch: 19 | Training Batch: 30 | Average loss: 0.2597\n",
      "Epoch: 19 | Training Batch: 40 | Average loss: 0.2544\n",
      "Epoch: 19 | Training Batch: 50 | Average loss: 0.2517\n",
      "Epoch: 19 | Training Batch: 60 | Average loss: 0.2615\n",
      "Epoch: 19 | Training Batch: 70 | Average loss: 0.2611\n",
      "Epoch: 19 | Training Batch: 80 | Average loss: 0.2582\n",
      "Epoch: 19 | Training Batch: 90 | Average loss: 0.2584\n",
      "Epoch: 19 | Training Batch: 100 | Average loss: 0.2584\n",
      "Epoch: 19 | Training Batch: 110 | Average loss: 0.2600\n",
      "Epoch: 19 | Training Batch: 120 | Average loss: 0.2587\n",
      "Epoch: 19 | Training Batch: 130 | Average loss: 0.2531\n",
      "Epoch: 19 | Training Batch: 140 | Average loss: 0.2569\n",
      "Epoch: 19 | Training Batch: 150 | Average loss: 0.2500\n",
      "Epoch: 19 | Training Batch: 160 | Average loss: 0.2520\n",
      "Epoch: 19 | Training Batch: 170 | Average loss: 0.2498\n",
      "Epoch: 19 | Training Batch: 180 | Average loss: 0.2568\n",
      "Epoch: 19 | Training Batch: 190 | Average loss: 0.2543\n",
      "Epoch: 19 | Training Batch: 200 | Average loss: 0.2659\n",
      "Epoch: 19 | Training Batch: 210 | Average loss: 0.2596\n",
      "Epoch: 19 | Training Batch: 220 | Average loss: 0.2637\n",
      "Epoch: 19 | Training Batch: 230 | Average loss: 0.2644\n",
      "Epoch: 19 | Training Batch: 240 | Average loss: 0.2594\n",
      "Epoch: 19 | Training Batch: 250 | Average loss: 0.2584\n",
      "Epoch: 19 | Training Batch: 260 | Average loss: 0.2510\n",
      "Epoch: 19 | Training Batch: 270 | Average loss: 0.2591\n",
      "Epoch: 19 | Training Batch: 280 | Average loss: 0.2587\n",
      "Epoch: 19 | Training Batch: 290 | Average loss: 0.2550\n",
      "Epoch: 19 | Training Batch: 300 | Average loss: 0.2505\n",
      "Epoch: 19 | Training Batch: 310 | Average loss: 0.2620\n",
      "Epoch: 19 | Training Batch: 320 | Average loss: 0.2561\n",
      "Epoch: 19 | Training Batch: 330 | Average loss: 0.2628\n",
      "Epoch: 19 | Training Batch: 340 | Average loss: 0.2535\n",
      "Epoch: 19 | Training Batch: 350 | Average loss: 0.2580\n",
      "Epoch: 19 | Training Batch: 360 | Average loss: 0.2607\n",
      "Epoch: 19 | Training Batch: 370 | Average loss: 0.2607\n",
      "Epoch: 19 | Training Batch: 380 | Average loss: 0.2541\n",
      "Epoch: 19 | Training Batch: 390 | Average loss: 0.2545\n",
      "Epoch: 19 | Training Batch: 400 | Average loss: 0.2600\n",
      "Epoch: 19 | Training Batch: 410 | Average loss: 0.2617\n",
      "Epoch: 19 | Training Batch: 420 | Average loss: 0.2621\n",
      "Epoch: 19 | Training Batch: 430 | Average loss: 0.2575\n",
      "Epoch: 19 | Training Batch: 440 | Average loss: 0.2529\n",
      "Average training batch loss at epoch 19: 0.2663\n",
      "Average validation fold accuracy at epoch 19: 0.2628\n",
      "Epoch: 20 | Training Batch: 10 | Average loss: 0.2605\n",
      "Epoch: 20 | Training Batch: 20 | Average loss: 0.2578\n",
      "Epoch: 20 | Training Batch: 30 | Average loss: 0.2496\n",
      "Epoch: 20 | Training Batch: 40 | Average loss: 0.2551\n",
      "Epoch: 20 | Training Batch: 50 | Average loss: 0.2560\n",
      "Epoch: 20 | Training Batch: 60 | Average loss: 0.2516\n",
      "Epoch: 20 | Training Batch: 70 | Average loss: 0.2614\n",
      "Epoch: 20 | Training Batch: 80 | Average loss: 0.2561\n",
      "Epoch: 20 | Training Batch: 90 | Average loss: 0.2552\n",
      "Epoch: 20 | Training Batch: 100 | Average loss: 0.2505\n",
      "Epoch: 20 | Training Batch: 110 | Average loss: 0.2477\n",
      "Epoch: 20 | Training Batch: 120 | Average loss: 0.2604\n",
      "Epoch: 20 | Training Batch: 130 | Average loss: 0.2627\n",
      "Epoch: 20 | Training Batch: 140 | Average loss: 0.2572\n",
      "Epoch: 20 | Training Batch: 150 | Average loss: 0.2568\n",
      "Epoch: 20 | Training Batch: 160 | Average loss: 0.2477\n",
      "Epoch: 20 | Training Batch: 170 | Average loss: 0.2627\n",
      "Epoch: 20 | Training Batch: 180 | Average loss: 0.2548\n",
      "Epoch: 20 | Training Batch: 190 | Average loss: 0.2576\n",
      "Epoch: 20 | Training Batch: 200 | Average loss: 0.2674\n",
      "Epoch: 20 | Training Batch: 210 | Average loss: 0.2666\n",
      "Epoch: 20 | Training Batch: 220 | Average loss: 0.2558\n",
      "Epoch: 20 | Training Batch: 230 | Average loss: 0.2530\n",
      "Epoch: 20 | Training Batch: 240 | Average loss: 0.2576\n",
      "Epoch: 20 | Training Batch: 250 | Average loss: 0.2523\n",
      "Epoch: 20 | Training Batch: 260 | Average loss: 0.2530\n",
      "Epoch: 20 | Training Batch: 270 | Average loss: 0.2512\n",
      "Epoch: 20 | Training Batch: 280 | Average loss: 0.2612\n",
      "Epoch: 20 | Training Batch: 290 | Average loss: 0.2528\n",
      "Epoch: 20 | Training Batch: 300 | Average loss: 0.2573\n",
      "Epoch: 20 | Training Batch: 310 | Average loss: 0.2503\n",
      "Epoch: 20 | Training Batch: 320 | Average loss: 0.2552\n",
      "Epoch: 20 | Training Batch: 330 | Average loss: 0.2521\n",
      "Epoch: 20 | Training Batch: 340 | Average loss: 0.2592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Training Batch: 350 | Average loss: 0.2582\n",
      "Epoch: 20 | Training Batch: 360 | Average loss: 0.2591\n",
      "Epoch: 20 | Training Batch: 370 | Average loss: 0.2535\n",
      "Epoch: 20 | Training Batch: 380 | Average loss: 0.2602\n",
      "Epoch: 20 | Training Batch: 390 | Average loss: 0.2571\n",
      "Epoch: 20 | Training Batch: 400 | Average loss: 0.2621\n",
      "Epoch: 20 | Training Batch: 410 | Average loss: 0.2586\n",
      "Epoch: 20 | Training Batch: 420 | Average loss: 0.2503\n",
      "Epoch: 20 | Training Batch: 430 | Average loss: 0.2584\n",
      "Epoch: 20 | Training Batch: 440 | Average loss: 0.2591\n",
      "Average training batch loss at epoch 20: 0.2658\n",
      "Average validation fold accuracy at epoch 20: 0.2626\n",
      "Epoch: 21 | Training Batch: 10 | Average loss: 0.2559\n",
      "Epoch: 21 | Training Batch: 20 | Average loss: 0.2630\n",
      "Epoch: 21 | Training Batch: 30 | Average loss: 0.2511\n",
      "Epoch: 21 | Training Batch: 40 | Average loss: 0.2510\n",
      "Epoch: 21 | Training Batch: 50 | Average loss: 0.2514\n",
      "Epoch: 21 | Training Batch: 60 | Average loss: 0.2611\n",
      "Epoch: 21 | Training Batch: 70 | Average loss: 0.2603\n",
      "Epoch: 21 | Training Batch: 80 | Average loss: 0.2621\n",
      "Epoch: 21 | Training Batch: 90 | Average loss: 0.2584\n",
      "Epoch: 21 | Training Batch: 100 | Average loss: 0.2557\n",
      "Epoch: 21 | Training Batch: 110 | Average loss: 0.2585\n",
      "Epoch: 21 | Training Batch: 120 | Average loss: 0.2540\n",
      "Epoch: 21 | Training Batch: 130 | Average loss: 0.2552\n",
      "Epoch: 21 | Training Batch: 140 | Average loss: 0.2590\n",
      "Epoch: 21 | Training Batch: 150 | Average loss: 0.2612\n",
      "Epoch: 21 | Training Batch: 160 | Average loss: 0.2624\n",
      "Epoch: 21 | Training Batch: 170 | Average loss: 0.2584\n",
      "Epoch: 21 | Training Batch: 180 | Average loss: 0.2582\n",
      "Epoch: 21 | Training Batch: 190 | Average loss: 0.2560\n",
      "Epoch: 21 | Training Batch: 200 | Average loss: 0.2495\n",
      "Epoch: 21 | Training Batch: 210 | Average loss: 0.2601\n",
      "Epoch: 21 | Training Batch: 220 | Average loss: 0.2505\n",
      "Epoch: 21 | Training Batch: 230 | Average loss: 0.2681\n",
      "Epoch: 21 | Training Batch: 240 | Average loss: 0.2458\n",
      "Epoch: 21 | Training Batch: 250 | Average loss: 0.2598\n",
      "Epoch: 21 | Training Batch: 260 | Average loss: 0.2597\n",
      "Epoch: 21 | Training Batch: 270 | Average loss: 0.2608\n",
      "Epoch: 21 | Training Batch: 280 | Average loss: 0.2530\n",
      "Epoch: 21 | Training Batch: 290 | Average loss: 0.2528\n",
      "Epoch: 21 | Training Batch: 300 | Average loss: 0.2557\n",
      "Epoch: 21 | Training Batch: 310 | Average loss: 0.2596\n",
      "Epoch: 21 | Training Batch: 320 | Average loss: 0.2563\n",
      "Epoch: 21 | Training Batch: 330 | Average loss: 0.2656\n",
      "Epoch: 21 | Training Batch: 340 | Average loss: 0.2405\n",
      "Epoch: 21 | Training Batch: 350 | Average loss: 0.2592\n",
      "Epoch: 21 | Training Batch: 360 | Average loss: 0.2608\n",
      "Epoch: 21 | Training Batch: 370 | Average loss: 0.2553\n",
      "Epoch: 21 | Training Batch: 380 | Average loss: 0.2568\n",
      "Epoch: 21 | Training Batch: 390 | Average loss: 0.2512\n",
      "Epoch: 21 | Training Batch: 400 | Average loss: 0.2558\n",
      "Epoch: 21 | Training Batch: 410 | Average loss: 0.2574\n",
      "Epoch: 21 | Training Batch: 420 | Average loss: 0.2619\n",
      "Epoch: 21 | Training Batch: 430 | Average loss: 0.2441\n",
      "Epoch: 21 | Training Batch: 440 | Average loss: 0.2569\n",
      "Average training batch loss at epoch 21: 0.2654\n",
      "Average validation fold accuracy at epoch 21: 0.2622\n",
      "Epoch: 22 | Training Batch: 10 | Average loss: 0.2518\n",
      "Epoch: 22 | Training Batch: 20 | Average loss: 0.2502\n",
      "Epoch: 22 | Training Batch: 30 | Average loss: 0.2592\n",
      "Epoch: 22 | Training Batch: 40 | Average loss: 0.2597\n",
      "Epoch: 22 | Training Batch: 50 | Average loss: 0.2570\n",
      "Epoch: 22 | Training Batch: 60 | Average loss: 0.2463\n",
      "Epoch: 22 | Training Batch: 70 | Average loss: 0.2584\n",
      "Epoch: 22 | Training Batch: 80 | Average loss: 0.2550\n",
      "Epoch: 22 | Training Batch: 90 | Average loss: 0.2535\n",
      "Epoch: 22 | Training Batch: 100 | Average loss: 0.2551\n",
      "Epoch: 22 | Training Batch: 110 | Average loss: 0.2537\n",
      "Epoch: 22 | Training Batch: 120 | Average loss: 0.2549\n",
      "Epoch: 22 | Training Batch: 130 | Average loss: 0.2488\n",
      "Epoch: 22 | Training Batch: 140 | Average loss: 0.2577\n",
      "Epoch: 22 | Training Batch: 150 | Average loss: 0.2555\n",
      "Epoch: 22 | Training Batch: 160 | Average loss: 0.2649\n",
      "Epoch: 22 | Training Batch: 170 | Average loss: 0.2593\n",
      "Epoch: 22 | Training Batch: 180 | Average loss: 0.2493\n",
      "Epoch: 22 | Training Batch: 190 | Average loss: 0.2597\n",
      "Epoch: 22 | Training Batch: 200 | Average loss: 0.2562\n",
      "Epoch: 22 | Training Batch: 210 | Average loss: 0.2504\n",
      "Epoch: 22 | Training Batch: 220 | Average loss: 0.2562\n",
      "Epoch: 22 | Training Batch: 230 | Average loss: 0.2643\n",
      "Epoch: 22 | Training Batch: 240 | Average loss: 0.2607\n",
      "Epoch: 22 | Training Batch: 250 | Average loss: 0.2555\n",
      "Epoch: 22 | Training Batch: 260 | Average loss: 0.2668\n",
      "Epoch: 22 | Training Batch: 270 | Average loss: 0.2590\n",
      "Epoch: 22 | Training Batch: 280 | Average loss: 0.2550\n",
      "Epoch: 22 | Training Batch: 290 | Average loss: 0.2593\n",
      "Epoch: 22 | Training Batch: 300 | Average loss: 0.2545\n",
      "Epoch: 22 | Training Batch: 310 | Average loss: 0.2498\n",
      "Epoch: 22 | Training Batch: 320 | Average loss: 0.2655\n",
      "Epoch: 22 | Training Batch: 330 | Average loss: 0.2471\n",
      "Epoch: 22 | Training Batch: 340 | Average loss: 0.2567\n",
      "Epoch: 22 | Training Batch: 350 | Average loss: 0.2587\n",
      "Epoch: 22 | Training Batch: 360 | Average loss: 0.2523\n",
      "Epoch: 22 | Training Batch: 370 | Average loss: 0.2489\n",
      "Epoch: 22 | Training Batch: 380 | Average loss: 0.2527\n",
      "Epoch: 22 | Training Batch: 390 | Average loss: 0.2568\n",
      "Epoch: 22 | Training Batch: 400 | Average loss: 0.2665\n",
      "Epoch: 22 | Training Batch: 410 | Average loss: 0.2517\n",
      "Epoch: 22 | Training Batch: 420 | Average loss: 0.2606\n",
      "Epoch: 22 | Training Batch: 430 | Average loss: 0.2550\n",
      "Epoch: 22 | Training Batch: 440 | Average loss: 0.2675\n",
      "Average training batch loss at epoch 22: 0.2650\n",
      "Average validation fold accuracy at epoch 22: 0.2620\n",
      "Epoch: 23 | Training Batch: 10 | Average loss: 0.2671\n",
      "Epoch: 23 | Training Batch: 20 | Average loss: 0.2594\n",
      "Epoch: 23 | Training Batch: 30 | Average loss: 0.2609\n",
      "Epoch: 23 | Training Batch: 40 | Average loss: 0.2553\n",
      "Epoch: 23 | Training Batch: 50 | Average loss: 0.2606\n",
      "Epoch: 23 | Training Batch: 60 | Average loss: 0.2545\n",
      "Epoch: 23 | Training Batch: 70 | Average loss: 0.2551\n",
      "Epoch: 23 | Training Batch: 80 | Average loss: 0.2568\n",
      "Epoch: 23 | Training Batch: 90 | Average loss: 0.2532\n",
      "Epoch: 23 | Training Batch: 100 | Average loss: 0.2582\n",
      "Epoch: 23 | Training Batch: 110 | Average loss: 0.2544\n",
      "Epoch: 23 | Training Batch: 120 | Average loss: 0.2567\n",
      "Epoch: 23 | Training Batch: 130 | Average loss: 0.2604\n",
      "Epoch: 23 | Training Batch: 140 | Average loss: 0.2556\n",
      "Epoch: 23 | Training Batch: 150 | Average loss: 0.2599\n",
      "Epoch: 23 | Training Batch: 160 | Average loss: 0.2535\n",
      "Epoch: 23 | Training Batch: 170 | Average loss: 0.2577\n",
      "Epoch: 23 | Training Batch: 180 | Average loss: 0.2535\n",
      "Epoch: 23 | Training Batch: 190 | Average loss: 0.2599\n",
      "Epoch: 23 | Training Batch: 200 | Average loss: 0.2511\n",
      "Epoch: 23 | Training Batch: 210 | Average loss: 0.2523\n",
      "Epoch: 23 | Training Batch: 220 | Average loss: 0.2504\n",
      "Epoch: 23 | Training Batch: 230 | Average loss: 0.2589\n",
      "Epoch: 23 | Training Batch: 240 | Average loss: 0.2572\n",
      "Epoch: 23 | Training Batch: 250 | Average loss: 0.2582\n",
      "Epoch: 23 | Training Batch: 260 | Average loss: 0.2641\n",
      "Epoch: 23 | Training Batch: 270 | Average loss: 0.2643\n",
      "Epoch: 23 | Training Batch: 280 | Average loss: 0.2495\n",
      "Epoch: 23 | Training Batch: 290 | Average loss: 0.2590\n",
      "Epoch: 23 | Training Batch: 300 | Average loss: 0.2550\n",
      "Epoch: 23 | Training Batch: 310 | Average loss: 0.2559\n",
      "Epoch: 23 | Training Batch: 320 | Average loss: 0.2597\n",
      "Epoch: 23 | Training Batch: 330 | Average loss: 0.2517\n",
      "Epoch: 23 | Training Batch: 340 | Average loss: 0.2522\n",
      "Epoch: 23 | Training Batch: 350 | Average loss: 0.2666\n",
      "Epoch: 23 | Training Batch: 360 | Average loss: 0.2540\n",
      "Epoch: 23 | Training Batch: 370 | Average loss: 0.2627\n",
      "Epoch: 23 | Training Batch: 380 | Average loss: 0.2461\n",
      "Epoch: 23 | Training Batch: 390 | Average loss: 0.2493\n",
      "Epoch: 23 | Training Batch: 400 | Average loss: 0.2557\n",
      "Epoch: 23 | Training Batch: 410 | Average loss: 0.2595\n",
      "Epoch: 23 | Training Batch: 420 | Average loss: 0.2453\n",
      "Epoch: 23 | Training Batch: 430 | Average loss: 0.2522\n",
      "Epoch: 23 | Training Batch: 440 | Average loss: 0.2636\n",
      "Average training batch loss at epoch 23: 0.2647\n",
      "Average validation fold accuracy at epoch 23: 0.2618\n",
      "Epoch: 24 | Training Batch: 10 | Average loss: 0.2545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 | Training Batch: 20 | Average loss: 0.2573\n",
      "Epoch: 24 | Training Batch: 30 | Average loss: 0.2601\n",
      "Epoch: 24 | Training Batch: 40 | Average loss: 0.2592\n",
      "Epoch: 24 | Training Batch: 50 | Average loss: 0.2471\n",
      "Epoch: 24 | Training Batch: 60 | Average loss: 0.2595\n",
      "Epoch: 24 | Training Batch: 70 | Average loss: 0.2456\n",
      "Epoch: 24 | Training Batch: 80 | Average loss: 0.2547\n",
      "Epoch: 24 | Training Batch: 90 | Average loss: 0.2573\n",
      "Epoch: 24 | Training Batch: 100 | Average loss: 0.2589\n",
      "Epoch: 24 | Training Batch: 110 | Average loss: 0.2626\n",
      "Epoch: 24 | Training Batch: 120 | Average loss: 0.2570\n",
      "Epoch: 24 | Training Batch: 130 | Average loss: 0.2519\n",
      "Epoch: 24 | Training Batch: 140 | Average loss: 0.2653\n",
      "Epoch: 24 | Training Batch: 150 | Average loss: 0.2660\n",
      "Epoch: 24 | Training Batch: 160 | Average loss: 0.2537\n",
      "Epoch: 24 | Training Batch: 170 | Average loss: 0.2615\n",
      "Epoch: 24 | Training Batch: 180 | Average loss: 0.2508\n",
      "Epoch: 24 | Training Batch: 190 | Average loss: 0.2559\n",
      "Epoch: 24 | Training Batch: 200 | Average loss: 0.2580\n",
      "Epoch: 24 | Training Batch: 210 | Average loss: 0.2546\n",
      "Epoch: 24 | Training Batch: 220 | Average loss: 0.2514\n",
      "Epoch: 24 | Training Batch: 230 | Average loss: 0.2581\n",
      "Epoch: 24 | Training Batch: 240 | Average loss: 0.2508\n",
      "Epoch: 24 | Training Batch: 250 | Average loss: 0.2586\n",
      "Epoch: 24 | Training Batch: 260 | Average loss: 0.2485\n",
      "Epoch: 24 | Training Batch: 270 | Average loss: 0.2567\n",
      "Epoch: 24 | Training Batch: 280 | Average loss: 0.2649\n",
      "Epoch: 24 | Training Batch: 290 | Average loss: 0.2533\n",
      "Epoch: 24 | Training Batch: 300 | Average loss: 0.2558\n",
      "Epoch: 24 | Training Batch: 310 | Average loss: 0.2552\n",
      "Epoch: 24 | Training Batch: 320 | Average loss: 0.2593\n",
      "Epoch: 24 | Training Batch: 330 | Average loss: 0.2550\n",
      "Epoch: 24 | Training Batch: 340 | Average loss: 0.2602\n",
      "Epoch: 24 | Training Batch: 350 | Average loss: 0.2437\n",
      "Epoch: 24 | Training Batch: 360 | Average loss: 0.2520\n",
      "Epoch: 24 | Training Batch: 370 | Average loss: 0.2568\n",
      "Epoch: 24 | Training Batch: 380 | Average loss: 0.2592\n",
      "Epoch: 24 | Training Batch: 390 | Average loss: 0.2537\n",
      "Epoch: 24 | Training Batch: 400 | Average loss: 0.2480\n",
      "Epoch: 24 | Training Batch: 410 | Average loss: 0.2590\n",
      "Epoch: 24 | Training Batch: 420 | Average loss: 0.2556\n",
      "Epoch: 24 | Training Batch: 430 | Average loss: 0.2661\n",
      "Epoch: 24 | Training Batch: 440 | Average loss: 0.2547\n",
      "Average training batch loss at epoch 24: 0.2643\n",
      "Average validation fold accuracy at epoch 24: 0.2616\n",
      "Epoch: 25 | Training Batch: 10 | Average loss: 0.2515\n",
      "Epoch: 25 | Training Batch: 20 | Average loss: 0.2601\n",
      "Epoch: 25 | Training Batch: 30 | Average loss: 0.2553\n",
      "Epoch: 25 | Training Batch: 40 | Average loss: 0.2546\n",
      "Epoch: 25 | Training Batch: 50 | Average loss: 0.2551\n",
      "Epoch: 25 | Training Batch: 60 | Average loss: 0.2468\n",
      "Epoch: 25 | Training Batch: 70 | Average loss: 0.2661\n",
      "Epoch: 25 | Training Batch: 80 | Average loss: 0.2527\n",
      "Epoch: 25 | Training Batch: 90 | Average loss: 0.2579\n",
      "Epoch: 25 | Training Batch: 100 | Average loss: 0.2617\n",
      "Epoch: 25 | Training Batch: 110 | Average loss: 0.2614\n",
      "Epoch: 25 | Training Batch: 120 | Average loss: 0.2509\n",
      "Epoch: 25 | Training Batch: 130 | Average loss: 0.2563\n",
      "Epoch: 25 | Training Batch: 140 | Average loss: 0.2619\n",
      "Epoch: 25 | Training Batch: 150 | Average loss: 0.2531\n",
      "Epoch: 25 | Training Batch: 160 | Average loss: 0.2482\n",
      "Epoch: 25 | Training Batch: 170 | Average loss: 0.2597\n",
      "Epoch: 25 | Training Batch: 180 | Average loss: 0.2582\n",
      "Epoch: 25 | Training Batch: 190 | Average loss: 0.2526\n",
      "Epoch: 25 | Training Batch: 200 | Average loss: 0.2554\n",
      "Epoch: 25 | Training Batch: 210 | Average loss: 0.2529\n",
      "Epoch: 25 | Training Batch: 220 | Average loss: 0.2495\n",
      "Epoch: 25 | Training Batch: 230 | Average loss: 0.2509\n",
      "Epoch: 25 | Training Batch: 240 | Average loss: 0.2652\n",
      "Epoch: 25 | Training Batch: 250 | Average loss: 0.2542\n",
      "Epoch: 25 | Training Batch: 260 | Average loss: 0.2606\n",
      "Epoch: 25 | Training Batch: 270 | Average loss: 0.2619\n",
      "Epoch: 25 | Training Batch: 280 | Average loss: 0.2480\n",
      "Epoch: 25 | Training Batch: 290 | Average loss: 0.2570\n",
      "Epoch: 25 | Training Batch: 300 | Average loss: 0.2551\n",
      "Epoch: 25 | Training Batch: 310 | Average loss: 0.2489\n",
      "Epoch: 25 | Training Batch: 320 | Average loss: 0.2550\n",
      "Epoch: 25 | Training Batch: 330 | Average loss: 0.2520\n",
      "Epoch: 25 | Training Batch: 340 | Average loss: 0.2558\n",
      "Epoch: 25 | Training Batch: 350 | Average loss: 0.2609\n",
      "Epoch: 25 | Training Batch: 360 | Average loss: 0.2490\n",
      "Epoch: 25 | Training Batch: 370 | Average loss: 0.2555\n",
      "Epoch: 25 | Training Batch: 380 | Average loss: 0.2520\n",
      "Epoch: 25 | Training Batch: 390 | Average loss: 0.2658\n",
      "Epoch: 25 | Training Batch: 400 | Average loss: 0.2511\n",
      "Epoch: 25 | Training Batch: 410 | Average loss: 0.2615\n",
      "Epoch: 25 | Training Batch: 420 | Average loss: 0.2579\n",
      "Epoch: 25 | Training Batch: 430 | Average loss: 0.2582\n",
      "Epoch: 25 | Training Batch: 440 | Average loss: 0.2596\n",
      "Average training batch loss at epoch 25: 0.2640\n",
      "Average validation fold accuracy at epoch 25: 0.2614\n",
      "Epoch: 26 | Training Batch: 10 | Average loss: 0.2460\n",
      "Epoch: 26 | Training Batch: 20 | Average loss: 0.2568\n",
      "Epoch: 26 | Training Batch: 30 | Average loss: 0.2556\n",
      "Epoch: 26 | Training Batch: 40 | Average loss: 0.2479\n",
      "Epoch: 26 | Training Batch: 50 | Average loss: 0.2607\n",
      "Epoch: 26 | Training Batch: 60 | Average loss: 0.2575\n",
      "Epoch: 26 | Training Batch: 70 | Average loss: 0.2477\n",
      "Epoch: 26 | Training Batch: 80 | Average loss: 0.2573\n",
      "Epoch: 26 | Training Batch: 90 | Average loss: 0.2514\n",
      "Epoch: 26 | Training Batch: 100 | Average loss: 0.2593\n",
      "Epoch: 26 | Training Batch: 110 | Average loss: 0.2619\n",
      "Epoch: 26 | Training Batch: 120 | Average loss: 0.2533\n",
      "Epoch: 26 | Training Batch: 130 | Average loss: 0.2520\n",
      "Epoch: 26 | Training Batch: 140 | Average loss: 0.2641\n",
      "Epoch: 26 | Training Batch: 150 | Average loss: 0.2444\n",
      "Epoch: 26 | Training Batch: 160 | Average loss: 0.2663\n",
      "Epoch: 26 | Training Batch: 170 | Average loss: 0.2567\n",
      "Epoch: 26 | Training Batch: 180 | Average loss: 0.2518\n",
      "Epoch: 26 | Training Batch: 190 | Average loss: 0.2595\n",
      "Epoch: 26 | Training Batch: 200 | Average loss: 0.2555\n",
      "Epoch: 26 | Training Batch: 210 | Average loss: 0.2521\n",
      "Epoch: 26 | Training Batch: 220 | Average loss: 0.2535\n",
      "Epoch: 26 | Training Batch: 230 | Average loss: 0.2601\n",
      "Epoch: 26 | Training Batch: 240 | Average loss: 0.2522\n",
      "Epoch: 26 | Training Batch: 250 | Average loss: 0.2594\n",
      "Epoch: 26 | Training Batch: 260 | Average loss: 0.2530\n",
      "Epoch: 26 | Training Batch: 270 | Average loss: 0.2541\n",
      "Epoch: 26 | Training Batch: 280 | Average loss: 0.2558\n",
      "Epoch: 26 | Training Batch: 290 | Average loss: 0.2476\n",
      "Epoch: 26 | Training Batch: 300 | Average loss: 0.2591\n",
      "Epoch: 26 | Training Batch: 310 | Average loss: 0.2657\n",
      "Epoch: 26 | Training Batch: 320 | Average loss: 0.2590\n",
      "Epoch: 26 | Training Batch: 330 | Average loss: 0.2596\n",
      "Epoch: 26 | Training Batch: 340 | Average loss: 0.2548\n",
      "Epoch: 26 | Training Batch: 350 | Average loss: 0.2519\n",
      "Epoch: 26 | Training Batch: 360 | Average loss: 0.2503\n",
      "Epoch: 26 | Training Batch: 370 | Average loss: 0.2515\n",
      "Epoch: 26 | Training Batch: 380 | Average loss: 0.2494\n",
      "Epoch: 26 | Training Batch: 390 | Average loss: 0.2568\n",
      "Epoch: 26 | Training Batch: 400 | Average loss: 0.2586\n",
      "Epoch: 26 | Training Batch: 410 | Average loss: 0.2502\n",
      "Epoch: 26 | Training Batch: 420 | Average loss: 0.2604\n",
      "Epoch: 26 | Training Batch: 430 | Average loss: 0.2483\n",
      "Epoch: 26 | Training Batch: 440 | Average loss: 0.2562\n",
      "Average training batch loss at epoch 26: 0.2637\n",
      "Average validation fold accuracy at epoch 26: 0.2612\n",
      "Epoch: 27 | Training Batch: 10 | Average loss: 0.2577\n",
      "Epoch: 27 | Training Batch: 20 | Average loss: 0.2549\n",
      "Epoch: 27 | Training Batch: 30 | Average loss: 0.2581\n",
      "Epoch: 27 | Training Batch: 40 | Average loss: 0.2589\n",
      "Epoch: 27 | Training Batch: 50 | Average loss: 0.2485\n",
      "Epoch: 27 | Training Batch: 60 | Average loss: 0.2502\n",
      "Epoch: 27 | Training Batch: 70 | Average loss: 0.2550\n",
      "Epoch: 27 | Training Batch: 80 | Average loss: 0.2598\n",
      "Epoch: 27 | Training Batch: 90 | Average loss: 0.2677\n",
      "Epoch: 27 | Training Batch: 100 | Average loss: 0.2553\n",
      "Epoch: 27 | Training Batch: 110 | Average loss: 0.2537\n",
      "Epoch: 27 | Training Batch: 120 | Average loss: 0.2450\n",
      "Epoch: 27 | Training Batch: 130 | Average loss: 0.2568\n",
      "Epoch: 27 | Training Batch: 140 | Average loss: 0.2678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Training Batch: 150 | Average loss: 0.2579\n",
      "Epoch: 27 | Training Batch: 160 | Average loss: 0.2565\n",
      "Epoch: 27 | Training Batch: 170 | Average loss: 0.2561\n",
      "Epoch: 27 | Training Batch: 180 | Average loss: 0.2560\n",
      "Epoch: 27 | Training Batch: 190 | Average loss: 0.2574\n",
      "Epoch: 27 | Training Batch: 200 | Average loss: 0.2564\n",
      "Epoch: 27 | Training Batch: 210 | Average loss: 0.2615\n",
      "Epoch: 27 | Training Batch: 220 | Average loss: 0.2569\n",
      "Epoch: 27 | Training Batch: 230 | Average loss: 0.2579\n",
      "Epoch: 27 | Training Batch: 240 | Average loss: 0.2567\n",
      "Epoch: 27 | Training Batch: 250 | Average loss: 0.2575\n",
      "Epoch: 27 | Training Batch: 260 | Average loss: 0.2465\n",
      "Epoch: 27 | Training Batch: 270 | Average loss: 0.2546\n",
      "Epoch: 27 | Training Batch: 280 | Average loss: 0.2575\n",
      "Epoch: 27 | Training Batch: 290 | Average loss: 0.2604\n",
      "Epoch: 27 | Training Batch: 300 | Average loss: 0.2534\n",
      "Epoch: 27 | Training Batch: 310 | Average loss: 0.2620\n",
      "Epoch: 27 | Training Batch: 320 | Average loss: 0.2613\n",
      "Epoch: 27 | Training Batch: 330 | Average loss: 0.2540\n",
      "Epoch: 27 | Training Batch: 340 | Average loss: 0.2400\n",
      "Epoch: 27 | Training Batch: 350 | Average loss: 0.2556\n",
      "Epoch: 27 | Training Batch: 360 | Average loss: 0.2512\n",
      "Epoch: 27 | Training Batch: 370 | Average loss: 0.2602\n",
      "Epoch: 27 | Training Batch: 380 | Average loss: 0.2612\n",
      "Epoch: 27 | Training Batch: 390 | Average loss: 0.2582\n",
      "Epoch: 27 | Training Batch: 400 | Average loss: 0.2596\n",
      "Epoch: 27 | Training Batch: 410 | Average loss: 0.2632\n",
      "Epoch: 27 | Training Batch: 420 | Average loss: 0.2538\n",
      "Epoch: 27 | Training Batch: 430 | Average loss: 0.2629\n",
      "Epoch: 27 | Training Batch: 440 | Average loss: 0.2524\n",
      "Average training batch loss at epoch 27: 0.2634\n",
      "Average validation fold accuracy at epoch 27: 0.2611\n",
      "Epoch: 28 | Training Batch: 10 | Average loss: 0.2611\n",
      "Epoch: 28 | Training Batch: 20 | Average loss: 0.2613\n",
      "Epoch: 28 | Training Batch: 30 | Average loss: 0.2686\n",
      "Epoch: 28 | Training Batch: 40 | Average loss: 0.2485\n",
      "Epoch: 28 | Training Batch: 50 | Average loss: 0.2520\n",
      "Epoch: 28 | Training Batch: 60 | Average loss: 0.2575\n",
      "Epoch: 28 | Training Batch: 70 | Average loss: 0.2565\n",
      "Epoch: 28 | Training Batch: 80 | Average loss: 0.2504\n",
      "Epoch: 28 | Training Batch: 90 | Average loss: 0.2510\n",
      "Epoch: 28 | Training Batch: 100 | Average loss: 0.2569\n",
      "Epoch: 28 | Training Batch: 110 | Average loss: 0.2498\n",
      "Epoch: 28 | Training Batch: 120 | Average loss: 0.2542\n",
      "Epoch: 28 | Training Batch: 130 | Average loss: 0.2542\n",
      "Epoch: 28 | Training Batch: 140 | Average loss: 0.2522\n",
      "Epoch: 28 | Training Batch: 150 | Average loss: 0.2566\n",
      "Epoch: 28 | Training Batch: 160 | Average loss: 0.2603\n",
      "Epoch: 28 | Training Batch: 170 | Average loss: 0.2535\n",
      "Epoch: 28 | Training Batch: 180 | Average loss: 0.2555\n",
      "Epoch: 28 | Training Batch: 190 | Average loss: 0.2545\n",
      "Epoch: 28 | Training Batch: 200 | Average loss: 0.2540\n",
      "Epoch: 28 | Training Batch: 210 | Average loss: 0.2606\n",
      "Epoch: 28 | Training Batch: 220 | Average loss: 0.2479\n",
      "Epoch: 28 | Training Batch: 230 | Average loss: 0.2610\n",
      "Epoch: 28 | Training Batch: 240 | Average loss: 0.2627\n",
      "Epoch: 28 | Training Batch: 250 | Average loss: 0.2514\n",
      "Epoch: 28 | Training Batch: 260 | Average loss: 0.2535\n",
      "Epoch: 28 | Training Batch: 270 | Average loss: 0.2585\n",
      "Epoch: 28 | Training Batch: 280 | Average loss: 0.2570\n",
      "Epoch: 28 | Training Batch: 290 | Average loss: 0.2493\n",
      "Epoch: 28 | Training Batch: 300 | Average loss: 0.2487\n",
      "Epoch: 28 | Training Batch: 310 | Average loss: 0.2569\n",
      "Epoch: 28 | Training Batch: 320 | Average loss: 0.2591\n",
      "Epoch: 28 | Training Batch: 330 | Average loss: 0.2504\n",
      "Epoch: 28 | Training Batch: 340 | Average loss: 0.2519\n",
      "Epoch: 28 | Training Batch: 350 | Average loss: 0.2572\n",
      "Epoch: 28 | Training Batch: 360 | Average loss: 0.2567\n",
      "Epoch: 28 | Training Batch: 370 | Average loss: 0.2550\n",
      "Epoch: 28 | Training Batch: 380 | Average loss: 0.2496\n",
      "Epoch: 28 | Training Batch: 390 | Average loss: 0.2543\n",
      "Epoch: 28 | Training Batch: 400 | Average loss: 0.2530\n",
      "Epoch: 28 | Training Batch: 410 | Average loss: 0.2555\n",
      "Epoch: 28 | Training Batch: 420 | Average loss: 0.2504\n",
      "Epoch: 28 | Training Batch: 430 | Average loss: 0.2506\n",
      "Epoch: 28 | Training Batch: 440 | Average loss: 0.2560\n",
      "Average training batch loss at epoch 28: 0.2631\n",
      "Average validation fold accuracy at epoch 28: 0.2610\n",
      "Epoch: 29 | Training Batch: 10 | Average loss: 0.2597\n",
      "Epoch: 29 | Training Batch: 20 | Average loss: 0.2535\n",
      "Epoch: 29 | Training Batch: 30 | Average loss: 0.2579\n",
      "Epoch: 29 | Training Batch: 40 | Average loss: 0.2467\n",
      "Epoch: 29 | Training Batch: 50 | Average loss: 0.2565\n",
      "Epoch: 29 | Training Batch: 60 | Average loss: 0.2532\n",
      "Epoch: 29 | Training Batch: 70 | Average loss: 0.2621\n",
      "Epoch: 29 | Training Batch: 80 | Average loss: 0.2535\n",
      "Epoch: 29 | Training Batch: 90 | Average loss: 0.2445\n",
      "Epoch: 29 | Training Batch: 100 | Average loss: 0.2567\n",
      "Epoch: 29 | Training Batch: 110 | Average loss: 0.2565\n",
      "Epoch: 29 | Training Batch: 120 | Average loss: 0.2532\n",
      "Epoch: 29 | Training Batch: 130 | Average loss: 0.2545\n",
      "Epoch: 29 | Training Batch: 140 | Average loss: 0.2510\n",
      "Epoch: 29 | Training Batch: 150 | Average loss: 0.2633\n",
      "Epoch: 29 | Training Batch: 160 | Average loss: 0.2517\n",
      "Epoch: 29 | Training Batch: 170 | Average loss: 0.2492\n",
      "Epoch: 29 | Training Batch: 180 | Average loss: 0.2605\n",
      "Epoch: 29 | Training Batch: 190 | Average loss: 0.2547\n",
      "Epoch: 29 | Training Batch: 200 | Average loss: 0.2471\n",
      "Epoch: 29 | Training Batch: 210 | Average loss: 0.2523\n",
      "Epoch: 29 | Training Batch: 220 | Average loss: 0.2630\n",
      "Epoch: 29 | Training Batch: 230 | Average loss: 0.2507\n",
      "Epoch: 29 | Training Batch: 240 | Average loss: 0.2531\n",
      "Epoch: 29 | Training Batch: 250 | Average loss: 0.2554\n",
      "Epoch: 29 | Training Batch: 260 | Average loss: 0.2552\n",
      "Epoch: 29 | Training Batch: 270 | Average loss: 0.2473\n",
      "Epoch: 29 | Training Batch: 280 | Average loss: 0.2615\n",
      "Epoch: 29 | Training Batch: 290 | Average loss: 0.2623\n",
      "Epoch: 29 | Training Batch: 300 | Average loss: 0.2568\n",
      "Epoch: 29 | Training Batch: 310 | Average loss: 0.2551\n",
      "Epoch: 29 | Training Batch: 320 | Average loss: 0.2481\n",
      "Epoch: 29 | Training Batch: 330 | Average loss: 0.2592\n",
      "Epoch: 29 | Training Batch: 340 | Average loss: 0.2492\n",
      "Epoch: 29 | Training Batch: 350 | Average loss: 0.2546\n",
      "Epoch: 29 | Training Batch: 360 | Average loss: 0.2621\n",
      "Epoch: 29 | Training Batch: 370 | Average loss: 0.2594\n",
      "Epoch: 29 | Training Batch: 380 | Average loss: 0.2474\n",
      "Epoch: 29 | Training Batch: 390 | Average loss: 0.2583\n",
      "Epoch: 29 | Training Batch: 400 | Average loss: 0.2543\n",
      "Epoch: 29 | Training Batch: 410 | Average loss: 0.2566\n",
      "Epoch: 29 | Training Batch: 420 | Average loss: 0.2549\n",
      "Epoch: 29 | Training Batch: 430 | Average loss: 0.2548\n",
      "Epoch: 29 | Training Batch: 440 | Average loss: 0.2603\n",
      "Average training batch loss at epoch 29: 0.2629\n",
      "Average validation fold accuracy at epoch 29: 0.2607\n",
      "Epoch: 30 | Training Batch: 10 | Average loss: 0.2495\n",
      "Epoch: 30 | Training Batch: 20 | Average loss: 0.2690\n",
      "Epoch: 30 | Training Batch: 30 | Average loss: 0.2612\n",
      "Epoch: 30 | Training Batch: 40 | Average loss: 0.2547\n",
      "Epoch: 30 | Training Batch: 50 | Average loss: 0.2512\n",
      "Epoch: 30 | Training Batch: 60 | Average loss: 0.2502\n",
      "Epoch: 30 | Training Batch: 70 | Average loss: 0.2449\n",
      "Epoch: 30 | Training Batch: 80 | Average loss: 0.2679\n",
      "Epoch: 30 | Training Batch: 90 | Average loss: 0.2502\n",
      "Epoch: 30 | Training Batch: 100 | Average loss: 0.2527\n",
      "Epoch: 30 | Training Batch: 110 | Average loss: 0.2484\n",
      "Epoch: 30 | Training Batch: 120 | Average loss: 0.2539\n",
      "Epoch: 30 | Training Batch: 130 | Average loss: 0.2587\n",
      "Epoch: 30 | Training Batch: 140 | Average loss: 0.2527\n",
      "Epoch: 30 | Training Batch: 150 | Average loss: 0.2604\n",
      "Epoch: 30 | Training Batch: 160 | Average loss: 0.2442\n",
      "Epoch: 30 | Training Batch: 170 | Average loss: 0.2450\n",
      "Epoch: 30 | Training Batch: 180 | Average loss: 0.2576\n",
      "Epoch: 30 | Training Batch: 190 | Average loss: 0.2597\n",
      "Epoch: 30 | Training Batch: 200 | Average loss: 0.2499\n",
      "Epoch: 30 | Training Batch: 210 | Average loss: 0.2587\n",
      "Epoch: 30 | Training Batch: 220 | Average loss: 0.2646\n",
      "Epoch: 30 | Training Batch: 230 | Average loss: 0.2516\n",
      "Epoch: 30 | Training Batch: 240 | Average loss: 0.2518\n",
      "Epoch: 30 | Training Batch: 250 | Average loss: 0.2535\n",
      "Epoch: 30 | Training Batch: 260 | Average loss: 0.2555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Training Batch: 270 | Average loss: 0.2484\n",
      "Epoch: 30 | Training Batch: 280 | Average loss: 0.2527\n",
      "Epoch: 30 | Training Batch: 290 | Average loss: 0.2608\n",
      "Epoch: 30 | Training Batch: 300 | Average loss: 0.2631\n",
      "Epoch: 30 | Training Batch: 310 | Average loss: 0.2578\n",
      "Epoch: 30 | Training Batch: 320 | Average loss: 0.2580\n",
      "Epoch: 30 | Training Batch: 330 | Average loss: 0.2499\n",
      "Epoch: 30 | Training Batch: 340 | Average loss: 0.2568\n",
      "Epoch: 30 | Training Batch: 350 | Average loss: 0.2569\n",
      "Epoch: 30 | Training Batch: 360 | Average loss: 0.2509\n",
      "Epoch: 30 | Training Batch: 370 | Average loss: 0.2559\n",
      "Epoch: 30 | Training Batch: 380 | Average loss: 0.2492\n",
      "Epoch: 30 | Training Batch: 390 | Average loss: 0.2587\n",
      "Epoch: 30 | Training Batch: 400 | Average loss: 0.2601\n",
      "Epoch: 30 | Training Batch: 410 | Average loss: 0.2526\n",
      "Epoch: 30 | Training Batch: 420 | Average loss: 0.2530\n",
      "Epoch: 30 | Training Batch: 430 | Average loss: 0.2547\n",
      "Epoch: 30 | Training Batch: 440 | Average loss: 0.2582\n",
      "Average training batch loss at epoch 30: 0.2626\n",
      "Average validation fold accuracy at epoch 30: 0.2605\n",
      "Epoch: 31 | Training Batch: 10 | Average loss: 0.2650\n",
      "Epoch: 31 | Training Batch: 20 | Average loss: 0.2583\n",
      "Epoch: 31 | Training Batch: 30 | Average loss: 0.2545\n",
      "Epoch: 31 | Training Batch: 40 | Average loss: 0.2495\n",
      "Epoch: 31 | Training Batch: 50 | Average loss: 0.2526\n",
      "Epoch: 31 | Training Batch: 60 | Average loss: 0.2582\n",
      "Epoch: 31 | Training Batch: 70 | Average loss: 0.2623\n",
      "Epoch: 31 | Training Batch: 80 | Average loss: 0.2536\n",
      "Epoch: 31 | Training Batch: 90 | Average loss: 0.2563\n",
      "Epoch: 31 | Training Batch: 100 | Average loss: 0.2422\n",
      "Epoch: 31 | Training Batch: 110 | Average loss: 0.2557\n",
      "Epoch: 31 | Training Batch: 120 | Average loss: 0.2531\n",
      "Epoch: 31 | Training Batch: 130 | Average loss: 0.2443\n",
      "Epoch: 31 | Training Batch: 140 | Average loss: 0.2559\n",
      "Epoch: 31 | Training Batch: 150 | Average loss: 0.2559\n",
      "Epoch: 31 | Training Batch: 160 | Average loss: 0.2568\n",
      "Epoch: 31 | Training Batch: 170 | Average loss: 0.2615\n",
      "Epoch: 31 | Training Batch: 180 | Average loss: 0.2576\n",
      "Epoch: 31 | Training Batch: 190 | Average loss: 0.2455\n",
      "Epoch: 31 | Training Batch: 200 | Average loss: 0.2574\n",
      "Epoch: 31 | Training Batch: 210 | Average loss: 0.2486\n",
      "Epoch: 31 | Training Batch: 220 | Average loss: 0.2517\n",
      "Epoch: 31 | Training Batch: 230 | Average loss: 0.2624\n",
      "Epoch: 31 | Training Batch: 240 | Average loss: 0.2554\n",
      "Epoch: 31 | Training Batch: 250 | Average loss: 0.2608\n",
      "Epoch: 31 | Training Batch: 260 | Average loss: 0.2493\n",
      "Epoch: 31 | Training Batch: 270 | Average loss: 0.2607\n",
      "Epoch: 31 | Training Batch: 280 | Average loss: 0.2610\n",
      "Epoch: 31 | Training Batch: 290 | Average loss: 0.2590\n",
      "Epoch: 31 | Training Batch: 300 | Average loss: 0.2574\n",
      "Epoch: 31 | Training Batch: 310 | Average loss: 0.2494\n",
      "Epoch: 31 | Training Batch: 320 | Average loss: 0.2521\n",
      "Epoch: 31 | Training Batch: 330 | Average loss: 0.2503\n",
      "Epoch: 31 | Training Batch: 340 | Average loss: 0.2561\n",
      "Epoch: 31 | Training Batch: 350 | Average loss: 0.2608\n",
      "Epoch: 31 | Training Batch: 360 | Average loss: 0.2537\n",
      "Epoch: 31 | Training Batch: 370 | Average loss: 0.2548\n",
      "Epoch: 31 | Training Batch: 380 | Average loss: 0.2551\n",
      "Epoch: 31 | Training Batch: 390 | Average loss: 0.2542\n",
      "Epoch: 31 | Training Batch: 400 | Average loss: 0.2584\n",
      "Epoch: 31 | Training Batch: 410 | Average loss: 0.2573\n",
      "Epoch: 31 | Training Batch: 420 | Average loss: 0.2497\n",
      "Epoch: 31 | Training Batch: 430 | Average loss: 0.2586\n",
      "Epoch: 31 | Training Batch: 440 | Average loss: 0.2589\n",
      "Average training batch loss at epoch 31: 0.2624\n",
      "Average validation fold accuracy at epoch 31: 0.2605\n",
      "Epoch: 32 | Training Batch: 10 | Average loss: 0.2528\n",
      "Epoch: 32 | Training Batch: 20 | Average loss: 0.2521\n",
      "Epoch: 32 | Training Batch: 30 | Average loss: 0.2561\n",
      "Epoch: 32 | Training Batch: 40 | Average loss: 0.2493\n",
      "Epoch: 32 | Training Batch: 50 | Average loss: 0.2524\n",
      "Epoch: 32 | Training Batch: 60 | Average loss: 0.2581\n",
      "Epoch: 32 | Training Batch: 70 | Average loss: 0.2550\n",
      "Epoch: 32 | Training Batch: 80 | Average loss: 0.2515\n",
      "Epoch: 32 | Training Batch: 90 | Average loss: 0.2603\n",
      "Epoch: 32 | Training Batch: 100 | Average loss: 0.2511\n",
      "Epoch: 32 | Training Batch: 110 | Average loss: 0.2554\n",
      "Epoch: 32 | Training Batch: 120 | Average loss: 0.2489\n",
      "Epoch: 32 | Training Batch: 130 | Average loss: 0.2641\n",
      "Epoch: 32 | Training Batch: 140 | Average loss: 0.2566\n",
      "Epoch: 32 | Training Batch: 150 | Average loss: 0.2581\n",
      "Epoch: 32 | Training Batch: 160 | Average loss: 0.2539\n",
      "Epoch: 32 | Training Batch: 170 | Average loss: 0.2549\n",
      "Epoch: 32 | Training Batch: 180 | Average loss: 0.2577\n",
      "Epoch: 32 | Training Batch: 190 | Average loss: 0.2503\n",
      "Epoch: 32 | Training Batch: 200 | Average loss: 0.2609\n",
      "Epoch: 32 | Training Batch: 210 | Average loss: 0.2593\n",
      "Epoch: 32 | Training Batch: 220 | Average loss: 0.2595\n",
      "Epoch: 32 | Training Batch: 230 | Average loss: 0.2553\n",
      "Epoch: 32 | Training Batch: 240 | Average loss: 0.2502\n",
      "Epoch: 32 | Training Batch: 250 | Average loss: 0.2483\n",
      "Epoch: 32 | Training Batch: 260 | Average loss: 0.2485\n",
      "Epoch: 32 | Training Batch: 270 | Average loss: 0.2613\n",
      "Epoch: 32 | Training Batch: 280 | Average loss: 0.2492\n",
      "Epoch: 32 | Training Batch: 290 | Average loss: 0.2487\n",
      "Epoch: 32 | Training Batch: 300 | Average loss: 0.2531\n",
      "Epoch: 32 | Training Batch: 310 | Average loss: 0.2604\n",
      "Epoch: 32 | Training Batch: 320 | Average loss: 0.2547\n",
      "Epoch: 32 | Training Batch: 330 | Average loss: 0.2454\n",
      "Epoch: 32 | Training Batch: 340 | Average loss: 0.2536\n",
      "Epoch: 32 | Training Batch: 350 | Average loss: 0.2540\n",
      "Epoch: 32 | Training Batch: 360 | Average loss: 0.2542\n",
      "Epoch: 32 | Training Batch: 370 | Average loss: 0.2488\n",
      "Epoch: 32 | Training Batch: 380 | Average loss: 0.2520\n",
      "Epoch: 32 | Training Batch: 390 | Average loss: 0.2607\n",
      "Epoch: 32 | Training Batch: 400 | Average loss: 0.2528\n",
      "Epoch: 32 | Training Batch: 410 | Average loss: 0.2536\n",
      "Epoch: 32 | Training Batch: 420 | Average loss: 0.2581\n",
      "Epoch: 32 | Training Batch: 430 | Average loss: 0.2479\n",
      "Epoch: 32 | Training Batch: 440 | Average loss: 0.2537\n",
      "Average training batch loss at epoch 32: 0.2621\n",
      "Average validation fold accuracy at epoch 32: 0.2604\n",
      "Epoch: 33 | Training Batch: 10 | Average loss: 0.2610\n",
      "Epoch: 33 | Training Batch: 20 | Average loss: 0.2489\n",
      "Epoch: 33 | Training Batch: 30 | Average loss: 0.2491\n",
      "Epoch: 33 | Training Batch: 40 | Average loss: 0.2547\n",
      "Epoch: 33 | Training Batch: 50 | Average loss: 0.2531\n",
      "Epoch: 33 | Training Batch: 60 | Average loss: 0.2615\n",
      "Epoch: 33 | Training Batch: 70 | Average loss: 0.2512\n",
      "Epoch: 33 | Training Batch: 80 | Average loss: 0.2633\n",
      "Epoch: 33 | Training Batch: 90 | Average loss: 0.2574\n",
      "Epoch: 33 | Training Batch: 100 | Average loss: 0.2468\n",
      "Epoch: 33 | Training Batch: 110 | Average loss: 0.2503\n",
      "Epoch: 33 | Training Batch: 120 | Average loss: 0.2468\n",
      "Epoch: 33 | Training Batch: 130 | Average loss: 0.2573\n",
      "Epoch: 33 | Training Batch: 140 | Average loss: 0.2581\n",
      "Epoch: 33 | Training Batch: 150 | Average loss: 0.2534\n",
      "Epoch: 33 | Training Batch: 160 | Average loss: 0.2572\n",
      "Epoch: 33 | Training Batch: 170 | Average loss: 0.2524\n",
      "Epoch: 33 | Training Batch: 180 | Average loss: 0.2563\n",
      "Epoch: 33 | Training Batch: 190 | Average loss: 0.2490\n",
      "Epoch: 33 | Training Batch: 200 | Average loss: 0.2558\n",
      "Epoch: 33 | Training Batch: 210 | Average loss: 0.2537\n",
      "Epoch: 33 | Training Batch: 220 | Average loss: 0.2548\n",
      "Epoch: 33 | Training Batch: 230 | Average loss: 0.2614\n",
      "Epoch: 33 | Training Batch: 240 | Average loss: 0.2585\n",
      "Epoch: 33 | Training Batch: 250 | Average loss: 0.2537\n",
      "Epoch: 33 | Training Batch: 260 | Average loss: 0.2585\n",
      "Epoch: 33 | Training Batch: 270 | Average loss: 0.2436\n",
      "Epoch: 33 | Training Batch: 280 | Average loss: 0.2544\n",
      "Epoch: 33 | Training Batch: 290 | Average loss: 0.2596\n",
      "Epoch: 33 | Training Batch: 300 | Average loss: 0.2613\n",
      "Epoch: 33 | Training Batch: 310 | Average loss: 0.2563\n",
      "Epoch: 33 | Training Batch: 320 | Average loss: 0.2569\n",
      "Epoch: 33 | Training Batch: 330 | Average loss: 0.2558\n",
      "Epoch: 33 | Training Batch: 340 | Average loss: 0.2526\n",
      "Epoch: 33 | Training Batch: 350 | Average loss: 0.2580\n",
      "Epoch: 33 | Training Batch: 360 | Average loss: 0.2574\n",
      "Epoch: 33 | Training Batch: 370 | Average loss: 0.2544\n",
      "Epoch: 33 | Training Batch: 380 | Average loss: 0.2538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33 | Training Batch: 390 | Average loss: 0.2541\n",
      "Epoch: 33 | Training Batch: 400 | Average loss: 0.2599\n",
      "Epoch: 33 | Training Batch: 410 | Average loss: 0.2549\n",
      "Epoch: 33 | Training Batch: 420 | Average loss: 0.2528\n",
      "Epoch: 33 | Training Batch: 430 | Average loss: 0.2501\n",
      "Epoch: 33 | Training Batch: 440 | Average loss: 0.2496\n",
      "Average training batch loss at epoch 33: 0.2619\n",
      "Average validation fold accuracy at epoch 33: 0.2603\n",
      "Epoch: 34 | Training Batch: 10 | Average loss: 0.2539\n",
      "Epoch: 34 | Training Batch: 20 | Average loss: 0.2580\n",
      "Epoch: 34 | Training Batch: 30 | Average loss: 0.2565\n",
      "Epoch: 34 | Training Batch: 40 | Average loss: 0.2492\n",
      "Epoch: 34 | Training Batch: 50 | Average loss: 0.2544\n",
      "Epoch: 34 | Training Batch: 60 | Average loss: 0.2593\n",
      "Epoch: 34 | Training Batch: 70 | Average loss: 0.2553\n",
      "Epoch: 34 | Training Batch: 80 | Average loss: 0.2563\n",
      "Epoch: 34 | Training Batch: 90 | Average loss: 0.2523\n",
      "Epoch: 34 | Training Batch: 100 | Average loss: 0.2646\n",
      "Epoch: 34 | Training Batch: 110 | Average loss: 0.2550\n",
      "Epoch: 34 | Training Batch: 120 | Average loss: 0.2451\n",
      "Epoch: 34 | Training Batch: 130 | Average loss: 0.2546\n",
      "Epoch: 34 | Training Batch: 140 | Average loss: 0.2599\n",
      "Epoch: 34 | Training Batch: 150 | Average loss: 0.2580\n",
      "Epoch: 34 | Training Batch: 160 | Average loss: 0.2515\n",
      "Epoch: 34 | Training Batch: 170 | Average loss: 0.2579\n",
      "Epoch: 34 | Training Batch: 180 | Average loss: 0.2584\n",
      "Epoch: 34 | Training Batch: 190 | Average loss: 0.2521\n",
      "Epoch: 34 | Training Batch: 200 | Average loss: 0.2475\n",
      "Epoch: 34 | Training Batch: 210 | Average loss: 0.2539\n",
      "Epoch: 34 | Training Batch: 220 | Average loss: 0.2498\n",
      "Epoch: 34 | Training Batch: 230 | Average loss: 0.2557\n",
      "Epoch: 34 | Training Batch: 240 | Average loss: 0.2512\n",
      "Epoch: 34 | Training Batch: 250 | Average loss: 0.2589\n",
      "Epoch: 34 | Training Batch: 260 | Average loss: 0.2478\n",
      "Epoch: 34 | Training Batch: 270 | Average loss: 0.2526\n",
      "Epoch: 34 | Training Batch: 280 | Average loss: 0.2512\n",
      "Epoch: 34 | Training Batch: 290 | Average loss: 0.2567\n",
      "Epoch: 34 | Training Batch: 300 | Average loss: 0.2632\n",
      "Epoch: 34 | Training Batch: 310 | Average loss: 0.2463\n",
      "Epoch: 34 | Training Batch: 320 | Average loss: 0.2593\n",
      "Epoch: 34 | Training Batch: 330 | Average loss: 0.2577\n",
      "Epoch: 34 | Training Batch: 340 | Average loss: 0.2553\n",
      "Epoch: 34 | Training Batch: 350 | Average loss: 0.2501\n",
      "Epoch: 34 | Training Batch: 360 | Average loss: 0.2574\n",
      "Epoch: 34 | Training Batch: 370 | Average loss: 0.2559\n",
      "Epoch: 34 | Training Batch: 380 | Average loss: 0.2542\n",
      "Epoch: 34 | Training Batch: 390 | Average loss: 0.2430\n",
      "Epoch: 34 | Training Batch: 400 | Average loss: 0.2501\n",
      "Epoch: 34 | Training Batch: 410 | Average loss: 0.2501\n",
      "Epoch: 34 | Training Batch: 420 | Average loss: 0.2572\n",
      "Epoch: 34 | Training Batch: 430 | Average loss: 0.2502\n",
      "Epoch: 34 | Training Batch: 440 | Average loss: 0.2609\n",
      "Average training batch loss at epoch 34: 0.2617\n",
      "Average validation fold accuracy at epoch 34: 0.2601\n",
      "Epoch: 35 | Training Batch: 10 | Average loss: 0.2452\n",
      "Epoch: 35 | Training Batch: 20 | Average loss: 0.2586\n",
      "Epoch: 35 | Training Batch: 30 | Average loss: 0.2502\n",
      "Epoch: 35 | Training Batch: 40 | Average loss: 0.2503\n",
      "Epoch: 35 | Training Batch: 50 | Average loss: 0.2607\n",
      "Epoch: 35 | Training Batch: 60 | Average loss: 0.2510\n",
      "Epoch: 35 | Training Batch: 70 | Average loss: 0.2542\n",
      "Epoch: 35 | Training Batch: 80 | Average loss: 0.2515\n",
      "Epoch: 35 | Training Batch: 90 | Average loss: 0.2592\n",
      "Epoch: 35 | Training Batch: 100 | Average loss: 0.2496\n",
      "Epoch: 35 | Training Batch: 110 | Average loss: 0.2517\n",
      "Epoch: 35 | Training Batch: 120 | Average loss: 0.2585\n",
      "Epoch: 35 | Training Batch: 130 | Average loss: 0.2499\n",
      "Epoch: 35 | Training Batch: 140 | Average loss: 0.2581\n",
      "Epoch: 35 | Training Batch: 150 | Average loss: 0.2514\n",
      "Epoch: 35 | Training Batch: 160 | Average loss: 0.2507\n",
      "Epoch: 35 | Training Batch: 170 | Average loss: 0.2466\n",
      "Epoch: 35 | Training Batch: 180 | Average loss: 0.2602\n",
      "Epoch: 35 | Training Batch: 190 | Average loss: 0.2512\n",
      "Epoch: 35 | Training Batch: 200 | Average loss: 0.2589\n",
      "Epoch: 35 | Training Batch: 210 | Average loss: 0.2564\n",
      "Epoch: 35 | Training Batch: 220 | Average loss: 0.2636\n",
      "Epoch: 35 | Training Batch: 230 | Average loss: 0.2559\n",
      "Epoch: 35 | Training Batch: 240 | Average loss: 0.2546\n",
      "Epoch: 35 | Training Batch: 250 | Average loss: 0.2562\n",
      "Epoch: 35 | Training Batch: 260 | Average loss: 0.2491\n",
      "Epoch: 35 | Training Batch: 270 | Average loss: 0.2547\n",
      "Epoch: 35 | Training Batch: 280 | Average loss: 0.2476\n",
      "Epoch: 35 | Training Batch: 290 | Average loss: 0.2503\n",
      "Epoch: 35 | Training Batch: 300 | Average loss: 0.2550\n",
      "Epoch: 35 | Training Batch: 310 | Average loss: 0.2595\n",
      "Epoch: 35 | Training Batch: 320 | Average loss: 0.2595\n",
      "Epoch: 35 | Training Batch: 330 | Average loss: 0.2520\n",
      "Epoch: 35 | Training Batch: 340 | Average loss: 0.2489\n",
      "Epoch: 35 | Training Batch: 350 | Average loss: 0.2494\n",
      "Epoch: 35 | Training Batch: 360 | Average loss: 0.2526\n",
      "Epoch: 35 | Training Batch: 370 | Average loss: 0.2531\n",
      "Epoch: 35 | Training Batch: 380 | Average loss: 0.2499\n",
      "Epoch: 35 | Training Batch: 390 | Average loss: 0.2603\n",
      "Epoch: 35 | Training Batch: 400 | Average loss: 0.2664\n",
      "Epoch: 35 | Training Batch: 410 | Average loss: 0.2560\n",
      "Epoch: 35 | Training Batch: 420 | Average loss: 0.2605\n",
      "Epoch: 35 | Training Batch: 430 | Average loss: 0.2539\n",
      "Epoch: 35 | Training Batch: 440 | Average loss: 0.2599\n",
      "Average training batch loss at epoch 35: 0.2615\n",
      "Average validation fold accuracy at epoch 35: 0.2600\n",
      "Epoch: 36 | Training Batch: 10 | Average loss: 0.2554\n",
      "Epoch: 36 | Training Batch: 20 | Average loss: 0.2456\n",
      "Epoch: 36 | Training Batch: 30 | Average loss: 0.2616\n",
      "Epoch: 36 | Training Batch: 40 | Average loss: 0.2554\n",
      "Epoch: 36 | Training Batch: 50 | Average loss: 0.2525\n",
      "Epoch: 36 | Training Batch: 60 | Average loss: 0.2562\n",
      "Epoch: 36 | Training Batch: 70 | Average loss: 0.2503\n",
      "Epoch: 36 | Training Batch: 80 | Average loss: 0.2518\n",
      "Epoch: 36 | Training Batch: 90 | Average loss: 0.2627\n",
      "Epoch: 36 | Training Batch: 100 | Average loss: 0.2563\n",
      "Epoch: 36 | Training Batch: 110 | Average loss: 0.2446\n",
      "Epoch: 36 | Training Batch: 120 | Average loss: 0.2506\n",
      "Epoch: 36 | Training Batch: 130 | Average loss: 0.2537\n",
      "Epoch: 36 | Training Batch: 140 | Average loss: 0.2471\n",
      "Epoch: 36 | Training Batch: 150 | Average loss: 0.2410\n",
      "Epoch: 36 | Training Batch: 160 | Average loss: 0.2578\n",
      "Epoch: 36 | Training Batch: 170 | Average loss: 0.2524\n",
      "Epoch: 36 | Training Batch: 180 | Average loss: 0.2503\n",
      "Epoch: 36 | Training Batch: 190 | Average loss: 0.2498\n",
      "Epoch: 36 | Training Batch: 200 | Average loss: 0.2490\n",
      "Epoch: 36 | Training Batch: 210 | Average loss: 0.2570\n",
      "Epoch: 36 | Training Batch: 220 | Average loss: 0.2583\n",
      "Epoch: 36 | Training Batch: 230 | Average loss: 0.2488\n",
      "Epoch: 36 | Training Batch: 240 | Average loss: 0.2470\n",
      "Epoch: 36 | Training Batch: 250 | Average loss: 0.2501\n",
      "Epoch: 36 | Training Batch: 260 | Average loss: 0.2620\n",
      "Epoch: 36 | Training Batch: 270 | Average loss: 0.2531\n",
      "Epoch: 36 | Training Batch: 280 | Average loss: 0.2599\n",
      "Epoch: 36 | Training Batch: 290 | Average loss: 0.2486\n",
      "Epoch: 36 | Training Batch: 300 | Average loss: 0.2609\n",
      "Epoch: 36 | Training Batch: 310 | Average loss: 0.2497\n",
      "Epoch: 36 | Training Batch: 320 | Average loss: 0.2515\n",
      "Epoch: 36 | Training Batch: 330 | Average loss: 0.2619\n",
      "Epoch: 36 | Training Batch: 340 | Average loss: 0.2570\n",
      "Epoch: 36 | Training Batch: 350 | Average loss: 0.2611\n",
      "Epoch: 36 | Training Batch: 360 | Average loss: 0.2517\n",
      "Epoch: 36 | Training Batch: 370 | Average loss: 0.2520\n",
      "Epoch: 36 | Training Batch: 380 | Average loss: 0.2539\n",
      "Epoch: 36 | Training Batch: 390 | Average loss: 0.2490\n",
      "Epoch: 36 | Training Batch: 400 | Average loss: 0.2586\n",
      "Epoch: 36 | Training Batch: 410 | Average loss: 0.2552\n",
      "Epoch: 36 | Training Batch: 420 | Average loss: 0.2637\n",
      "Epoch: 36 | Training Batch: 430 | Average loss: 0.2510\n",
      "Epoch: 36 | Training Batch: 440 | Average loss: 0.2548\n",
      "Average training batch loss at epoch 36: 0.2613\n",
      "Average validation fold accuracy at epoch 36: 0.2599\n",
      "Epoch: 37 | Training Batch: 10 | Average loss: 0.2517\n",
      "Epoch: 37 | Training Batch: 20 | Average loss: 0.2597\n",
      "Epoch: 37 | Training Batch: 30 | Average loss: 0.2562\n",
      "Epoch: 37 | Training Batch: 40 | Average loss: 0.2521\n",
      "Epoch: 37 | Training Batch: 50 | Average loss: 0.2515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37 | Training Batch: 60 | Average loss: 0.2499\n",
      "Epoch: 37 | Training Batch: 70 | Average loss: 0.2589\n",
      "Epoch: 37 | Training Batch: 80 | Average loss: 0.2541\n",
      "Epoch: 37 | Training Batch: 90 | Average loss: 0.2542\n",
      "Epoch: 37 | Training Batch: 100 | Average loss: 0.2535\n",
      "Epoch: 37 | Training Batch: 110 | Average loss: 0.2457\n",
      "Epoch: 37 | Training Batch: 120 | Average loss: 0.2493\n",
      "Epoch: 37 | Training Batch: 130 | Average loss: 0.2483\n",
      "Epoch: 37 | Training Batch: 140 | Average loss: 0.2564\n",
      "Epoch: 37 | Training Batch: 150 | Average loss: 0.2533\n",
      "Epoch: 37 | Training Batch: 160 | Average loss: 0.2582\n",
      "Epoch: 37 | Training Batch: 170 | Average loss: 0.2572\n",
      "Epoch: 37 | Training Batch: 180 | Average loss: 0.2559\n",
      "Epoch: 37 | Training Batch: 190 | Average loss: 0.2574\n",
      "Epoch: 37 | Training Batch: 200 | Average loss: 0.2529\n",
      "Epoch: 37 | Training Batch: 210 | Average loss: 0.2583\n",
      "Epoch: 37 | Training Batch: 220 | Average loss: 0.2513\n",
      "Epoch: 37 | Training Batch: 230 | Average loss: 0.2570\n",
      "Epoch: 37 | Training Batch: 240 | Average loss: 0.2562\n",
      "Epoch: 37 | Training Batch: 250 | Average loss: 0.2593\n",
      "Epoch: 37 | Training Batch: 260 | Average loss: 0.2575\n",
      "Epoch: 37 | Training Batch: 270 | Average loss: 0.2525\n",
      "Epoch: 37 | Training Batch: 280 | Average loss: 0.2541\n",
      "Epoch: 37 | Training Batch: 290 | Average loss: 0.2542\n",
      "Epoch: 37 | Training Batch: 300 | Average loss: 0.2540\n",
      "Epoch: 37 | Training Batch: 310 | Average loss: 0.2521\n",
      "Epoch: 37 | Training Batch: 320 | Average loss: 0.2544\n",
      "Epoch: 37 | Training Batch: 330 | Average loss: 0.2530\n",
      "Epoch: 37 | Training Batch: 340 | Average loss: 0.2496\n",
      "Epoch: 37 | Training Batch: 350 | Average loss: 0.2522\n",
      "Epoch: 37 | Training Batch: 360 | Average loss: 0.2545\n",
      "Epoch: 37 | Training Batch: 370 | Average loss: 0.2574\n",
      "Epoch: 37 | Training Batch: 380 | Average loss: 0.2500\n",
      "Epoch: 37 | Training Batch: 390 | Average loss: 0.2576\n",
      "Epoch: 37 | Training Batch: 400 | Average loss: 0.2540\n",
      "Epoch: 37 | Training Batch: 410 | Average loss: 0.2511\n",
      "Epoch: 37 | Training Batch: 420 | Average loss: 0.2555\n",
      "Epoch: 37 | Training Batch: 430 | Average loss: 0.2549\n",
      "Epoch: 37 | Training Batch: 440 | Average loss: 0.2487\n",
      "Average training batch loss at epoch 37: 0.2611\n",
      "Average validation fold accuracy at epoch 37: 0.2598\n",
      "Epoch: 38 | Training Batch: 10 | Average loss: 0.2490\n",
      "Epoch: 38 | Training Batch: 20 | Average loss: 0.2527\n",
      "Epoch: 38 | Training Batch: 30 | Average loss: 0.2639\n",
      "Epoch: 38 | Training Batch: 40 | Average loss: 0.2536\n",
      "Epoch: 38 | Training Batch: 50 | Average loss: 0.2543\n",
      "Epoch: 38 | Training Batch: 60 | Average loss: 0.2460\n",
      "Epoch: 38 | Training Batch: 70 | Average loss: 0.2513\n",
      "Epoch: 38 | Training Batch: 80 | Average loss: 0.2527\n",
      "Epoch: 38 | Training Batch: 90 | Average loss: 0.2574\n",
      "Epoch: 38 | Training Batch: 100 | Average loss: 0.2552\n",
      "Epoch: 38 | Training Batch: 110 | Average loss: 0.2581\n",
      "Epoch: 38 | Training Batch: 120 | Average loss: 0.2459\n",
      "Epoch: 38 | Training Batch: 130 | Average loss: 0.2591\n",
      "Epoch: 38 | Training Batch: 140 | Average loss: 0.2629\n",
      "Epoch: 38 | Training Batch: 150 | Average loss: 0.2512\n",
      "Epoch: 38 | Training Batch: 160 | Average loss: 0.2490\n",
      "Epoch: 38 | Training Batch: 170 | Average loss: 0.2492\n",
      "Epoch: 38 | Training Batch: 180 | Average loss: 0.2569\n",
      "Epoch: 38 | Training Batch: 190 | Average loss: 0.2594\n",
      "Epoch: 38 | Training Batch: 200 | Average loss: 0.2539\n",
      "Epoch: 38 | Training Batch: 210 | Average loss: 0.2510\n",
      "Epoch: 38 | Training Batch: 220 | Average loss: 0.2620\n",
      "Epoch: 38 | Training Batch: 230 | Average loss: 0.2502\n",
      "Epoch: 38 | Training Batch: 240 | Average loss: 0.2475\n",
      "Epoch: 38 | Training Batch: 250 | Average loss: 0.2532\n",
      "Epoch: 38 | Training Batch: 260 | Average loss: 0.2503\n",
      "Epoch: 38 | Training Batch: 270 | Average loss: 0.2554\n",
      "Epoch: 38 | Training Batch: 280 | Average loss: 0.2468\n",
      "Epoch: 38 | Training Batch: 290 | Average loss: 0.2528\n",
      "Epoch: 38 | Training Batch: 300 | Average loss: 0.2534\n",
      "Epoch: 38 | Training Batch: 310 | Average loss: 0.2558\n",
      "Epoch: 38 | Training Batch: 320 | Average loss: 0.2533\n",
      "Epoch: 38 | Training Batch: 330 | Average loss: 0.2562\n",
      "Epoch: 38 | Training Batch: 340 | Average loss: 0.2485\n",
      "Epoch: 38 | Training Batch: 350 | Average loss: 0.2571\n",
      "Epoch: 38 | Training Batch: 360 | Average loss: 0.2489\n",
      "Epoch: 38 | Training Batch: 370 | Average loss: 0.2512\n",
      "Epoch: 38 | Training Batch: 380 | Average loss: 0.2597\n",
      "Epoch: 38 | Training Batch: 390 | Average loss: 0.2475\n",
      "Epoch: 38 | Training Batch: 400 | Average loss: 0.2559\n",
      "Epoch: 38 | Training Batch: 410 | Average loss: 0.2577\n",
      "Epoch: 38 | Training Batch: 420 | Average loss: 0.2566\n",
      "Epoch: 38 | Training Batch: 430 | Average loss: 0.2470\n",
      "Epoch: 38 | Training Batch: 440 | Average loss: 0.2540\n",
      "Average training batch loss at epoch 38: 0.2609\n",
      "Average validation fold accuracy at epoch 38: 0.2596\n",
      "Epoch: 39 | Training Batch: 10 | Average loss: 0.2504\n",
      "Epoch: 39 | Training Batch: 20 | Average loss: 0.2506\n",
      "Epoch: 39 | Training Batch: 30 | Average loss: 0.2574\n",
      "Epoch: 39 | Training Batch: 40 | Average loss: 0.2549\n",
      "Epoch: 39 | Training Batch: 50 | Average loss: 0.2481\n",
      "Epoch: 39 | Training Batch: 60 | Average loss: 0.2505\n",
      "Epoch: 39 | Training Batch: 70 | Average loss: 0.2534\n",
      "Epoch: 39 | Training Batch: 80 | Average loss: 0.2592\n",
      "Epoch: 39 | Training Batch: 90 | Average loss: 0.2512\n",
      "Epoch: 39 | Training Batch: 100 | Average loss: 0.2513\n",
      "Epoch: 39 | Training Batch: 110 | Average loss: 0.2513\n",
      "Epoch: 39 | Training Batch: 120 | Average loss: 0.2481\n",
      "Epoch: 39 | Training Batch: 130 | Average loss: 0.2440\n",
      "Epoch: 39 | Training Batch: 140 | Average loss: 0.2542\n",
      "Epoch: 39 | Training Batch: 150 | Average loss: 0.2402\n",
      "Epoch: 39 | Training Batch: 160 | Average loss: 0.2619\n",
      "Epoch: 39 | Training Batch: 170 | Average loss: 0.2571\n",
      "Epoch: 39 | Training Batch: 180 | Average loss: 0.2587\n",
      "Epoch: 39 | Training Batch: 190 | Average loss: 0.2442\n",
      "Epoch: 39 | Training Batch: 200 | Average loss: 0.2523\n",
      "Epoch: 39 | Training Batch: 210 | Average loss: 0.2488\n",
      "Epoch: 39 | Training Batch: 220 | Average loss: 0.2530\n",
      "Epoch: 39 | Training Batch: 230 | Average loss: 0.2529\n",
      "Epoch: 39 | Training Batch: 240 | Average loss: 0.2493\n",
      "Epoch: 39 | Training Batch: 250 | Average loss: 0.2589\n",
      "Epoch: 39 | Training Batch: 260 | Average loss: 0.2544\n",
      "Epoch: 39 | Training Batch: 270 | Average loss: 0.2552\n",
      "Epoch: 39 | Training Batch: 280 | Average loss: 0.2594\n",
      "Epoch: 39 | Training Batch: 290 | Average loss: 0.2566\n",
      "Epoch: 39 | Training Batch: 300 | Average loss: 0.2607\n",
      "Epoch: 39 | Training Batch: 310 | Average loss: 0.2579\n",
      "Epoch: 39 | Training Batch: 320 | Average loss: 0.2540\n",
      "Epoch: 39 | Training Batch: 330 | Average loss: 0.2547\n",
      "Epoch: 39 | Training Batch: 340 | Average loss: 0.2604\n",
      "Epoch: 39 | Training Batch: 350 | Average loss: 0.2553\n",
      "Epoch: 39 | Training Batch: 360 | Average loss: 0.2510\n",
      "Epoch: 39 | Training Batch: 370 | Average loss: 0.2528\n",
      "Epoch: 39 | Training Batch: 380 | Average loss: 0.2508\n",
      "Epoch: 39 | Training Batch: 390 | Average loss: 0.2549\n",
      "Epoch: 39 | Training Batch: 400 | Average loss: 0.2582\n",
      "Epoch: 39 | Training Batch: 410 | Average loss: 0.2534\n",
      "Epoch: 39 | Training Batch: 420 | Average loss: 0.2574\n",
      "Epoch: 39 | Training Batch: 430 | Average loss: 0.2512\n",
      "Epoch: 39 | Training Batch: 440 | Average loss: 0.2414\n",
      "Average training batch loss at epoch 39: 0.2607\n",
      "Average validation fold accuracy at epoch 39: 0.2595\n",
      "Epoch: 40 | Training Batch: 10 | Average loss: 0.2550\n",
      "Epoch: 40 | Training Batch: 20 | Average loss: 0.2587\n",
      "Epoch: 40 | Training Batch: 30 | Average loss: 0.2489\n",
      "Epoch: 40 | Training Batch: 40 | Average loss: 0.2505\n",
      "Epoch: 40 | Training Batch: 50 | Average loss: 0.2587\n",
      "Epoch: 40 | Training Batch: 60 | Average loss: 0.2541\n",
      "Epoch: 40 | Training Batch: 70 | Average loss: 0.2629\n",
      "Epoch: 40 | Training Batch: 80 | Average loss: 0.2487\n",
      "Epoch: 40 | Training Batch: 90 | Average loss: 0.2484\n",
      "Epoch: 40 | Training Batch: 100 | Average loss: 0.2543\n",
      "Epoch: 40 | Training Batch: 110 | Average loss: 0.2594\n",
      "Epoch: 40 | Training Batch: 120 | Average loss: 0.2521\n",
      "Epoch: 40 | Training Batch: 130 | Average loss: 0.2514\n",
      "Epoch: 40 | Training Batch: 140 | Average loss: 0.2492\n",
      "Epoch: 40 | Training Batch: 150 | Average loss: 0.2503\n",
      "Epoch: 40 | Training Batch: 160 | Average loss: 0.2574\n",
      "Epoch: 40 | Training Batch: 170 | Average loss: 0.2613\n",
      "Epoch: 40 | Training Batch: 180 | Average loss: 0.2558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40 | Training Batch: 190 | Average loss: 0.2488\n",
      "Epoch: 40 | Training Batch: 200 | Average loss: 0.2650\n",
      "Epoch: 40 | Training Batch: 210 | Average loss: 0.2499\n",
      "Epoch: 40 | Training Batch: 220 | Average loss: 0.2574\n",
      "Epoch: 40 | Training Batch: 230 | Average loss: 0.2571\n",
      "Epoch: 40 | Training Batch: 240 | Average loss: 0.2554\n",
      "Epoch: 40 | Training Batch: 250 | Average loss: 0.2551\n",
      "Epoch: 40 | Training Batch: 260 | Average loss: 0.2492\n",
      "Epoch: 40 | Training Batch: 270 | Average loss: 0.2514\n",
      "Epoch: 40 | Training Batch: 280 | Average loss: 0.2525\n",
      "Epoch: 40 | Training Batch: 290 | Average loss: 0.2574\n",
      "Epoch: 40 | Training Batch: 300 | Average loss: 0.2559\n",
      "Epoch: 40 | Training Batch: 310 | Average loss: 0.2543\n",
      "Epoch: 40 | Training Batch: 320 | Average loss: 0.2496\n",
      "Epoch: 40 | Training Batch: 330 | Average loss: 0.2592\n",
      "Epoch: 40 | Training Batch: 340 | Average loss: 0.2547\n",
      "Epoch: 40 | Training Batch: 350 | Average loss: 0.2537\n",
      "Epoch: 40 | Training Batch: 360 | Average loss: 0.2557\n",
      "Epoch: 40 | Training Batch: 370 | Average loss: 0.2538\n",
      "Epoch: 40 | Training Batch: 380 | Average loss: 0.2524\n",
      "Epoch: 40 | Training Batch: 390 | Average loss: 0.2639\n",
      "Epoch: 40 | Training Batch: 400 | Average loss: 0.2528\n",
      "Epoch: 40 | Training Batch: 410 | Average loss: 0.2490\n",
      "Epoch: 40 | Training Batch: 420 | Average loss: 0.2569\n",
      "Epoch: 40 | Training Batch: 430 | Average loss: 0.2570\n",
      "Epoch: 40 | Training Batch: 440 | Average loss: 0.2570\n",
      "Average training batch loss at epoch 40: 0.2606\n",
      "Average validation fold accuracy at epoch 40: 0.2594\n",
      "Epoch: 41 | Training Batch: 10 | Average loss: 0.2551\n",
      "Epoch: 41 | Training Batch: 20 | Average loss: 0.2539\n",
      "Epoch: 41 | Training Batch: 30 | Average loss: 0.2550\n",
      "Epoch: 41 | Training Batch: 40 | Average loss: 0.2553\n",
      "Epoch: 41 | Training Batch: 50 | Average loss: 0.2583\n",
      "Epoch: 41 | Training Batch: 60 | Average loss: 0.2486\n",
      "Epoch: 41 | Training Batch: 70 | Average loss: 0.2495\n",
      "Epoch: 41 | Training Batch: 80 | Average loss: 0.2544\n",
      "Epoch: 41 | Training Batch: 90 | Average loss: 0.2641\n",
      "Epoch: 41 | Training Batch: 100 | Average loss: 0.2529\n",
      "Epoch: 41 | Training Batch: 110 | Average loss: 0.2493\n",
      "Epoch: 41 | Training Batch: 120 | Average loss: 0.2418\n",
      "Epoch: 41 | Training Batch: 130 | Average loss: 0.2513\n",
      "Epoch: 41 | Training Batch: 140 | Average loss: 0.2570\n",
      "Epoch: 41 | Training Batch: 150 | Average loss: 0.2522\n",
      "Epoch: 41 | Training Batch: 160 | Average loss: 0.2548\n",
      "Epoch: 41 | Training Batch: 170 | Average loss: 0.2587\n",
      "Epoch: 41 | Training Batch: 180 | Average loss: 0.2540\n",
      "Epoch: 41 | Training Batch: 190 | Average loss: 0.2525\n",
      "Epoch: 41 | Training Batch: 200 | Average loss: 0.2516\n",
      "Epoch: 41 | Training Batch: 210 | Average loss: 0.2541\n",
      "Epoch: 41 | Training Batch: 220 | Average loss: 0.2538\n",
      "Epoch: 41 | Training Batch: 230 | Average loss: 0.2564\n",
      "Epoch: 41 | Training Batch: 240 | Average loss: 0.2495\n",
      "Epoch: 41 | Training Batch: 250 | Average loss: 0.2575\n",
      "Epoch: 41 | Training Batch: 260 | Average loss: 0.2504\n",
      "Epoch: 41 | Training Batch: 270 | Average loss: 0.2525\n",
      "Epoch: 41 | Training Batch: 280 | Average loss: 0.2543\n",
      "Epoch: 41 | Training Batch: 290 | Average loss: 0.2550\n",
      "Epoch: 41 | Training Batch: 300 | Average loss: 0.2491\n",
      "Epoch: 41 | Training Batch: 310 | Average loss: 0.2597\n",
      "Epoch: 41 | Training Batch: 320 | Average loss: 0.2615\n",
      "Epoch: 41 | Training Batch: 330 | Average loss: 0.2565\n",
      "Epoch: 41 | Training Batch: 340 | Average loss: 0.2532\n",
      "Epoch: 41 | Training Batch: 350 | Average loss: 0.2489\n",
      "Epoch: 41 | Training Batch: 360 | Average loss: 0.2590\n",
      "Epoch: 41 | Training Batch: 370 | Average loss: 0.2480\n",
      "Epoch: 41 | Training Batch: 380 | Average loss: 0.2634\n",
      "Epoch: 41 | Training Batch: 390 | Average loss: 0.2480\n",
      "Epoch: 41 | Training Batch: 400 | Average loss: 0.2460\n",
      "Epoch: 41 | Training Batch: 410 | Average loss: 0.2490\n",
      "Epoch: 41 | Training Batch: 420 | Average loss: 0.2482\n",
      "Epoch: 41 | Training Batch: 430 | Average loss: 0.2548\n",
      "Epoch: 41 | Training Batch: 440 | Average loss: 0.2503\n",
      "Average training batch loss at epoch 41: 0.2604\n",
      "Average validation fold accuracy at epoch 41: 0.2593\n",
      "Epoch: 42 | Training Batch: 10 | Average loss: 0.2541\n",
      "Epoch: 42 | Training Batch: 20 | Average loss: 0.2501\n",
      "Epoch: 42 | Training Batch: 30 | Average loss: 0.2611\n",
      "Epoch: 42 | Training Batch: 40 | Average loss: 0.2506\n",
      "Epoch: 42 | Training Batch: 50 | Average loss: 0.2558\n",
      "Epoch: 42 | Training Batch: 60 | Average loss: 0.2574\n",
      "Epoch: 42 | Training Batch: 70 | Average loss: 0.2610\n",
      "Epoch: 42 | Training Batch: 80 | Average loss: 0.2526\n",
      "Epoch: 42 | Training Batch: 90 | Average loss: 0.2596\n",
      "Epoch: 42 | Training Batch: 100 | Average loss: 0.2531\n",
      "Epoch: 42 | Training Batch: 110 | Average loss: 0.2526\n",
      "Epoch: 42 | Training Batch: 120 | Average loss: 0.2454\n",
      "Epoch: 42 | Training Batch: 130 | Average loss: 0.2614\n",
      "Epoch: 42 | Training Batch: 140 | Average loss: 0.2593\n",
      "Epoch: 42 | Training Batch: 150 | Average loss: 0.2456\n",
      "Epoch: 42 | Training Batch: 160 | Average loss: 0.2421\n",
      "Epoch: 42 | Training Batch: 170 | Average loss: 0.2517\n",
      "Epoch: 42 | Training Batch: 180 | Average loss: 0.2484\n",
      "Epoch: 42 | Training Batch: 190 | Average loss: 0.2423\n",
      "Epoch: 42 | Training Batch: 200 | Average loss: 0.2546\n",
      "Epoch: 42 | Training Batch: 210 | Average loss: 0.2527\n",
      "Epoch: 42 | Training Batch: 220 | Average loss: 0.2459\n",
      "Epoch: 42 | Training Batch: 230 | Average loss: 0.2505\n",
      "Epoch: 42 | Training Batch: 240 | Average loss: 0.2497\n",
      "Epoch: 42 | Training Batch: 250 | Average loss: 0.2561\n",
      "Epoch: 42 | Training Batch: 260 | Average loss: 0.2540\n",
      "Epoch: 42 | Training Batch: 270 | Average loss: 0.2453\n",
      "Epoch: 42 | Training Batch: 280 | Average loss: 0.2575\n",
      "Epoch: 42 | Training Batch: 290 | Average loss: 0.2530\n",
      "Epoch: 42 | Training Batch: 300 | Average loss: 0.2473\n",
      "Epoch: 42 | Training Batch: 310 | Average loss: 0.2571\n",
      "Epoch: 42 | Training Batch: 320 | Average loss: 0.2520\n",
      "Epoch: 42 | Training Batch: 330 | Average loss: 0.2456\n",
      "Epoch: 42 | Training Batch: 340 | Average loss: 0.2478\n",
      "Epoch: 42 | Training Batch: 350 | Average loss: 0.2601\n",
      "Epoch: 42 | Training Batch: 360 | Average loss: 0.2611\n",
      "Epoch: 42 | Training Batch: 370 | Average loss: 0.2512\n",
      "Epoch: 42 | Training Batch: 380 | Average loss: 0.2592\n",
      "Epoch: 42 | Training Batch: 390 | Average loss: 0.2564\n",
      "Epoch: 42 | Training Batch: 400 | Average loss: 0.2424\n",
      "Epoch: 42 | Training Batch: 410 | Average loss: 0.2558\n",
      "Epoch: 42 | Training Batch: 420 | Average loss: 0.2566\n",
      "Epoch: 42 | Training Batch: 430 | Average loss: 0.2590\n",
      "Epoch: 42 | Training Batch: 440 | Average loss: 0.2479\n",
      "Average training batch loss at epoch 42: 0.2602\n",
      "Average validation fold accuracy at epoch 42: 0.2592\n",
      "Epoch: 43 | Training Batch: 10 | Average loss: 0.2566\n",
      "Epoch: 43 | Training Batch: 20 | Average loss: 0.2502\n",
      "Epoch: 43 | Training Batch: 30 | Average loss: 0.2477\n",
      "Epoch: 43 | Training Batch: 40 | Average loss: 0.2489\n",
      "Epoch: 43 | Training Batch: 50 | Average loss: 0.2580\n",
      "Epoch: 43 | Training Batch: 60 | Average loss: 0.2522\n",
      "Epoch: 43 | Training Batch: 70 | Average loss: 0.2567\n",
      "Epoch: 43 | Training Batch: 80 | Average loss: 0.2520\n",
      "Epoch: 43 | Training Batch: 90 | Average loss: 0.2445\n",
      "Epoch: 43 | Training Batch: 100 | Average loss: 0.2597\n",
      "Epoch: 43 | Training Batch: 110 | Average loss: 0.2601\n",
      "Epoch: 43 | Training Batch: 120 | Average loss: 0.2538\n",
      "Epoch: 43 | Training Batch: 130 | Average loss: 0.2545\n",
      "Epoch: 43 | Training Batch: 140 | Average loss: 0.2563\n",
      "Epoch: 43 | Training Batch: 150 | Average loss: 0.2559\n",
      "Epoch: 43 | Training Batch: 160 | Average loss: 0.2578\n",
      "Epoch: 43 | Training Batch: 170 | Average loss: 0.2545\n",
      "Epoch: 43 | Training Batch: 180 | Average loss: 0.2545\n",
      "Epoch: 43 | Training Batch: 190 | Average loss: 0.2531\n",
      "Epoch: 43 | Training Batch: 200 | Average loss: 0.2570\n",
      "Epoch: 43 | Training Batch: 210 | Average loss: 0.2502\n",
      "Epoch: 43 | Training Batch: 220 | Average loss: 0.2622\n",
      "Epoch: 43 | Training Batch: 230 | Average loss: 0.2530\n",
      "Epoch: 43 | Training Batch: 240 | Average loss: 0.2452\n",
      "Epoch: 43 | Training Batch: 250 | Average loss: 0.2542\n",
      "Epoch: 43 | Training Batch: 260 | Average loss: 0.2479\n",
      "Epoch: 43 | Training Batch: 270 | Average loss: 0.2490\n",
      "Epoch: 43 | Training Batch: 280 | Average loss: 0.2551\n",
      "Epoch: 43 | Training Batch: 290 | Average loss: 0.2536\n",
      "Epoch: 43 | Training Batch: 300 | Average loss: 0.2539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 | Training Batch: 310 | Average loss: 0.2528\n",
      "Epoch: 43 | Training Batch: 320 | Average loss: 0.2505\n",
      "Epoch: 43 | Training Batch: 330 | Average loss: 0.2587\n",
      "Epoch: 43 | Training Batch: 340 | Average loss: 0.2551\n",
      "Epoch: 43 | Training Batch: 350 | Average loss: 0.2471\n",
      "Epoch: 43 | Training Batch: 360 | Average loss: 0.2535\n",
      "Epoch: 43 | Training Batch: 370 | Average loss: 0.2581\n",
      "Epoch: 43 | Training Batch: 380 | Average loss: 0.2503\n",
      "Epoch: 43 | Training Batch: 390 | Average loss: 0.2502\n",
      "Epoch: 43 | Training Batch: 400 | Average loss: 0.2471\n",
      "Epoch: 43 | Training Batch: 410 | Average loss: 0.2474\n",
      "Epoch: 43 | Training Batch: 420 | Average loss: 0.2501\n",
      "Epoch: 43 | Training Batch: 430 | Average loss: 0.2507\n",
      "Epoch: 43 | Training Batch: 440 | Average loss: 0.2533\n",
      "Average training batch loss at epoch 43: 0.2600\n",
      "Average validation fold accuracy at epoch 43: 0.2592\n",
      "Epoch: 44 | Training Batch: 10 | Average loss: 0.2550\n",
      "Epoch: 44 | Training Batch: 20 | Average loss: 0.2562\n",
      "Epoch: 44 | Training Batch: 30 | Average loss: 0.2597\n",
      "Epoch: 44 | Training Batch: 40 | Average loss: 0.2488\n",
      "Epoch: 44 | Training Batch: 50 | Average loss: 0.2552\n",
      "Epoch: 44 | Training Batch: 60 | Average loss: 0.2539\n",
      "Epoch: 44 | Training Batch: 70 | Average loss: 0.2511\n",
      "Epoch: 44 | Training Batch: 80 | Average loss: 0.2510\n",
      "Epoch: 44 | Training Batch: 90 | Average loss: 0.2503\n",
      "Epoch: 44 | Training Batch: 100 | Average loss: 0.2503\n",
      "Epoch: 44 | Training Batch: 110 | Average loss: 0.2503\n",
      "Epoch: 44 | Training Batch: 120 | Average loss: 0.2565\n",
      "Epoch: 44 | Training Batch: 130 | Average loss: 0.2586\n",
      "Epoch: 44 | Training Batch: 140 | Average loss: 0.2532\n",
      "Epoch: 44 | Training Batch: 150 | Average loss: 0.2553\n",
      "Epoch: 44 | Training Batch: 160 | Average loss: 0.2498\n",
      "Epoch: 44 | Training Batch: 170 | Average loss: 0.2489\n",
      "Epoch: 44 | Training Batch: 180 | Average loss: 0.2563\n",
      "Epoch: 44 | Training Batch: 190 | Average loss: 0.2467\n",
      "Epoch: 44 | Training Batch: 200 | Average loss: 0.2477\n",
      "Epoch: 44 | Training Batch: 210 | Average loss: 0.2532\n",
      "Epoch: 44 | Training Batch: 220 | Average loss: 0.2578\n",
      "Epoch: 44 | Training Batch: 230 | Average loss: 0.2546\n",
      "Epoch: 44 | Training Batch: 240 | Average loss: 0.2529\n",
      "Epoch: 44 | Training Batch: 250 | Average loss: 0.2547\n",
      "Epoch: 44 | Training Batch: 260 | Average loss: 0.2580\n",
      "Epoch: 44 | Training Batch: 270 | Average loss: 0.2511\n",
      "Epoch: 44 | Training Batch: 280 | Average loss: 0.2567\n",
      "Epoch: 44 | Training Batch: 290 | Average loss: 0.2639\n",
      "Epoch: 44 | Training Batch: 300 | Average loss: 0.2599\n",
      "Epoch: 44 | Training Batch: 310 | Average loss: 0.2486\n",
      "Epoch: 44 | Training Batch: 320 | Average loss: 0.2510\n",
      "Epoch: 44 | Training Batch: 330 | Average loss: 0.2538\n",
      "Epoch: 44 | Training Batch: 340 | Average loss: 0.2533\n",
      "Epoch: 44 | Training Batch: 350 | Average loss: 0.2500\n",
      "Epoch: 44 | Training Batch: 360 | Average loss: 0.2518\n",
      "Epoch: 44 | Training Batch: 370 | Average loss: 0.2562\n",
      "Epoch: 44 | Training Batch: 380 | Average loss: 0.2530\n",
      "Epoch: 44 | Training Batch: 390 | Average loss: 0.2633\n",
      "Epoch: 44 | Training Batch: 400 | Average loss: 0.2549\n",
      "Epoch: 44 | Training Batch: 410 | Average loss: 0.2637\n",
      "Epoch: 44 | Training Batch: 420 | Average loss: 0.2497\n",
      "Epoch: 44 | Training Batch: 430 | Average loss: 0.2583\n",
      "Epoch: 44 | Training Batch: 440 | Average loss: 0.2541\n",
      "Average training batch loss at epoch 44: 0.2599\n",
      "Average validation fold accuracy at epoch 44: 0.2590\n",
      "Epoch: 45 | Training Batch: 10 | Average loss: 0.2607\n",
      "Epoch: 45 | Training Batch: 20 | Average loss: 0.2502\n",
      "Epoch: 45 | Training Batch: 30 | Average loss: 0.2555\n",
      "Epoch: 45 | Training Batch: 40 | Average loss: 0.2573\n",
      "Epoch: 45 | Training Batch: 50 | Average loss: 0.2500\n",
      "Epoch: 45 | Training Batch: 60 | Average loss: 0.2546\n",
      "Epoch: 45 | Training Batch: 70 | Average loss: 0.2531\n",
      "Epoch: 45 | Training Batch: 80 | Average loss: 0.2506\n",
      "Epoch: 45 | Training Batch: 90 | Average loss: 0.2617\n",
      "Epoch: 45 | Training Batch: 100 | Average loss: 0.2458\n",
      "Epoch: 45 | Training Batch: 110 | Average loss: 0.2432\n",
      "Epoch: 45 | Training Batch: 120 | Average loss: 0.2504\n",
      "Epoch: 45 | Training Batch: 130 | Average loss: 0.2550\n",
      "Epoch: 45 | Training Batch: 140 | Average loss: 0.2503\n",
      "Epoch: 45 | Training Batch: 150 | Average loss: 0.2507\n",
      "Epoch: 45 | Training Batch: 160 | Average loss: 0.2615\n",
      "Epoch: 45 | Training Batch: 170 | Average loss: 0.2517\n",
      "Epoch: 45 | Training Batch: 180 | Average loss: 0.2517\n",
      "Epoch: 45 | Training Batch: 190 | Average loss: 0.2527\n",
      "Epoch: 45 | Training Batch: 200 | Average loss: 0.2531\n",
      "Epoch: 45 | Training Batch: 210 | Average loss: 0.2504\n",
      "Epoch: 45 | Training Batch: 220 | Average loss: 0.2443\n",
      "Epoch: 45 | Training Batch: 230 | Average loss: 0.2506\n",
      "Epoch: 45 | Training Batch: 240 | Average loss: 0.2603\n",
      "Epoch: 45 | Training Batch: 250 | Average loss: 0.2512\n",
      "Epoch: 45 | Training Batch: 260 | Average loss: 0.2522\n",
      "Epoch: 45 | Training Batch: 270 | Average loss: 0.2492\n",
      "Epoch: 45 | Training Batch: 280 | Average loss: 0.2481\n",
      "Epoch: 45 | Training Batch: 290 | Average loss: 0.2526\n",
      "Epoch: 45 | Training Batch: 300 | Average loss: 0.2557\n",
      "Epoch: 45 | Training Batch: 310 | Average loss: 0.2589\n",
      "Epoch: 45 | Training Batch: 320 | Average loss: 0.2516\n",
      "Epoch: 45 | Training Batch: 330 | Average loss: 0.2508\n",
      "Epoch: 45 | Training Batch: 340 | Average loss: 0.2519\n",
      "Epoch: 45 | Training Batch: 350 | Average loss: 0.2504\n",
      "Epoch: 45 | Training Batch: 360 | Average loss: 0.2524\n",
      "Epoch: 45 | Training Batch: 370 | Average loss: 0.2462\n",
      "Epoch: 45 | Training Batch: 380 | Average loss: 0.2551\n",
      "Epoch: 45 | Training Batch: 390 | Average loss: 0.2513\n",
      "Epoch: 45 | Training Batch: 400 | Average loss: 0.2639\n",
      "Epoch: 45 | Training Batch: 410 | Average loss: 0.2478\n",
      "Epoch: 45 | Training Batch: 420 | Average loss: 0.2485\n",
      "Epoch: 45 | Training Batch: 430 | Average loss: 0.2593\n",
      "Epoch: 45 | Training Batch: 440 | Average loss: 0.2626\n",
      "Average training batch loss at epoch 45: 0.2598\n",
      "Average validation fold accuracy at epoch 45: 0.2589\n",
      "Epoch: 46 | Training Batch: 10 | Average loss: 0.2527\n",
      "Epoch: 46 | Training Batch: 20 | Average loss: 0.2468\n",
      "Epoch: 46 | Training Batch: 30 | Average loss: 0.2551\n",
      "Epoch: 46 | Training Batch: 40 | Average loss: 0.2509\n",
      "Epoch: 46 | Training Batch: 50 | Average loss: 0.2601\n",
      "Epoch: 46 | Training Batch: 60 | Average loss: 0.2512\n",
      "Epoch: 46 | Training Batch: 70 | Average loss: 0.2501\n",
      "Epoch: 46 | Training Batch: 80 | Average loss: 0.2536\n",
      "Epoch: 46 | Training Batch: 90 | Average loss: 0.2583\n",
      "Epoch: 46 | Training Batch: 100 | Average loss: 0.2441\n",
      "Epoch: 46 | Training Batch: 110 | Average loss: 0.2524\n",
      "Epoch: 46 | Training Batch: 120 | Average loss: 0.2470\n",
      "Epoch: 46 | Training Batch: 130 | Average loss: 0.2491\n",
      "Epoch: 46 | Training Batch: 140 | Average loss: 0.2501\n",
      "Epoch: 46 | Training Batch: 150 | Average loss: 0.2569\n",
      "Epoch: 46 | Training Batch: 160 | Average loss: 0.2508\n",
      "Epoch: 46 | Training Batch: 170 | Average loss: 0.2559\n",
      "Epoch: 46 | Training Batch: 180 | Average loss: 0.2520\n",
      "Epoch: 46 | Training Batch: 190 | Average loss: 0.2520\n",
      "Epoch: 46 | Training Batch: 200 | Average loss: 0.2572\n",
      "Epoch: 46 | Training Batch: 210 | Average loss: 0.2483\n",
      "Epoch: 46 | Training Batch: 220 | Average loss: 0.2520\n",
      "Epoch: 46 | Training Batch: 230 | Average loss: 0.2476\n",
      "Epoch: 46 | Training Batch: 240 | Average loss: 0.2515\n",
      "Epoch: 46 | Training Batch: 250 | Average loss: 0.2507\n",
      "Epoch: 46 | Training Batch: 260 | Average loss: 0.2500\n",
      "Epoch: 46 | Training Batch: 270 | Average loss: 0.2469\n",
      "Epoch: 46 | Training Batch: 280 | Average loss: 0.2561\n",
      "Epoch: 46 | Training Batch: 290 | Average loss: 0.2481\n",
      "Epoch: 46 | Training Batch: 300 | Average loss: 0.2495\n",
      "Epoch: 46 | Training Batch: 310 | Average loss: 0.2413\n",
      "Epoch: 46 | Training Batch: 320 | Average loss: 0.2576\n",
      "Epoch: 46 | Training Batch: 330 | Average loss: 0.2534\n",
      "Epoch: 46 | Training Batch: 340 | Average loss: 0.2549\n",
      "Epoch: 46 | Training Batch: 350 | Average loss: 0.2483\n",
      "Epoch: 46 | Training Batch: 360 | Average loss: 0.2609\n",
      "Epoch: 46 | Training Batch: 370 | Average loss: 0.2535\n",
      "Epoch: 46 | Training Batch: 380 | Average loss: 0.2516\n",
      "Epoch: 46 | Training Batch: 390 | Average loss: 0.2574\n",
      "Epoch: 46 | Training Batch: 400 | Average loss: 0.2582\n",
      "Epoch: 46 | Training Batch: 410 | Average loss: 0.2519\n",
      "Epoch: 46 | Training Batch: 420 | Average loss: 0.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46 | Training Batch: 430 | Average loss: 0.2496\n",
      "Epoch: 46 | Training Batch: 440 | Average loss: 0.2558\n",
      "Average training batch loss at epoch 46: 0.2596\n",
      "Average validation fold accuracy at epoch 46: 0.2588\n",
      "Epoch: 47 | Training Batch: 10 | Average loss: 0.2572\n",
      "Epoch: 47 | Training Batch: 20 | Average loss: 0.2502\n",
      "Epoch: 47 | Training Batch: 30 | Average loss: 0.2493\n",
      "Epoch: 47 | Training Batch: 40 | Average loss: 0.2530\n",
      "Epoch: 47 | Training Batch: 50 | Average loss: 0.2470\n",
      "Epoch: 47 | Training Batch: 60 | Average loss: 0.2483\n",
      "Epoch: 47 | Training Batch: 70 | Average loss: 0.2487\n",
      "Epoch: 47 | Training Batch: 80 | Average loss: 0.2513\n",
      "Epoch: 47 | Training Batch: 90 | Average loss: 0.2554\n",
      "Epoch: 47 | Training Batch: 100 | Average loss: 0.2516\n",
      "Epoch: 47 | Training Batch: 110 | Average loss: 0.2562\n",
      "Epoch: 47 | Training Batch: 120 | Average loss: 0.2540\n",
      "Epoch: 47 | Training Batch: 130 | Average loss: 0.2547\n",
      "Epoch: 47 | Training Batch: 140 | Average loss: 0.2499\n",
      "Epoch: 47 | Training Batch: 150 | Average loss: 0.2525\n",
      "Epoch: 47 | Training Batch: 160 | Average loss: 0.2546\n",
      "Epoch: 47 | Training Batch: 170 | Average loss: 0.2544\n",
      "Epoch: 47 | Training Batch: 180 | Average loss: 0.2507\n",
      "Epoch: 47 | Training Batch: 190 | Average loss: 0.2498\n",
      "Epoch: 47 | Training Batch: 200 | Average loss: 0.2565\n",
      "Epoch: 47 | Training Batch: 210 | Average loss: 0.2498\n",
      "Epoch: 47 | Training Batch: 220 | Average loss: 0.2500\n",
      "Epoch: 47 | Training Batch: 230 | Average loss: 0.2493\n",
      "Epoch: 47 | Training Batch: 240 | Average loss: 0.2502\n",
      "Epoch: 47 | Training Batch: 250 | Average loss: 0.2514\n",
      "Epoch: 47 | Training Batch: 260 | Average loss: 0.2473\n",
      "Epoch: 47 | Training Batch: 270 | Average loss: 0.2542\n",
      "Epoch: 47 | Training Batch: 280 | Average loss: 0.2521\n",
      "Epoch: 47 | Training Batch: 290 | Average loss: 0.2594\n",
      "Epoch: 47 | Training Batch: 300 | Average loss: 0.2485\n",
      "Epoch: 47 | Training Batch: 310 | Average loss: 0.2517\n",
      "Epoch: 47 | Training Batch: 320 | Average loss: 0.2532\n",
      "Epoch: 47 | Training Batch: 330 | Average loss: 0.2568\n",
      "Epoch: 47 | Training Batch: 340 | Average loss: 0.2427\n",
      "Epoch: 47 | Training Batch: 350 | Average loss: 0.2565\n",
      "Epoch: 47 | Training Batch: 360 | Average loss: 0.2603\n",
      "Epoch: 47 | Training Batch: 370 | Average loss: 0.2522\n",
      "Epoch: 47 | Training Batch: 380 | Average loss: 0.2453\n",
      "Epoch: 47 | Training Batch: 390 | Average loss: 0.2604\n",
      "Epoch: 47 | Training Batch: 400 | Average loss: 0.2517\n",
      "Epoch: 47 | Training Batch: 410 | Average loss: 0.2538\n",
      "Epoch: 47 | Training Batch: 420 | Average loss: 0.2535\n",
      "Epoch: 47 | Training Batch: 430 | Average loss: 0.2521\n",
      "Epoch: 47 | Training Batch: 440 | Average loss: 0.2492\n",
      "Average training batch loss at epoch 47: 0.2594\n",
      "Average validation fold accuracy at epoch 47: 0.2587\n",
      "Epoch: 48 | Training Batch: 10 | Average loss: 0.2615\n",
      "Epoch: 48 | Training Batch: 20 | Average loss: 0.2409\n",
      "Epoch: 48 | Training Batch: 30 | Average loss: 0.2485\n",
      "Epoch: 48 | Training Batch: 40 | Average loss: 0.2438\n",
      "Epoch: 48 | Training Batch: 50 | Average loss: 0.2552\n",
      "Epoch: 48 | Training Batch: 60 | Average loss: 0.2517\n",
      "Epoch: 48 | Training Batch: 70 | Average loss: 0.2506\n",
      "Epoch: 48 | Training Batch: 80 | Average loss: 0.2570\n",
      "Epoch: 48 | Training Batch: 90 | Average loss: 0.2517\n",
      "Epoch: 48 | Training Batch: 100 | Average loss: 0.2566\n",
      "Epoch: 48 | Training Batch: 110 | Average loss: 0.2534\n",
      "Epoch: 48 | Training Batch: 120 | Average loss: 0.2537\n",
      "Epoch: 48 | Training Batch: 130 | Average loss: 0.2547\n",
      "Epoch: 48 | Training Batch: 140 | Average loss: 0.2551\n",
      "Epoch: 48 | Training Batch: 150 | Average loss: 0.2542\n",
      "Epoch: 48 | Training Batch: 160 | Average loss: 0.2533\n",
      "Epoch: 48 | Training Batch: 170 | Average loss: 0.2600\n",
      "Epoch: 48 | Training Batch: 180 | Average loss: 0.2533\n",
      "Epoch: 48 | Training Batch: 190 | Average loss: 0.2478\n",
      "Epoch: 48 | Training Batch: 200 | Average loss: 0.2564\n",
      "Epoch: 48 | Training Batch: 210 | Average loss: 0.2556\n",
      "Epoch: 48 | Training Batch: 220 | Average loss: 0.2544\n",
      "Epoch: 48 | Training Batch: 230 | Average loss: 0.2522\n",
      "Epoch: 48 | Training Batch: 240 | Average loss: 0.2525\n",
      "Epoch: 48 | Training Batch: 250 | Average loss: 0.2453\n",
      "Epoch: 48 | Training Batch: 260 | Average loss: 0.2606\n",
      "Epoch: 48 | Training Batch: 270 | Average loss: 0.2496\n",
      "Epoch: 48 | Training Batch: 280 | Average loss: 0.2490\n",
      "Epoch: 48 | Training Batch: 290 | Average loss: 0.2467\n",
      "Epoch: 48 | Training Batch: 300 | Average loss: 0.2602\n",
      "Epoch: 48 | Training Batch: 310 | Average loss: 0.2564\n",
      "Epoch: 48 | Training Batch: 320 | Average loss: 0.2472\n",
      "Epoch: 48 | Training Batch: 330 | Average loss: 0.2497\n",
      "Epoch: 48 | Training Batch: 340 | Average loss: 0.2534\n",
      "Epoch: 48 | Training Batch: 350 | Average loss: 0.2497\n",
      "Epoch: 48 | Training Batch: 360 | Average loss: 0.2569\n",
      "Epoch: 48 | Training Batch: 370 | Average loss: 0.2483\n",
      "Epoch: 48 | Training Batch: 380 | Average loss: 0.2522\n",
      "Epoch: 48 | Training Batch: 390 | Average loss: 0.2558\n",
      "Epoch: 48 | Training Batch: 400 | Average loss: 0.2555\n",
      "Epoch: 48 | Training Batch: 410 | Average loss: 0.2587\n",
      "Epoch: 48 | Training Batch: 420 | Average loss: 0.2572\n",
      "Epoch: 48 | Training Batch: 430 | Average loss: 0.2547\n",
      "Epoch: 48 | Training Batch: 440 | Average loss: 0.2484\n",
      "Average training batch loss at epoch 48: 0.2593\n",
      "Average validation fold accuracy at epoch 48: 0.2586\n",
      "Epoch: 49 | Training Batch: 10 | Average loss: 0.2539\n",
      "Epoch: 49 | Training Batch: 20 | Average loss: 0.2550\n",
      "Epoch: 49 | Training Batch: 30 | Average loss: 0.2523\n",
      "Epoch: 49 | Training Batch: 40 | Average loss: 0.2477\n",
      "Epoch: 49 | Training Batch: 50 | Average loss: 0.2618\n",
      "Epoch: 49 | Training Batch: 60 | Average loss: 0.2496\n",
      "Epoch: 49 | Training Batch: 70 | Average loss: 0.2519\n",
      "Epoch: 49 | Training Batch: 80 | Average loss: 0.2538\n",
      "Epoch: 49 | Training Batch: 90 | Average loss: 0.2535\n",
      "Epoch: 49 | Training Batch: 100 | Average loss: 0.2592\n",
      "Epoch: 49 | Training Batch: 110 | Average loss: 0.2568\n",
      "Epoch: 49 | Training Batch: 120 | Average loss: 0.2522\n",
      "Epoch: 49 | Training Batch: 130 | Average loss: 0.2513\n",
      "Epoch: 49 | Training Batch: 140 | Average loss: 0.2510\n",
      "Epoch: 49 | Training Batch: 150 | Average loss: 0.2540\n",
      "Epoch: 49 | Training Batch: 160 | Average loss: 0.2601\n",
      "Epoch: 49 | Training Batch: 170 | Average loss: 0.2470\n",
      "Epoch: 49 | Training Batch: 180 | Average loss: 0.2482\n",
      "Epoch: 49 | Training Batch: 190 | Average loss: 0.2554\n",
      "Epoch: 49 | Training Batch: 200 | Average loss: 0.2473\n",
      "Epoch: 49 | Training Batch: 210 | Average loss: 0.2519\n",
      "Epoch: 49 | Training Batch: 220 | Average loss: 0.2490\n",
      "Epoch: 49 | Training Batch: 230 | Average loss: 0.2564\n",
      "Epoch: 49 | Training Batch: 240 | Average loss: 0.2517\n",
      "Epoch: 49 | Training Batch: 250 | Average loss: 0.2512\n",
      "Epoch: 49 | Training Batch: 260 | Average loss: 0.2514\n",
      "Epoch: 49 | Training Batch: 270 | Average loss: 0.2532\n",
      "Epoch: 49 | Training Batch: 280 | Average loss: 0.2590\n",
      "Epoch: 49 | Training Batch: 290 | Average loss: 0.2557\n",
      "Epoch: 49 | Training Batch: 300 | Average loss: 0.2493\n",
      "Epoch: 49 | Training Batch: 310 | Average loss: 0.2467\n",
      "Epoch: 49 | Training Batch: 320 | Average loss: 0.2536\n",
      "Epoch: 49 | Training Batch: 330 | Average loss: 0.2546\n",
      "Epoch: 49 | Training Batch: 340 | Average loss: 0.2511\n",
      "Epoch: 49 | Training Batch: 350 | Average loss: 0.2447\n",
      "Epoch: 49 | Training Batch: 360 | Average loss: 0.2550\n",
      "Epoch: 49 | Training Batch: 370 | Average loss: 0.2549\n",
      "Epoch: 49 | Training Batch: 380 | Average loss: 0.2545\n",
      "Epoch: 49 | Training Batch: 390 | Average loss: 0.2499\n",
      "Epoch: 49 | Training Batch: 400 | Average loss: 0.2491\n",
      "Epoch: 49 | Training Batch: 410 | Average loss: 0.2527\n",
      "Epoch: 49 | Training Batch: 420 | Average loss: 0.2542\n",
      "Epoch: 49 | Training Batch: 430 | Average loss: 0.2500\n",
      "Epoch: 49 | Training Batch: 440 | Average loss: 0.2555\n",
      "Average training batch loss at epoch 49: 0.2592\n",
      "Average validation fold accuracy at epoch 49: 0.2585\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "pretrain=False\n",
    "\n",
    "opt = {\n",
    "    'node_size':201,\n",
    "    'hidden_size':30,\n",
    "    'num_layers':1,\n",
    "    'embedding_dim':50,\n",
    "    'learning_rate':0.0001,#10**np.random.uniform(-5, -3)\n",
    "    'bidirection_lstm':False,\n",
    "    'seqlen':50,\n",
    "    'batch_size':32\n",
    "}\n",
    "print('learning_rate: ',opt['learning_rate'])\n",
    "\n",
    "batch_size = opt['batch_size']\n",
    "# Initialize global tracking variables\n",
    "best_validation_accuracy = 0\n",
    "epochs_without_improvement = 0\n",
    "total_train_loss = list()\n",
    "total_valid_loss = []\n",
    "avg_trainings = []\n",
    "avg_valids = []\n",
    "\n",
    "# Loading model\n",
    "if pretrain:\n",
    "    classifier = torch.load('SiameseNN1.pt')\n",
    "else:\n",
    "    classifier = SiameseClassifier(opt, is_train=True)\n",
    "    # Initialize parameters\n",
    "    classifier.initialize_parameters()\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Initiate the training data loader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
    "    running_loss = list()\n",
    "    # Training loop\n",
    "    for i, (batch_x,label_var) in enumerate(train_loader):\n",
    "        s1_var = batch_x[:,0,:]\n",
    "        s2_var  = batch_x[:,1,:]\n",
    "        #s1_var = one_hot(s1_var)\n",
    "        #s2_var = one_hot(s2_var)\n",
    "        classifier.train_step(s1_var, s2_var, label_var)\n",
    "        train_batch_loss = classifier.loss.data[0]\n",
    "        running_loss.append(train_batch_loss)\n",
    "        total_train_loss.append(train_batch_loss)\n",
    "\n",
    "        if i % 10 == 0 and i != 0:\n",
    "            running_avg_loss = sum(running_loss) / len(running_loss)\n",
    "            print('Epoch: %d | Training Batch: %d | Average loss: %.4f' %\n",
    "                  (epoch, i , running_avg_loss))\n",
    "            running_loss = []\n",
    "            \n",
    "\n",
    "    # Report epoch statistics\n",
    "    avg_training_accuracy = sum(total_train_loss) / len(total_train_loss)\n",
    "    print('Average training batch loss at epoch %d: %.4f' % (epoch, avg_training_accuracy))\n",
    "    avg_trainings.append(avg_training_accuracy) \n",
    "    \n",
    "\n",
    "    # Validate after each epoch; set tracking variables\n",
    "    if epoch >= 0:\n",
    "        # Initiate the training data loader\n",
    "        valid_loader = DataLoader(val_dataset, batch_size=32,shuffle=True)\n",
    "        \n",
    "        # Validation loop (i.e. perform inference on the validation set)\n",
    "        for i, (batch_x,label_var) in enumerate(valid_loader):\n",
    "            s1_var = batch_x[:,0,:]\n",
    "            s2_var  = batch_x[:,1,:]\n",
    "            #s1_var = one_hot(s1_var)\n",
    "            #s2_var = one_hot(s2_var)\n",
    "            # Get predictions and update tracking values\n",
    "            classifier.test_step(s1_var, s2_var, label_var)\n",
    "            valid_batch_loss = classifier.loss.data[0]\n",
    "            total_valid_loss.append(valid_batch_loss)\n",
    "\n",
    "        # Report fold statistics\n",
    "        avg_valid_accuracy = sum(total_valid_loss) / len(total_valid_loss)\n",
    "        print('Average validation fold accuracy at epoch %d: %.4f' % (epoch, avg_valid_accuracy))\n",
    "        avg_valids.append(avg_valid_accuracy)\n",
    "        # Save network parameters if performance has improved\n",
    "        if avg_valid_accuracy <= best_validation_accuracy:\n",
    "            epochs_without_improvement += 1\n",
    "        else:\n",
    "            best_validation_accuracy = avg_valid_accuracy\n",
    "            epochs_without_improvement = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize= (20,5))\n",
    "ax.plot(avg_valids,marker='o',label='Val')\n",
    "ax.plot(avg_trainings,marker='o',label='Train')\n",
    "ax.set_xlabel('Loss',fontsize=15)\n",
    "ax.set_ylabel('Number of epochs',fontsize=15)\n",
    "ax.set_title('Loss',fontsize=20,fontweight =\"bold\")\n",
    "ax.legend()\n",
    "graph_path = \"./Pytorch_Results/FullDataOnlyLSTM1lr\" + str(opt['learning_rate']) +'.png'\n",
    "fig.savefig(graph_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richard/anaconda3/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type SiameseClassifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/richard/anaconda3/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type LSTMEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./Pytorch_Model/FullDataOnlyLSTM1lr\" + str(opt['learning_rate']) +'.pt'\n",
    "torch.save(classifier, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['embedding_table.weight', 'lstm_rnn.weight_ih_l0', 'lstm_rnn.weight_hh_l0', 'lstm_rnn.bias_ih_l0', 'lstm_rnn.bias_hh_l0', 'lstm_rnn.weight_ih_l1', 'lstm_rnn.weight_hh_l1', 'lstm_rnn.bias_ih_l1', 'lstm_rnn.bias_hh_l1', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.encoder_a.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters in this model:  27860\n"
     ]
    }
   ],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "print(\"number of parameters in this model: \",get_n_params(classifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Pytorch_Results/OnlyLSTM1lr9.708935755359682e-05.png'"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
